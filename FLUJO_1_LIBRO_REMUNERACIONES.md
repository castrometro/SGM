# FLUJO 1: Procesamiento Completo de Libro de Remuneraciones

**Fecha**: 24 octubre 2025  
**Estado**: üìù EN DOCUMENTACI√ìN - Smoke Test en progreso  
**Prop√≥sito**: Subir, clasificar y procesar archivo Excel con n√≥mina mensual completa

---

## üìã Resumen Ejecutivo

- **Trigger**: Usuario sube archivo Excel desde el frontend
- **Pasos principales**: 
  1. Subir archivo
  2. Analizar headers autom√°ticamente
  3. Clasificar headers manualmente
  4. Procesar libro (crear empleados y conceptos)
- **Duraci√≥n t√≠pica**: 
  - Subida + An√°lisis: 5-10 segundos
  - Clasificaci√≥n: Manual (1-2 minutos)
  - Procesamiento: 30-60 segundos
  - **Total**: ~2-3 minutos
- **Resultado**: Crea `EmpleadoCierre` y `RegistroConceptoEmpleado` en BD
- **Patr√≥n**: Upload ‚Üí Task an√°lisis ‚Üí UI clasificaci√≥n ‚Üí Celery Chord paralelo

---

## üéØ PASO 1: Subir Archivo Excel

### 1.1 Frontend: Selecci√≥n de Archivo
**Archivo**: `/src/components/TarjetasCierreNomina/LibroRemuneracionesCardConLogging.jsx`
**Archivo**: `/src/pages/CierreDetalleNomina.jsx`
```jsx
import CierreProgresoNomina from "../components/TarjetasCierreNomina/CierreProgresoNomina";

<CierreProgresoNomina
  cierre={cierre}
  cliente={cliente}
  onCierreActualizado={refrescarCierre}
/>
```

### 2. Componente de Progreso
**Archivo**: `/src/components/TarjetasCierreNomina/CierreProgresoNomina.jsx`

**Handler del bot√≥n** (l√≠nea 545):
```javascript
const handleProcesarLibro = async () => {
  console.log('=== PROCESAR LIBRO ===');
  
  const id = libro?.id || libroId;
  
  // FORZAR el estado a "procesando" ANTES de la llamada
  setLibro(prev => ({
    ...prev,
    estado: "procesando"
  }));
  setLibroListo(false);
  
  await procesarLibroRemuneraciones(id);  // üëà LLAMADA A API
  console.log('‚úÖ Procesamiento iniciado - el polling monitorear√° el progreso');
}
```

**Estados locales**:
- `libro`: Objeto con datos del libro cargado
- `libroId`: ID del libro en BD
- `subiendo`: Boolean para mostrar spinner de carga
- `libroListo`: Boolean para habilitar/deshabilitar bot√≥n

---

## üì° Capa de API: Cliente HTTP

### 3. Cliente de API
**Archivo**: `/src/api/nomina.js` (l√≠nea 767)

```javascript
export const procesarLibroRemuneraciones = async (libroId) => {
  const res = await api.post(
    `/nomina/libros-remuneraciones/${libroId}/procesar/`  // üëà ENDPOINT
  );
  return res.data;
};
```

**Request**:
```http
POST /api/nomina/libros-remuneraciones/{libroId}/procesar/
Content-Type: application/json
Authorization: Bearer {token}
```

**Response**:
```json
{
  "task_id": "abc-123-def-456",
  "mensaje": "Procesamiento optimizado iniciado (usando Celery Chord)",
  "optimizado": true
}
```

---

## üîó Backend: ViewSet

### 4. Vista de Django REST Framework
**Archivo**: `/backend/nomina/views.py` (l√≠nea 1489)

**ViewSet**: `LibroRemuneracionesViewSet`  
**Acci√≥n**: `@action(detail=True, methods=['post'])`

```python
def procesar(self, request, pk=None):
    """
    üöÄ Procesar libro completo: actualizar empleados y guardar registros.
    Versi√≥n optimizada con Celery Chord para mejor rendimiento.
    """
    libro = self.get_object()
    libro.estado = 'procesando'
    libro.save(update_fields=['estado'])
    
    # Leer par√°metros opcionales - optimizaci√≥n activada por defecto
    usar_optimizacion = request.data.get('usar_optimizacion', True)
    
    logger.info(f"üîÑ Iniciando procesamiento de libro {libro.id}")
    
    if usar_optimizacion:
        # üöÄ USAR VERSIONES OPTIMIZADAS CON CHORD
        result = chain(
            actualizar_empleados_desde_libro_optimizado.s(libro.id),  # üëà TASK 1
            guardar_registros_nomina_optimizado.s(),                   # üëà TASK 2
        ).apply_async()
        
        mensaje = 'Procesamiento optimizado iniciado'
    else:
        # üìù USAR VERSIONES CL√ÅSICAS
        result = chain(
            actualizar_empleados_desde_libro.s(libro.id),
            guardar_registros_nomina.s(),
        ).apply_async()
        mensaje = 'Procesamiento cl√°sico iniciado'
    
    return Response({
        'task_id': result.id,
        'mensaje': mensaje,
        'optimizado': usar_optimizacion
    }, status=status.HTTP_202_ACCEPTED)
```

**L√≥gica de Negocio**:
1. ‚úÖ Cambiar estado del libro a `'procesando'`
2. ‚úÖ Decidir si usar versi√≥n optimizada (por defecto: S√ç)
3. ‚úÖ Crear Celery Chain con 2 tareas secuenciales
4. ‚úÖ Retornar task_id para polling

---

## ‚öôÔ∏è Celery Tasks: Procesamiento As√≠ncrono

### 5. Task Principal: Actualizar Empleados
**Archivo**: `/backend/nomina/tasks_refactored/libro_remuneraciones.py`

```python
@shared_task(bind=True, queue='nomina_queue')
def actualizar_empleados_desde_libro_optimizado(self, libro_id):
    """
    üìä Fase 1: Crear/actualizar EmpleadoCierre con Celery Chord
    Divide empleados en chunks y los procesa en paralelo
    """
    libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
    
    # Leer Excel
    df = pd.read_excel(libro.archivo.path, sheet_name=0, dtype=str)
    
    # Obtener columnas de empleado
    columnas_empleado = ["rut", "nombre_completo", "cargo", etc...]
    df_empleados = df[columnas_empleado].drop_duplicates(subset=['rut'])
    
    # üîÄ DIVIDIR EN CHUNKS PARA PROCESAMIENTO PARALELO
    CHUNK_SIZE = 10
    chunks = [
        df_empleados.iloc[i:i+CHUNK_SIZE]
        for i in range(0, len(df_empleados), CHUNK_SIZE)
    ]
    
    # üéº CREAR CHORD: Procesar chunks en paralelo + consolidar
    job = chord(
        [
            procesar_chunk_empleados_task.s(
                chunk.to_dict('records'),
                libro_id,
                idx
            )
            for idx, chunk in enumerate(chunks)
        ]
    )(consolidar_empleados_task.s(libro_id))  # üëà CALLBACK
    
    return {
        'libro_id': libro_id,
        'total_empleados': len(df_empleados),
        'num_chunks': len(chunks),
        'job_id': job.id
    }
```

### 6. Subtask: Procesar Chunk de Empleados
```python
@shared_task(queue='nomina_queue')
def procesar_chunk_empleados_task(empleados_data, libro_id, chunk_idx):
    """
    üë• Procesa un chunk de empleados en paralelo
    Cada worker procesa ~10 empleados simult√°neamente
    """
    libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
    cierre = libro.cierre
    
    empleados_creados = []
    for emp_data in empleados_data:
        # Crear o actualizar EmpleadoCierre
        empleado, created = EmpleadoCierre.objects.update_or_create(
            cierre=cierre,
            rut=emp_data['rut'],
            defaults={
                'nombre_completo': emp_data['nombre_completo'],
                'cargo': emp_data.get('cargo'),
                'area': emp_data.get('area'),
                # ... otros campos
            }
        )
        empleados_creados.append(empleado.id)
    
    logger.info(f"‚úÖ Chunk {chunk_idx}: {len(empleados_creados)} empleados procesados")
    return empleados_creados
```

### 7. Callback: Consolidar Empleados
```python
@shared_task(queue='nomina_queue')
def consolidar_empleados_task(resultados_chunks, libro_id):
    """
    üîó Consolidar resultados de todos los chunks
    Se ejecuta cuando TODOS los chunks terminaron
    """
    libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
    
    # Aplanar lista de IDs
    todos_empleados = []
    for chunk_result in resultados_chunks:
        todos_empleados.extend(chunk_result)
    
    logger.info(f"‚úÖ Consolidaci√≥n: {len(todos_empleados)} empleados totales")
    
    return {
        'libro_id': libro_id,
        'empleados_ids': todos_empleados,
        'total': len(todos_empleados)
    }
```

### 8. Task Principal 2: Guardar Registros de Conceptos
```python
@shared_task(bind=True, queue='nomina_queue')
def guardar_registros_nomina_optimizado(self, resultado_empleados):
    """
    üí∞ Fase 2: Crear RegistroConceptoEmpleado con Celery Chord
    Procesa conceptos de n√≥mina en paralelo por chunks
    """
    libro_id = resultado_empleados['libro_id']
    libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
    
    # Leer Excel completo
    df = pd.read_excel(libro.archivo.path, sheet_name=0, dtype=str)
    
    # Obtener columnas de conceptos (excluyendo columnas de empleado)
    columnas_conceptos = [col for col in df.columns if col not in COLUMNAS_EMPLEADO]
    
    # üîÄ DIVIDIR EN CHUNKS POR EMPLEADO
    CHUNK_SIZE = 10
    ruts_unicos = df['rut'].unique()
    chunks = [
        ruts_unicos[i:i+CHUNK_SIZE]
        for i in range(0, len(ruts_unicos), CHUNK_SIZE)
    ]
    
    # üéº CREAR CHORD: Procesar registros en paralelo
    job = chord(
        [
            procesar_chunk_registros_task.s(
                chunk.tolist(),
                df[df['rut'].isin(chunk)].to_dict('records'),
                libro_id,
                idx
            )
            for idx, chunk in enumerate(chunks)
        ]
    )(consolidar_registros_task.s(libro_id))  # üëà CALLBACK FINAL
    
    return {
        'libro_id': libro_id,
        'total_registros_esperados': len(df) * len(columnas_conceptos),
        'job_id': job.id
    }
```

### 9. Subtask: Procesar Chunk de Registros
```python
@shared_task(queue='nomina_queue')
def procesar_chunk_registros_task(ruts_chunk, registros_data, libro_id, chunk_idx):
    """
    üí∏ Procesa registros de conceptos para un chunk de empleados
    """
    libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
    cierre = libro.cierre
    
    registros_creados = 0
    
    for registro in registros_data:
        rut = registro['rut']
        
        # Obtener empleado
        try:
            empleado = EmpleadoCierre.objects.get(cierre=cierre, rut=rut)
        except EmpleadoCierre.DoesNotExist:
            logger.warning(f"‚ö†Ô∏è Empleado {rut} no encontrado en chunk {chunk_idx}")
            continue
        
        # Crear registros de conceptos
        for concepto, valor in registro.items():
            if concepto in COLUMNAS_EMPLEADO:
                continue  # Saltar columnas de empleado
            
            if valor and str(valor).strip():
                RegistroConceptoEmpleado.objects.create(
                    empleado=empleado,
                    concepto=concepto,
                    valor=float(valor) if valor else 0,
                    periodo=cierre.periodo
                )
                registros_creados += 1
    
    logger.info(f"‚úÖ Chunk {chunk_idx}: {registros_creados} registros de conceptos creados")
    return registros_creados
```

### 10. Callback Final: Consolidar y Finalizar
```python
@shared_task(queue='nomina_queue')
def consolidar_registros_task(resultados_chunks, libro_id):
    """
    üéâ Consolidaci√≥n FINAL: Actualizar estado del libro
    Se ejecuta cuando TODO el procesamiento termin√≥
    """
    libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
    
    # Sumar todos los registros creados
    total_registros = sum(resultados_chunks)
    
    # ‚úÖ ACTUALIZAR ESTADO FINAL
    libro.estado = 'procesado'
    libro.save(update_fields=['estado'])
    
    logger.info(f"üéâ Libro {libro_id} COMPLETADO: {total_registros} registros totales")
    
    return {
        'libro_id': libro_id,
        'estado_final': 'procesado',
        'total_registros': total_registros
    }
```

---

## üíæ Modelos de Base de Datos

### Modelos Involucrados

#### 1. `LibroRemuneracionesUpload`
```python
# Estado cambia: 'clasificado' ‚Üí 'procesando' ‚Üí 'procesado'
{
    'id': 78,
    'cierre': ForeignKey(CierreNomina),
    'archivo': FileField,  # Excel subido
    'estado': 'procesado',
    'header_json': {...},  # Headers clasificados
    'fecha_upload': datetime
}
```

#### 2. `EmpleadoCierre`
```python
# Se crean/actualizan en Fase 1
{
    'id': 123,
    'cierre': ForeignKey(CierreNomina),
    'rut': '12345678-9',
    'nombre_completo': 'JUAN PEREZ',
    'cargo': 'ANALISTA',
    'area': 'OPERACIONES',
    'centro_costo': 'CC-100'
}
```

#### 3. `RegistroConceptoEmpleado`
```python
# Se crean en Fase 2
{
    'id': 456,
    'empleado': ForeignKey(EmpleadoCierre),
    'concepto': 'SUELDO BASE',
    'valor': 1500000.00,
    'periodo': '2025-10'
}
```

---

## üîÑ Diagrama de Flujo Completo

```
USUARIO                    FRONTEND                    API                    BACKEND                    CELERY
  |                           |                        |                         |                          |
  | 1. Click "Procesar"       |                        |                         |                          |
  |-------------------------->|                        |                         |                          |
  |                           |                        |                         |                          |
  |                           | 2. POST /procesar/     |                         |                          |
  |                           |----------------------->|                         |                          |
  |                           |                        |                         |                          |
  |                           |                        | 3. ViewSet.procesar()   |                          |
  |                           |                        |------------------------>|                          |
  |                           |                        |                         |                          |
  |                           |                        |                         | 4. Chain.apply_async()   |
  |                           |                        |                         |------------------------->|
  |                           |                        |                         |                          |
  |                           | 5. Response task_id    |                         |                          |
  |                           |<-----------------------|                         |                          |
  |                           |                        |                         |                          |
  | 6. Polling cada 2s        |                        |                         |                          |
  |<------------------------->|                        |                         |                          |
  |                           |                        |                         |                          |
  |                           |                        |                         | 7. FASE 1: Empleados     |
  |                           |                        |                         |                          |
  |                           |                        |                         |   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>| Chunk 1 (10 emp)
  |                           |                        |                         |   | Chord paralelo       |
  |                           |                        |                         |   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>| Chunk 2 (10 emp)
  |                           |                        |                         |   |                      |
  |                           |                        |                         |   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>| Chunk N
  |                           |                        |                         |   |                      |
  |                           |                        |                         |   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|
  |                           |                        |                         |          ‚Üì               |
  |                           |                        |                         |   Consolidar empleados   |
  |                           |                        |                         |<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|
  |                           |                        |                         |                          |
  |                           |                        |                         | 8. FASE 2: Conceptos     |
  |                           |                        |                         |                          |
  |                           |                        |                         |   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>| Chunk 1 (conceptos)
  |                           |                        |                         |   | Chord paralelo       |
  |                           |                        |                         |   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>| Chunk 2 (conceptos)
  |                           |                        |                         |   |                      |
  |                           |                        |                         |   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|
  |                           |                        |                         |          ‚Üì               |
  |                           |                        |                         |   Consolidar registros   |
  |                           |                        |                         |<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|
  |                           |                        |                         |                          |
  |                           |                        |                         | 9. Estado = 'procesado'  |
  |                           |                        |                         |------------------------->|
  |                           |                        |                         |                          |
  | 10. Polling detecta       |                        |                         |                          |
  |     estado = 'procesado'  |                        |                         |                          |
  |<--------------------------|                        |                         |                          |
  |                           |                        |                         |                          |
  | 11. UI actualizada ‚úÖ     |                        |                         |                          |
  |                           |                        |                         |                          |
```

---

## üìä M√©tricas de Rendimiento

**Caso de prueba: Libro 78**
- Empleados: 62
- Conceptos por empleado: 48
- Registros totales: 2,976

**Tiempos**:
- Fase 1 (Empleados): ~10-15 segundos
- Fase 2 (Conceptos): ~20-30 segundos
- **Total**: ~35-45 segundos

**Optimizaci√≥n con Chord**:
- Sin paralelizaci√≥n: ~90 segundos
- Con paralelizaci√≥n (10 chunks): ~40 segundos
- **Mejora**: 55% m√°s r√°pido

---

## ‚úÖ Validaciones

1. **Pre-procesamiento**:
   - ‚úÖ Libro debe estar en estado `'clasificado'`
   - ‚úÖ Headers deben estar clasificados
   - ‚úÖ Archivo Excel debe existir

2. **Durante procesamiento**:
   - ‚úÖ RUT √∫nico por empleado
   - ‚úÖ Valores num√©ricos v√°lidos para conceptos
   - ‚úÖ Chunks se procesan sin overlapping

3. **Post-procesamiento**:
   - ‚úÖ Todos los empleados creados
   - ‚úÖ Todos los conceptos guardados
   - ‚úÖ Estado actualizado correctamente

---

## ‚ùå Casos de Error

| Error | Causa | Soluci√≥n |
|-------|-------|----------|
| `Libro no encontrado` | ID incorrecto | Verificar que libro existe |
| `Estado inv√°lido` | Libro ya procesado | Reprocesar solo si necesario |
| `Empleado duplicado` | RUT repetido en Excel | Limpiar datos de entrada |
| `Valor inv√°lido` | Concepto no num√©rico | Validar formato de columnas |
| `Chunk timeout` | Worker sobrecargado | Reducir CHUNK_SIZE |

---

## üîó Flujos Relacionados

- **Anterior**: FLUJO 0: Subir y Clasificar Headers
- **Siguiente**: FLUJO 5: Consolidar Informaci√≥n
- **Depende de**: CierreNomina debe existir

---

## üß™ Resultados del Smoke Test

**Estado**: ‚è≥ PENDIENTE - Por probar

**Funciones utilizadas**:
- [ ] `actualizar_empleados_desde_libro_optimizado` ‚úÖ Refactorizada
- [ ] `procesar_chunk_empleados_task` ‚úÖ Refactorizada
- [ ] `consolidar_empleados_task` ‚úÖ Refactorizada
- [ ] `guardar_registros_nomina_optimizado` ‚úÖ Refactorizada
- [ ] `procesar_chunk_registros_task` ‚úÖ Refactorizada
- [ ] `consolidar_registros_task` ‚úÖ Refactorizada

**Stubs llamados**: (Se completar√° despu√©s de la prueba)

**Errores encontrados**: (Se completar√° despu√©s de la prueba)

---

**Documentado por**: GitHub Copilot  
**√öltima actualizaci√≥n**: 24 octubre 2025
