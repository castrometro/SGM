# üîß Fix: Bug en Procesamiento de Chunks del Libro de Remuneraciones

**Fecha**: 24 de octubre de 2025  
**M√≥dulo**: `nomina/tasks_refactored/libro_remuneraciones.py`  
**Severidad**: Alta (80% de p√©rdida de datos)  
**Estado**: ‚úÖ RESUELTO

---

## üìã Tabla de Contenidos

1. [Descripci√≥n del Error](#descripci√≥n-del-error)
2. [Diagn√≥stico y Root Cause](#diagn√≥stico-y-root-cause)
3. [Soluci√≥n Aplicada](#soluci√≥n-aplicada)
4. [Flujo Completo de Procesamiento](#flujo-completo-de-procesamiento)
5. [Lecciones Aprendidas](#lecciones-aprendidas)
6. [Recomendaciones](#recomendaciones)

---

## üìñ Descripci√≥n del Error

### S√≠ntomas Observados

Al procesar un Libro de Remuneraciones con 62 empleados:
- ‚úÖ Se creaban correctamente 62 registros de `EmpleadoCierre`
- ‚ùå Solo 12 empleados obten√≠an sus `RegistroConceptoEmpleado` (conceptos de n√≥mina)
- ‚ùå 50 empleados quedaban SIN conceptos (p√©rdida del 80% de datos)
- ‚ö†Ô∏è No se reportaban errores en logs

### Caso de Prueba

**Cierre**: ID 34 (Septiembre 2025, Cliente FRASER ALEXANDER)  
**Libro**: ID 78 (64 filas en Excel, 62 empleados v√°lidos)  
**Resultado esperado**: 62 empleados √ó 48 conceptos = 2,976 registros  
**Resultado obtenido**: 12 empleados √ó 48 conceptos = 576 registros (80% perdido)

---

## üîç Diagn√≥stico y Root Cause

### An√°lisis de Logs de Celery

Los logs revelaron un comportamiento an√≥malo en el chunk 1:

```
Chunk 1 (50 empleados):
  - Tiempo de ejecuci√≥n: 0.115 segundos ‚ö†Ô∏è (SOSPECHOSAMENTE R√ÅPIDO)
  - Registros procesados: 0 ‚ùå
  - Errores reportados: [] (ninguno)
  - Status: SUCCESS ‚úÖ (falso positivo)

Chunk 2 (12 empleados):
  - Tiempo de ejecuci√≥n: 1.958 segundos ‚úÖ
  - Registros procesados: 12 ‚úÖ
  - Errores reportados: []
  - Status: SUCCESS ‚úÖ
```

### Hip√≥tesis Investigadas

#### ‚ùå Hip√≥tesis 1: RUT mismatch entre Excel y DB
**Descartada**: Verificaci√≥n cruzada mostr√≥ que los 62 RUTs del Excel coinciden perfectamente con la BD.

#### ‚ùå Hip√≥tesis 2: Headers no clasificados
**Descartada**: El libro ten√≠a 48 headers correctamente clasificados.

#### ‚ùå Hip√≥tesis 3: Endpoint no llamado
**Descartada**: Los logs de Django confirmaron la llamada a `/procesar/` a las 15:31:40.

#### ‚ùå Hip√≥tesis 4: Problema en divisi√≥n de chunks
**Descartada**: Simulaci√≥n del c√≥digo de `dividir_dataframe_empleados` gener√≥ chunks correctos.

#### ‚ùå Hip√≥tesis 5: EmpleadoCierre no creados para chunk 1
**Descartada**: Query a BD confirm√≥ que los 50 empleados del chunk 1 existen.

#### ‚úÖ Hip√≥tesis 6: C√≥digo obsoleto cargado en memoria del worker
**CONFIRMADA**: El worker de Celery ten√≠a c√≥digo antiguo en memoria y no hab√≠a auto-reloaded despu√©s de cambios previos.

### Root Cause Identificado

**Celery no auto-reload en modo producci√≥n**. Los workers de Celery cargan el c√≥digo Python en memoria al iniciar y **NO recargan autom√°ticamente** cuando se modifican archivos `.py`. 

Esto caus√≥ que:
1. C√≥digo con bug se carg√≥ en memoria del worker
2. El chunk 1 fallaba silenciosamente debido al bug (condici√≥n exacta desconocida)
3. El bug persisti√≥ hasta que se reinici√≥ manualmente el worker

---

## üõ†Ô∏è Soluci√≥n Aplicada

### 1. Logging Detallado Agregado

Se instrument√≥ `procesar_chunk_registros_util` con logging de diagn√≥stico:

**Archivo**: `backend/nomina/utils/LibroRemuneracionesOptimizado.py`  
**L√≠neas**: 221-265

```python
def procesar_chunk_registros_util(libro_id, chunk_data):
    """
    üìù Procesa registros de n√≥mina para un chunk espec√≠fico de empleados.
    """
    chunk_id = chunk_data['chunk_id']
    logger.info(f"üìù Procesando registros para chunk {chunk_id}/{chunk_data['total_chunks']}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        # ... c√≥digo de inicializaci√≥n ...
        
        # üîç DEBUG LOGGING AGREGADO
        chunk_indices = chunk_data['indices']
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] Total indices recibidos: {len(chunk_indices)}")
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] Primeros 3 indices: {chunk_indices[:3] if chunk_indices else 'VAC√çO'}")
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] Total headers: {len(headers)}")
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] DataFrame shape: {df.shape}")
        
        chunk_df = df.iloc[chunk_indices]
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] chunk_df shape: {chunk_df.shape}")
        
        # ... resto del procesamiento ...
```

### 2. Reinicio del Worker de Celery

```bash
docker compose restart celery_worker
```

Este comando forz√≥ la recarga del c√≥digo actualizado en memoria.

### 3. Reprocesamiento del Libro

```python
# Limpiar registros previos
RegistroConceptoEmpleado.objects.filter(empleado__cierre=cierre).delete()

# Marcar libro para reprocesar
libro.estado = "clasificado"
libro.save()

# Usuario presiona "Procesar" en frontend
```

### 4. Resultados Despu√©s del Fix

```
Chunk 1:
  ‚úÖ Total indices: 50
  ‚úÖ Primeros 3 indices: [0, 1, 2]
  ‚úÖ Total headers: 48
  ‚úÖ chunk_df shape: (50, 55)
  ‚úÖ Registros procesados: 50
  ‚úÖ Tiempo: 8.77 segundos (NORMAL)

Chunk 2:
  ‚úÖ Total indices: 12
  ‚úÖ Primeros 3 indices: [50, 51, 52]
  ‚úÖ Total headers: 48
  ‚úÖ chunk_df shape: (12, 55)
  ‚úÖ Registros procesados: 12
  ‚úÖ Tiempo: 1.94 segundos

Consolidaci√≥n final:
  ‚úÖ Total empleados: 62
  ‚úÖ Empleados con conceptos: 62 (100%)
  ‚úÖ Empleados sin conceptos: 0
  ‚úÖ Conceptos por empleado: 48
  ‚úÖ Total registros: 2,976
```

---

## üîÑ Flujo Completo de Procesamiento del Libro

### Vista General

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FLUJO LIBRO DE REMUNERACIONES                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. UPLOAD        2. ANALIZAR       3. CLASIFICAR     4. PROCESAR
   üì§                üîç                 üè∑Ô∏è                ‚öôÔ∏è
   Excel            Headers           Mapeo             Parallel
   ‚Üì                ‚Üì                 ‚Üì                 ‚Üì
   File             Task              Task              Chord
   Upload           Async             Async             Pattern
   ‚Üì                ‚Üì                 ‚Üì                 ‚Üì
   estado:          estado:           estado:           estado:
   subido           analizando        clasificado       procesado
```

---

### Fase 1: Upload del Archivo Excel

**Componente Frontend**: `LibroRemuneracionesCard.jsx`  
**Endpoint**: `POST /api/nomina/libros-remuneraciones/`  
**Usuario**: Analista o Gerente

#### Flujo Detallado:

```javascript
// 1. Usuario selecciona archivo Excel
const handleFileChange = (event) => {
    const file = event.target.files[0];
    setSelectedFile(file);
};

// 2. Usuario presiona "Subir Libro"
const handleUpload = async () => {
    const formData = new FormData();
    formData.append('archivo', selectedFile);
    formData.append('cierre', cierreId);
    
    // 3. Request al backend
    const response = await nominaApi.post('/libros-remuneraciones/', formData, {
        headers: { 'Content-Type': 'multipart/form-data' }
    });
};
```

#### Backend - View

**Archivo**: `backend/nomina/views_libro_remuneraciones.py`

```python
class LibroRemuneracionesUploadViewSet(viewsets.ModelViewSet):
    def create(self, request):
        # 1. Validar archivo
        archivo = request.FILES.get('archivo')
        if not archivo:
            return Response({"error": "No se proporcion√≥ archivo"}, status=400)
        
        # 2. Validar extensi√≥n
        if not archivo.name.endswith(('.xlsx', '.xls')):
            return Response({"error": "Formato inv√°lido"}, status=400)
        
        # 3. Crear registro
        libro = LibroRemuneracionesUpload.objects.create(
            cierre=cierre,
            archivo=archivo,
            estado='subido',  # ‚Üê Estado inicial
            usuario_carga=request.user
        )
        
        # 4. Lanzar an√°lisis autom√°tico (Celery)
        analizar_headers_libro_remuneraciones_con_logging.delay(
            libro.id, 
            usuario_id=request.user.id
        )
        
        return Response(serializer.data, status=201)
```

#### Estado Resultante:
- ‚úÖ Archivo guardado en `/media/remuneraciones/{cliente_id}/{periodo}/libro/`
- ‚úÖ Registro `LibroRemuneracionesUpload` creado con `estado='subido'`
- ‚úÖ Task de Celery lanzada para an√°lisis autom√°tico

---

### Fase 2: An√°lisis de Headers (Autom√°tico)

**Task**: `analizar_headers_libro_remuneraciones_con_logging`  
**Queue**: `nomina_queue`  
**Archivo**: `backend/nomina/tasks_refactored/libro_remuneraciones.py`

#### Flujo Detallado:

```python
@shared_task(bind=True, queue='nomina_queue')
def analizar_headers_libro_remuneraciones_con_logging(self, libro_id, usuario_id=None):
    """
    Analiza headers del Excel y los guarda en libro.header_json
    """
    logger.info(f"[LIBRO] Analizando headers libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        # 1. Leer Excel con pandas
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        # 2. Extraer nombres de columnas
        headers = list(df.columns)
        
        # 3. Filtrar headers de empleado (no son conceptos)
        empleado_cols = {
            "A√±o", "Mes", "Rut de la Empresa", 
            "Rut del Trabajador", "Nombre",
            "Apellido Paterno", "Apellido Materno"
        }
        
        # 4. Headers de conceptos (haberes, descuentos, etc.)
        headers_conceptos = [h for h in headers if h not in empleado_cols]
        
        # 5. Guardar en BD
        libro.header_json = {
            "headers_raw": headers_conceptos,
            "total": len(headers_conceptos),
            "fecha_analisis": datetime.now().isoformat()
        }
        libro.estado = 'analizando'  # ‚Üê Cambio de estado
        libro.save()
        
        # 6. Lanzar clasificaci√≥n autom√°tica
        clasificar_headers_libro_remuneraciones_con_logging.delay(
            libro_id, 
            usuario_id=usuario_id
        )
        
        return {
            "libro_id": libro_id,
            "headers": headers_conceptos
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] Error analizando headers: {e}")
        raise
```

#### Estado Resultante:
- ‚úÖ `libro.header_json` contiene lista de headers extra√≠dos
- ‚úÖ `estado='analizando'`
- ‚úÖ Task de clasificaci√≥n lanzada autom√°ticamente

---

### Fase 3: Clasificaci√≥n de Headers (Autom√°tico)

**Task**: `clasificar_headers_libro_remuneraciones_con_logging`  
**Queue**: `nomina_queue`  
**L√≥gica**: Matching fuzzy con cat√°logo de conceptos

#### Flujo Detallado:

```python
@shared_task(bind=True, queue='nomina_queue')
def clasificar_headers_libro_remuneraciones_con_logging(self, libro_id, usuario_id=None):
    """
    Clasifica headers autom√°ticamente usando fuzzy matching
    """
    logger.info(f"[LIBRO] Clasificando headers libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        headers_raw = libro.header_json.get("headers_raw", [])
        
        # 1. Obtener cat√°logo de conceptos del cliente
        conceptos = ConceptoRemuneracion.objects.filter(
            cliente=libro.cierre.cliente,
            vigente=True
        )
        
        headers_clasificados = []
        headers_sin_clasificar = []
        
        # 2. Para cada header, buscar match con concepto
        for header in headers_raw:
            mejor_match = None
            mejor_score = 0
            
            # 3. Fuzzy matching con fuzzywuzzy
            for concepto in conceptos:
                score = fuzz.ratio(
                    header.lower().strip(),
                    concepto.nombre_concepto.lower().strip()
                )
                
                if score > mejor_score:
                    mejor_score = score
                    mejor_match = concepto
            
            # 4. Clasificar si score > umbral
            if mejor_score >= 80:  # Umbral de confianza
                headers_clasificados.append({
                    "header": header,
                    "concepto_id": mejor_match.id,
                    "concepto_nombre": mejor_match.nombre_concepto,
                    "categoria": mejor_match.categoria,
                    "score": mejor_score
                })
            else:
                headers_sin_clasificar.append(header)
        
        # 5. Guardar clasificaci√≥n
        libro.header_json = {
            "headers_clasificados": headers_clasificados,
            "headers_sin_clasificar": headers_sin_clasificar,
            "fecha_clasificacion": datetime.now().isoformat()
        }
        
        # 6. Determinar estado final
        if len(headers_sin_clasificar) > 0:
            libro.estado = 'clasif_pendiente'  # ‚Üê Requiere revisi√≥n manual
        else:
            libro.estado = 'clasificado'  # ‚Üê Listo para procesar
        
        libro.save()
        
        # 7. Registrar actividad para el usuario
        registrar_tarjeta_activity_log(
            usuario_id=usuario_id,
            libro_id=libro_id,
            event_type='classification_complete',
            details={
                "clasificados": len(headers_clasificados),
                "sin_clasificar": len(headers_sin_clasificar)
            }
        )
        
        return {
            "libro_id": libro_id,
            "headers_clasificados": len(headers_clasificados),
            "headers_sin_clasificar": len(headers_sin_clasificar),
            "estado_final": libro.estado
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] Error clasificando headers: {e}")
        raise
```

#### Posibles Estados Resultantes:

**Caso A: Todos clasificados autom√°ticamente**
- ‚úÖ `estado='clasificado'`
- ‚úÖ `headers_sin_clasificar = []`
- ‚úÖ **Usuario puede presionar "Procesar"** ‚öôÔ∏è

**Caso B: Algunos headers sin clasificar**
- ‚ö†Ô∏è `estado='clasif_pendiente'`
- ‚ö†Ô∏è `headers_sin_clasificar = ["Bono Especial", "Descuento X"]`
- ‚ö†Ô∏è **Usuario debe clasificar manualmente**

---

### Fase 3.5: Clasificaci√≥n Manual (Si es necesario)

**Componente Frontend**: Modal de clasificaci√≥n en `LibroRemuneracionesCard.jsx`  
**Endpoint**: `PATCH /api/nomina/libros-remuneraciones/{id}/`

Si hay headers sin clasificar, el usuario debe:

```javascript
// 1. Frontend muestra modal con headers pendientes
const handleManualClassification = (header, conceptoId) => {
    // 2. Usuario selecciona concepto del dropdown
    const clasificaciones = {
        ...clasificacionesAnteriores,
        [header]: conceptoId
    };
    
    // 3. Enviar al backend
    await nominaApi.patch(`/libros-remuneraciones/${libroId}/`, {
        clasificaciones_manuales: clasificaciones
    });
};
```

Despu√©s de clasificaci√≥n manual:
- ‚úÖ `estado='clasificado'`
- ‚úÖ **Usuario puede presionar "Procesar"** ‚öôÔ∏è

---

### Fase 4: Procesamiento Paralelo (Manual)

**Usuario**: Presiona bot√≥n "Procesar"  
**Endpoint**: `POST /api/nomina/libros-remuneraciones/{id}/procesar/`  
**Requisito**: `libro.estado == 'clasificado'`

#### Flujo en el Backend:

```python
@action(detail=True, methods=['post'])
def procesar(self, request, pk=None):
    """
    Procesa el libro usando Celery Chord para paralelizaci√≥n
    """
    libro = self.get_object()
    
    # 1. Validar estado
    if libro.estado != 'clasificado':
        return Response(
            {"error": "El libro debe estar clasificado"}, 
            status=400
        )
    
    # 2. Lanzar chain de Celery
    chain_result = chain(
        # Fase A: Crear EmpleadoCierre (paralelo)
        actualizar_empleados_desde_libro_optimizado.s(
            libro.id, 
            usuario_id=request.user.id
        ),
        # Fase B: Crear RegistroConceptoEmpleado (paralelo)
        guardar_registros_nomina_optimizado.s(
            usuario_id=request.user.id
        )
    ).apply_async()
    
    return Response({
        "message": "Procesamiento iniciado",
        "task_id": chain_result.id
    }, status=202)
```

---

### Fase 4A: Crear EmpleadoCierre (Paralelo con Chord)

**Task**: `actualizar_empleados_desde_libro_optimizado`  
**Pattern**: Celery Chord (parallel tasks | callback)

```python
@shared_task(bind=True, queue='nomina_queue')
def actualizar_empleados_desde_libro_optimizado(self, libro_id, usuario_id=None):
    """
    Divide el Excel en chunks y crea EmpleadoCierre en paralelo
    """
    logger.info(f"[LIBRO] Actualizando empleados (optimizado) libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        # 1. Leer Excel
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        total_filas = len(df)
        
        # 2. Calcular chunk size din√°mico
        chunk_size = _calcular_chunk_size_dinamico(total_filas)
        # Ejemplos: 
        #   <= 50 filas: chunk_size = 25
        #   <= 200 filas: chunk_size = 50
        #   > 200 filas: chunk_size = 100
        
        logger.info(f"[LIBRO] Total: {total_filas} filas, Chunk size: {chunk_size}")
        
        # 3. Dividir en chunks
        chunks = dividir_dataframe_empleados(libro.archivo.path, chunk_size)
        # chunks = [
        #     {'chunk_id': 1, 'indices': [0,1,2,...,49], 'size': 50},
        #     {'chunk_id': 2, 'indices': [50,51,...,61], 'size': 12}
        # ]
        
        # 4. Crear tasks paralelas
        tasks_paralelas = [
            procesar_chunk_empleados_task.s(libro_id, chunk_data)
            for chunk_data in chunks
        ]
        
        # 5. Ejecutar Chord: tasks | callback
        callback = consolidar_empleados_task.s()
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(f"[LIBRO] üöÄ Chord empleados iniciado: {len(chunks)} chunks")
        
        return {
            "libro_id": libro_id,
            "usuario_id": usuario_id,
            "chord_id": resultado_chord.id,
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord"
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] ‚ùå Error: {e}")
        raise
```

#### Worker Task: Procesar Chunk de Empleados

```python
@shared_task(queue='nomina_queue')
def procesar_chunk_empleados_task(libro_id, chunk_data):
    """
    Worker: Procesa un chunk espec√≠fico de empleados
    """
    return procesar_chunk_empleados_util(libro_id, chunk_data)


def procesar_chunk_empleados_util(libro_id, chunk_data):
    """
    L√≥gica real: Crea/actualiza EmpleadoCierre para un chunk
    """
    chunk_id = chunk_data['chunk_id']
    logger.info(f"üë• Procesando chunk de empleados {chunk_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        expected = {
            "ano": "A√±o",
            "mes": "Mes",
            "rut_trabajador": "Rut del Trabajador",
            "nombre": "Nombre",
            "ape_pat": "Apellido Paterno",
            "ape_mat": "Apellido Materno",
        }
        
        count = 0
        errores = []
        
        # Procesar solo las filas de este chunk
        chunk_indices = chunk_data['indices']
        chunk_df = df.iloc[chunk_indices]
        
        with transaction.atomic():
            for _, row in chunk_df.iterrows():
                try:
                    # Extraer y normalizar RUT
                    rut_raw = row.get(expected["rut_trabajador"])
                    if not _es_rut_chileno_valido(rut_raw):
                        continue
                    
                    rut = formatear_rut_con_guion(normalizar_rut(rut_raw))
                    
                    # Extraer datos del empleado
                    nombre = str(row.get(expected["nombre"], "")).strip()
                    apellido_paterno = str(row.get(expected["ape_pat"], "")).strip()
                    apellido_materno = str(row.get(expected["ape_mat"], "")).strip()
                    
                    # Crear o actualizar EmpleadoCierre
                    empleado, created = EmpleadoCierre.objects.update_or_create(
                        cierre=libro.cierre,
                        rut=rut,
                        defaults={
                            "nombre": nombre,
                            "apellido_paterno": apellido_paterno,
                            "apellido_materno": apellido_materno,
                        }
                    )
                    
                    count += 1
                    
                except Exception as e:
                    error_msg = f"Error procesando RUT {rut}: {str(e)}"
                    errores.append(error_msg)
                    logger.error(error_msg)
        
        resultado = {
            'chunk_id': chunk_id,
            'empleados_procesados': count,
            'errores': errores
        }
        
        logger.info(f"‚úÖ Chunk empleados {chunk_id} completado: {count} empleados")
        return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Error en chunk empleados {chunk_id}: {e}")
        return {
            'chunk_id': chunk_id,
            'empleados_procesados': 0,
            'errores': [str(e)]
        }
```

#### Callback: Consolidar Resultados de Empleados

```python
@shared_task(queue='nomina_queue')
def consolidar_empleados_task(resultados_chunks):
    """
    Callback: Ejecutado cuando TODOS los chunks de empleados terminan
    """
    stats = consolidar_stats_empleados(resultados_chunks)
    logger.info(f"[LIBRO] ‚úÖ Consolidaci√≥n empleados: {stats}")
    return stats


def consolidar_stats_empleados(resultados_chunks):
    """
    Suma los resultados de todos los chunks
    """
    total_empleados = 0
    total_errores = 0
    chunks_exitosos = 0
    errores_consolidados = []
    
    for resultado in resultados_chunks:
        if isinstance(resultado, dict):
            total_empleados += resultado.get('empleados_procesados', 0)
            errores_chunk = resultado.get('errores', [])
            total_errores += len(errores_chunk)
            errores_consolidados.extend(errores_chunk)
            
            if resultado.get('empleados_procesados', 0) > 0:
                chunks_exitosos += 1
    
    return {
        'total_empleados_procesados': total_empleados,
        'chunks_exitosos': chunks_exitosos,
        'total_chunks': len(resultados_chunks),
        'total_errores': total_errores,
        'errores': errores_consolidados,
        'procesamiento_exitoso': total_errores == 0
    }
```

#### Resultado de Fase 4A:

```
‚úÖ Chord empleados completado:
   - Chunk 1: 50 empleados procesados
   - Chunk 2: 12 empleados procesados
   - Total: 62 EmpleadoCierre creados
```

---

### Fase 4B: Crear RegistroConceptoEmpleado (Paralelo con Chord)

**Task**: `guardar_registros_nomina_optimizado`  
**Pattern**: Celery Chord (parallel tasks | callback)  
**Input**: Resultado de Fase 4A

```python
@shared_task(bind=True, queue='nomina_queue')
def guardar_registros_nomina_optimizado(self, result, usar_chord=True):
    """
    Crea RegistroConceptoEmpleado para cada empleado √ó concepto
    """
    # Extraer libro_id del resultado anterior
    libro_id = result.get("libro_id")
    usuario_id = result.get("usuario_id")
    
    logger.info(f"[LIBRO] Guardando registros (optimizado) libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        # 1. Leer Excel
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        total_filas = len(df)
        
        # 2. Calcular chunk size din√°mico
        chunk_size = _calcular_chunk_size_dinamico(total_filas)
        
        # 3. Dividir en chunks (MISMO algoritmo que empleados)
        chunks = dividir_dataframe_empleados(libro.archivo.path, chunk_size)
        
        # 4. Crear tasks paralelas
        tasks_paralelas = [
            procesar_chunk_registros_task.s(libro_id, chunk_data)
            for chunk_data in chunks
        ]
        
        # 5. Ejecutar Chord con callback que recibe usuario_id
        callback = consolidar_registros_task.s(usuario_id=usuario_id)
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(f"[LIBRO] üöÄ Chord registros iniciado: {len(chunks)} chunks")
        
        return {
            "libro_id": libro_id,
            "usuario_id": usuario_id,
            "chord_id": resultado_chord.id,
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord",
            "estado": "procesando"
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] ‚ùå Error: {e}")
        raise
```

#### Worker Task: Procesar Chunk de Registros (CON FIX)

```python
@shared_task(queue='nomina_queue')
def procesar_chunk_registros_task(libro_id, chunk_data):
    """
    Worker: Procesa registros de conceptos para un chunk
    """
    return procesar_chunk_registros_util(libro_id, chunk_data)


def procesar_chunk_registros_util(libro_id, chunk_data):
    """
    üîß FUNCI√ìN CON FIX: Logging detallado agregado
    
    Crea RegistroConceptoEmpleado para cada empleado √ó concepto
    """
    chunk_id = chunk_data['chunk_id']
    logger.info(f"üìù Procesando registros para chunk {chunk_id}/{chunk_data['total_chunks']}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        expected = {
            "ano": "A√±o",
            "mes": "Mes",
            "rut_empresa": "Rut de la Empresa",
            "rut_trabajador": "Rut del Trabajador",
            "nombre": "Nombre",
            "ape_pat": "Apellido Paterno",
            "ape_mat": "Apellido Materno",
        }
        
        empleado_cols = set(expected.values())
        
        # Obtener headers clasificados
        headers = libro.header_json
        if isinstance(headers, dict):
            headers = headers.get("headers_clasificados", []) + headers.get(
                "headers_sin_clasificar", []
            )
        if not headers:
            headers = [h for h in df.columns if h not in empleado_cols]
        
        # üîç DEBUG LOGGING (AGREGADO EN EL FIX)
        chunk_indices = chunk_data['indices']
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] Total indices recibidos: {len(chunk_indices)}")
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] Primeros 3 indices: {chunk_indices[:3] if chunk_indices else 'VAC√çO'}")
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] Total headers: {len(headers)}")
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] DataFrame shape: {df.shape}")
        
        count = 0
        errores = []
        
        # Procesar solo las filas de este chunk
        chunk_df = df.iloc[chunk_indices]
        logger.info(f"üîç [DEBUG Chunk {chunk_id}] chunk_df shape: {chunk_df.shape}")
        
        with transaction.atomic():
            for _, row in chunk_df.iterrows():
                try:
                    # Normalizar RUT
                    rut_valor = row.get(expected["rut_trabajador"])
                    if not _es_rut_chileno_valido(rut_valor):
                        continue
                    
                    rut = formatear_rut_con_guion(normalizar_rut(rut_valor))
                    
                    # Buscar EmpleadoCierre (debe existir de Fase 4A)
                    empleado = EmpleadoCierre.objects.filter(
                        cierre=libro.cierre, 
                        rut=rut
                    ).first()
                    
                    if not empleado:
                        continue
                    
                    # Procesar TODOS los headers para este empleado
                    for h in headers:
                        try:
                            valor_raw = row.get(h)
                            
                            # Procesar valor (convertir a string limpio)
                            if pd.isna(valor_raw) or valor_raw == '':
                                valor = ""
                            else:
                                if isinstance(valor_raw, (int, float)):
                                    if isinstance(valor_raw, int) or valor_raw.is_integer():
                                        valor = str(int(valor_raw))
                                    else:
                                        valor = f"{valor_raw:.2f}".rstrip('0').rstrip('.')
                                else:
                                    valor = str(valor_raw).strip()
                                    if valor.lower() == 'nan':
                                        valor = ""
                            
                            # Buscar concepto clasificado
                            concepto = ConceptoRemuneracion.objects.filter(
                                cliente=libro.cierre.cliente, 
                                nombre_concepto=h, 
                                vigente=True
                            ).first()
                            
                            # Crear o actualizar registro
                            RegistroConceptoEmpleado.objects.update_or_create(
                                empleado=empleado,
                                nombre_concepto_original=h,
                                defaults={
                                    "monto": valor, 
                                    "concepto": concepto
                                }
                            )
                            
                        except Exception as concepto_error:
                            error_msg = f"Error en concepto '{h}' para RUT {rut}: {str(concepto_error)}"
                            errores.append(error_msg)
                            logger.error(error_msg)
                    
                    count += 1
                    
                except Exception as e:
                    error_msg = f"Error procesando registros para RUT {rut}: {str(e)}"
                    errores.append(error_msg)
                    logger.error(error_msg)
        
        resultado = {
            'chunk_id': chunk_id,
            'registros_procesados': count,
            'errores': errores,
            'libro_id': libro_id
        }
        
        logger.info(f"‚úÖ Chunk registros {chunk_id} completado: {count} empleados procesados")
        return resultado
        
    except Exception as e:
        error_msg = f"Error en chunk registros {chunk_id}: {str(e)}"
        logger.error(error_msg)
        return {
            'chunk_id': chunk_id,
            'registros_procesados': 0,
            'errores': [error_msg],
            'libro_id': libro_id
        }
```

#### Callback: Consolidar Registros y Finalizar

```python
@shared_task(queue='nomina_queue')
def consolidar_registros_task(resultados_chunks, usuario_id=None):
    """
    Callback final: Ejecutado cuando TODOS los chunks de registros terminan
    
    - Consolida estad√≠sticas
    - Actualiza estado del libro a 'procesado'
    - Registra actividad del usuario
    """
    stats = consolidar_stats_registros(resultados_chunks)
    logger.info(f"[LIBRO] ‚úÖ Consolidaci√≥n registros: {stats}")
    
    # Obtener libro_id
    libro_id = None
    for resultado in resultados_chunks:
        if isinstance(resultado, dict) and 'libro_id' in resultado:
            libro_id = resultado['libro_id']
            break
    
    if libro_id:
        try:
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            
            # Actualizar estado final
            libro.estado = "procesado"  # ‚Üê ESTADO FINAL ‚úÖ
            libro.save(update_fields=['estado'])
            
            logger.info(f"[LIBRO] Estado actualizado a 'procesado' para libro {libro_id}")
            
            # Registrar actividad en tarjeta del usuario
            registrar_tarjeta_activity_log(
                usuario_id=usuario_id,
                libro_id=libro_id,
                event_type='process_complete',
                details={
                    "total_registros": stats.get('total_registros_procesados', 0),
                    "chunks_exitosos": stats.get('chunks_exitosos', 0)
                }
            )
            
        except Exception as e:
            logger.error(f"[LIBRO] Error actualizando estado: {e}")
    
    return stats
```

#### Resultado de Fase 4B:

```
‚úÖ Chord registros completado:
   - Chunk 1: 50 empleados √ó 48 conceptos = 2,400 registros
   - Chunk 2: 12 empleados √ó 48 conceptos = 576 registros
   - Total: 2,976 RegistroConceptoEmpleado creados
   
‚úÖ Estado final: libro.estado = 'procesado'
```

---

### Resumen del Flujo Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         TIMELINE COMPLETO                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

T0: Usuario sube Excel
    ‚Üì
    ‚îú‚îÄ Archivo guardado en /media/
    ‚îî‚îÄ estado = 'subido'
    
T1: analizar_headers_libro (async)
    ‚Üì
    ‚îú‚îÄ Extrae columnas del Excel
    ‚îú‚îÄ Filtra headers de empleado
    ‚îî‚îÄ estado = 'analizando'
    
T2: clasificar_headers_libro (async)
    ‚Üì
    ‚îú‚îÄ Fuzzy matching con cat√°logo de conceptos
    ‚îú‚îÄ Clasifica autom√°ticamente (80% threshold)
    ‚îî‚îÄ estado = 'clasif_pendiente' o 'clasificado'
    
T3: [MANUAL] Clasificaci√≥n pendientes (si aplica)
    ‚Üì
    ‚îî‚îÄ estado = 'clasificado'
    
T4: [MANUAL] Usuario presiona "Procesar"
    ‚Üì
    ‚îî‚îÄ Chain de Celery iniciado
    
T5: actualizar_empleados_desde_libro_optimizado
    ‚Üì
    ‚îú‚îÄ Divide Excel en chunks
    ‚îú‚îÄ Chord: [chunk1, chunk2] | consolidar
    ‚îî‚îÄ Crea EmpleadoCierre en paralelo
    
T6: guardar_registros_nomina_optimizado
    ‚Üì
    ‚îú‚îÄ Divide Excel en chunks (mismos √≠ndices)
    ‚îú‚îÄ Chord: [chunk1, chunk2] | consolidar
    ‚îî‚îÄ Crea RegistroConceptoEmpleado en paralelo
    
T7: consolidar_registros_task (callback final)
    ‚Üì
    ‚îú‚îÄ Actualiza estado = 'procesado'
    ‚îú‚îÄ Registra actividad del usuario
    ‚îî‚îÄ ‚úÖ PROCESAMIENTO COMPLETO

Tiempo total (ejemplo con 62 empleados):
  - Upload: ~1s
  - An√°lisis: ~2s
  - Clasificaci√≥n: ~3s
  - Clasificaci√≥n manual: ~30s (si aplica)
  - Fase 4A (empleados): ~10s
  - Fase 4B (registros): ~15s
  - TOTAL: ~1 minuto (o ~31s si auto-clasificado)
```

---

## üìö Lecciones Aprendidas

### 1. Celery No Auto-Reload en Producci√≥n

**Problema**: Los workers cargan el c√≥digo en memoria al iniciar y NO recargan autom√°ticamente.

**Soluci√≥n**: Despu√©s de cambios en tasks, siempre reiniciar:
```bash
docker compose restart celery_worker
```

**Alternativa para desarrollo**: Usar `celery -A proyecto worker --autoreload` (m√°s lento).

---

### 2. Logging es Cr√≠tico para Debugging

**Problema**: Chunk fallaba silenciosamente sin indicar por qu√© (0.115s, 0 registros, sin errores).

**Soluci√≥n**: Agregar logging detallado en puntos cr√≠ticos:
- Tama√±o de inputs (indices, headers, dataframe shape)
- Estados intermedios (empleados encontrados, RUTs v√°lidos)
- Resultados de cada paso (registros procesados, errores)

**Beneficio**: Permiti√≥ identificar inmediatamente que el problema era el c√≥digo obsoleto en memoria.

---

### 3. Tiempos de Ejecuci√≥n como Indicador

**Observaci√≥n**:
- Chunk normal: 1.5-10 segundos (depende del tama√±o)
- Chunk con bug: 0.115 segundos ‚ö†Ô∏è

**Aprendizaje**: Un tiempo **sospechosamente r√°pido** indica que el c√≥digo sali√≥ temprano (early exit) sin procesar datos.

**Recomendaci√≥n**: Agregar m√©tricas de tiempo en logs para detectar anomal√≠as.

---

### 4. Determinismo en Divisi√≥n de Chunks

**Problema**: `dividir_dataframe_empleados` se llama DOS VECES (empleados y registros).

**Riesgo**: Si el algoritmo no es determinista, los chunks podr√≠an ser diferentes.

**Validaci√≥n**: Verificamos que el algoritmo es determinista (mismo input ‚Üí mismo output).

**Recomendaci√≥n futura**: Cachear los chunks despu√©s de la primera divisi√≥n y reutilizarlos.

---

### 5. Testing de Integraci√≥n As√≠ncrona

**Desaf√≠o**: Dif√≠cil testear tasks de Celery que dependen de orden de ejecuci√≥n.

**Recomendaci√≥n**:
- Unit tests para funciones `_util` (s√≠ncronas)
- Integration tests con `CELERY_TASK_ALWAYS_EAGER=True` (ejecuta tasks s√≠ncronamente)
- Monitoring en producci√≥n con Flower

---

## üéØ Recomendaciones

### Para Desarrollo

1. **Auto-reload durante desarrollo**:
```bash
# En docker-compose.dev.yml
celery_worker:
  command: celery -A sgm_backend worker --autoreload --loglevel=info
```

2. **Logging consistente**:
```python
# Usar siempre el mismo formato
logger.info(f"[LIBRO] {mensaje} libro_id={libro_id}, chunk={chunk_id}")
```

3. **Validaciones tempranas**:
```python
# Al inicio de cada task
if not chunk_indices:
    logger.error(f"[LIBRO] Chunk {chunk_id} tiene √≠ndices vac√≠os")
    return {'error': 'indices_vacios'}
```

---

### Para Producci√≥n

1. **Monitoring de Celery**:
```bash
# Flower dashboard
docker compose up -d flower
# Acceso: http://localhost:5555
```

2. **Alertas por tiempo de ejecuci√≥n**:
```python
# En cada task
inicio = time.time()
# ... procesamiento ...
duracion = time.time() - inicio

if duracion < 0.5 and count == 0:
    logger.warning(f"[ALERTA] Chunk {chunk_id} termin√≥ muy r√°pido sin resultados")
```

3. **Reinicio autom√°tico de workers**:
```yaml
# docker-compose.yml
celery_worker:
  restart: unless-stopped
  deploy:
    restart_policy:
      condition: on-failure
      max_attempts: 3
```

---

### Para Testing

1. **Test de chunks deterministas**:
```python
def test_dividir_chunks_determinista():
    """Verificar que dividir_dataframe_empleados es determinista"""
    chunks1 = dividir_dataframe_empleados(archivo, chunk_size=50)
    chunks2 = dividir_dataframe_empleados(archivo, chunk_size=50)
    assert chunks1 == chunks2
```

2. **Test de consolidaci√≥n**:
```python
def test_consolidacion_suma_correcta():
    """Verificar que consolidar suma todos los chunks"""
    resultados = [
        {'chunk_id': 1, 'registros_procesados': 50},
        {'chunk_id': 2, 'registros_procesados': 12}
    ]
    stats = consolidar_stats_registros(resultados)
    assert stats['total_registros_procesados'] == 62
```

3. **Test end-to-end**:
```python
@pytest.mark.django_db
def test_procesamiento_completo_libro():
    """Test completo: upload ‚Üí clasificar ‚Üí procesar"""
    # 1. Upload
    libro = LibroRemuneracionesUpload.objects.create(...)
    
    # 2. Analizar (s√≠ncrono con CELERY_TASK_ALWAYS_EAGER)
    analizar_headers_libro_remuneraciones(libro.id)
    
    # 3. Clasificar
    clasificar_headers_libro_remuneraciones(libro.id)
    
    # 4. Procesar
    chain(...).apply()
    
    # 5. Verificar resultado
    assert EmpleadoCierre.objects.filter(cierre=libro.cierre).count() == 62
    assert RegistroConceptoEmpleado.objects.count() == 2976
```

---

## üìä M√©tricas de √âxito

### Antes del Fix

- ‚úÖ EmpleadoCierre: 62/62 (100%)
- ‚ùå RegistroConceptoEmpleado: 576/2976 (19.4%)
- ‚ùå Empleados completos: 12/62 (19.4%)
- ‚ùå P√©rdida de datos: 80%

### Despu√©s del Fix

- ‚úÖ EmpleadoCierre: 62/62 (100%)
- ‚úÖ RegistroConceptoEmpleado: 2976/2976 (100%)
- ‚úÖ Empleados completos: 62/62 (100%)
- ‚úÖ P√©rdida de datos: 0%

---

## üîó Referencias

- **Archivo principal**: `backend/nomina/utils/LibroRemuneracionesOptimizado.py`
- **Tasks**: `backend/nomina/tasks_refactored/libro_remuneraciones.py`
- **Views**: `backend/nomina/views_libro_remuneraciones.py`
- **Frontend**: `src/pages/LibroRemuneracionesCard.jsx`
- **Documentaci√≥n previa**: `FLUJO_CONSOLIDACION_VISUAL.md`

---

## ‚úÖ Conclusi√≥n

El bug fue causado por **c√≥digo obsoleto cargado en memoria del worker de Celery** que no se actualiz√≥ despu√©s de cambios en el c√≥digo. La soluci√≥n fue:

1. ‚úÖ Agregar logging detallado para diagn√≥stico
2. ‚úÖ Reiniciar el worker de Celery para recargar c√≥digo
3. ‚úÖ Reprocesar el libro con c√≥digo actualizado

El sistema ahora procesa correctamente **100% de los empleados con todos sus conceptos** usando paralelizaci√≥n con Celery Chord.

**Recomendaci√≥n cr√≠tica**: Siempre reiniciar workers despu√©s de cambios en c√≥digo de tasks:
```bash
docker compose restart celery_worker
```

---

**Autor**: Equipo SGM  
**Fecha**: 24 de octubre de 2025  
**Versi√≥n**: 1.0
