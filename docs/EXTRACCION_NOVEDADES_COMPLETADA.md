# Extracci√≥n M√≥dulo Novedades - Completada ‚úÖ

**Fecha:** 18 de octubre de 2025  
**M√≥dulo:** `backend/nomina/tasks_refactored/novedades.py`  
**Versi√≥n:** 2.3.0

---

## üìã Resumen

Se complet√≥ exitosamente la extracci√≥n del m√≥dulo **Novedades** desde el monol√≠tico `tasks.py` (5,289 l√≠neas) hacia un archivo modular dedicado con **dual logging** completo y **propagaci√≥n de usuario_id**.

### Tareas Extra√≠das: **11 tareas totales**

#### Grupo 1: An√°lisis y Clasificaci√≥n (Chain Inicial)
1. **`procesar_archivo_novedades_con_logging`** - Funci√≥n entry point (no @shared_task)
2. **`analizar_headers_archivo_novedades`** - Extrae headers del Excel
3. **`clasificar_headers_archivo_novedades_task`** - Clasifica headers autom√°ticamente

#### Grupo 2: Procesamiento Final Simple
4. **`actualizar_empleados_desde_novedades_task`** - Actualiza datos de empleados
5. **`guardar_registros_novedades_task`** - Guarda registros de n√≥mina

#### Grupo 3: Procesamiento Optimizado con Chord (Archivos grandes >50 filas)
6. **`actualizar_empleados_desde_novedades_task_optimizado`** - Versi√≥n con chord para empleados
7. **`guardar_registros_novedades_task_optimizado`** - Versi√≥n con chord para registros

#### Grupo 4: Procesamiento Paralelo en Chunks
8. **`procesar_chunk_empleados_novedades_task`** - Procesa un chunk de empleados
9. **`procesar_chunk_registros_novedades_task`** - Procesa un chunk de registros

#### Grupo 5: Consolidaci√≥n
10. **`consolidar_empleados_novedades_task`** - Callback del chord de empleados
11. **`finalizar_procesamiento_novedades_task`** - Callback del chord de registros

---

## üîÑ Workflow Completo

### Fase 1: Upload y An√°lisis Inicial
```
Usuario sube archivo
    ‚Üì
procesar_archivo_novedades_con_logging(archivo_id, usuario_id)
    ‚Üì
Chain:
    analizar_headers_archivo_novedades.s(archivo_id, usuario_id)
        ‚Üì
    clasificar_headers_archivo_novedades_task.s(usuario_id)
        ‚Üì
    Estado: 'clasificado' o 'clasif_pendiente'
```

### Fase 2A: Procesamiento Simple (archivos peque√±os ‚â§50 filas)
```
Usuario click "Procesar Final"
    ‚Üì
Chain:
    actualizar_empleados_desde_novedades_task.s({"archivo_id": X, "usuario_id": Y})
        ‚Üì
    guardar_registros_novedades_task.s()
        ‚Üì
    Estado: 'procesado'
```

### Fase 2B: Procesamiento Optimizado (archivos grandes >50 filas)
```
Usuario click "Procesar Final Optimizado"
    ‚Üì
Chain:
    actualizar_empleados_desde_novedades_task_optimizado.s({"archivo_id": X, "usuario_id": Y})
        ‚Üì (divide en chunks)
        Chord:
            [procesar_chunk_empleados_novedades_task.s(chunk_1),
             procesar_chunk_empleados_novedades_task.s(chunk_2),
             ...
             procesar_chunk_empleados_novedades_task.s(chunk_N)]
            ‚Üì (callback)
        consolidar_empleados_novedades_task.s()
        ‚Üì
    guardar_registros_novedades_task_optimizado.s()
        ‚Üì (divide en chunks)
        Chord:
            [procesar_chunk_registros_novedades_task.s(chunk_1),
             procesar_chunk_registros_novedades_task.s(chunk_2),
             ...
             procesar_chunk_registros_novedades_task.s(chunk_N)]
            ‚Üì (callback)
        finalizar_procesamiento_novedades_task.s(usuario_id=Y)
        ‚Üì
    Estado: 'procesado' / 'con_errores_parciales' / 'con_error'
```

---

## ‚úÖ Implementaci√≥n de Dual Logging

### Funciones Helper Creadas

#### 1. `log_process_start_novedades(archivo_id, fase, usuario_id, detalles_extra)`
- Registra inicio de fase en **TarjetaActivityLogNomina**
- Registra inicio de fase en **ActivityEvent**
- Fallback inteligente de usuario: `usuario_id ‚Üí sistema_user`

#### 2. `log_process_complete_novedades(archivo_id, fase, usuario_id, resultado, detalles_extra)`
- Registra completado en ambos sistemas
- Par√°metro `resultado`: 'exito', 'warning', 'error'
- Incluye timestamp y estad√≠sticas

### Puntos de Logging

| Fase | Event Type | Tarjeta | Ubicaci√≥n |
|------|-----------|---------|-----------|
| **Inicio procesamiento** | process_start | novedades | procesar_archivo_novedades_con_logging |
| **An√°lisis headers** | process_start ‚Üí process_complete | novedades | analizar_headers_archivo_novedades |
| **Clasificaci√≥n** | process_start ‚Üí process_complete | novedades | clasificar_headers_archivo_novedades_task |
| **Actualizaci√≥n empleados** | process_start ‚Üí process_complete | novedades | actualizar_empleados_desde_novedades_task |
| **Guardado registros** | process_start ‚Üí process_complete | novedades | guardar_registros_novedades_task |
| **Empleados optimizado** | process_start ‚Üí process_complete | novedades | actualizar_empleados_desde_novedades_task_optimizado |
| **Registros optimizado** | process_start ‚Üí process_complete | novedades | guardar_registros_novedades_task_optimizado |
| **Finalizaci√≥n paralela** | process_complete | novedades | finalizar_procesamiento_novedades_task |

---

## üîß Propagaci√≥n de usuario_id

### Estrategia Implementada

```python
# 1. Entry point recibe usuario_id
procesar_archivo_novedades_con_logging(archivo_id, usuario_id)

# 2. Primera tarea lo recibe como par√°metro
analizar_headers_archivo_novedades(archivo_id, usuario_id)
    return {"archivo_id": X, "headers": [...], "usuario_id": Y}

# 3. Segunda tarea lo extrae del result
clasificar_headers_archivo_novedades_task(result, usuario_id=None):
    if not usuario_id:
        usuario_id = result.get("usuario_id")
    return {"archivo_id": X, ..., "usuario_id": Y}

# 4. Se propaga a trav√©s de toda la cadena
actualizar_empleados_desde_novedades_task(result, usuario_id=None):
    if not usuario_id:
        usuario_id = result.get("usuario_id") if isinstance(result, dict) else None
```

### Fallback Inteligente en Logging

```python
if usuario_id:
    try:
        usuario = User.objects.get(id=usuario_id)
    except User.DoesNotExist:
        usuario = get_sistema_user()
else:
    usuario = get_sistema_user()
```

---

## üìÅ Archivos Modificados

### 1. **Creado:** `backend/nomina/tasks_refactored/novedades.py` (1,159 l√≠neas)
- 11 tareas con decorador `@shared_task(bind=True, queue='nomina_queue')`
- 1 funci√≥n entry point (no decorada)
- 2 funciones helper de logging dual
- Imports completos de utilities
- Documentaci√≥n exhaustiva con emojis

### 2. **Modificado:** `backend/nomina/tasks_refactored/__init__.py`
- Agregadas 11 exportaciones de novedades
- Actualizada versi√≥n: `2.2.0` ‚Üí `2.3.0`
- Actualizado `TAREAS_MIGRADAS['novedades']`: `False` ‚Üí `True`
- Total exports ahora: **23 tareas** (10 libro + 1 movimientos + 1 archivos_analista + 11 novedades)

### 3. **Modificado:** `backend/nomina/views_archivos_novedades.py`
- **L√≠nea 20:** Import cambiado a `tasks_refactored.novedades`
- **L√≠nea 163:** Upload action - usa `procesar_archivo_novedades_con_logging(id, user.id)`
- **L√≠nea 190:** Reprocesar action - usa `procesar_archivo_novedades_con_logging(id, user.id)`
- **L√≠nea 375-401:** procesar_final - usa tareas refactorizadas con usuario_id
- **L√≠nea 403-450:** procesar_final_optimizado - usa tareas refactorizadas con usuario_id
- **L√≠nea 434:** Agregado `usuario=request.user` en registro de actividad

---

## üéØ Caracter√≠sticas Especiales

### 1. **Procesamiento Adaptativo**
```python
# Archivos peque√±os (‚â§50 filas): procesamiento directo
if total_filas <= 50:
    count = actualizar_empleados_desde_novedades(archivo)
    return {"empleados_actualizados": count, "modo": "directo"}

# Archivos grandes (>50 filas): procesamiento paralelo con chord
chunks = dividir_dataframe_novedades(ruta_archivo, chunk_size)
tasks_paralelas = [procesar_chunk_empleados_novedades_task.s(id, chunk) for chunk in chunks]
callback = consolidar_empleados_novedades_task.s()
resultado_chord = chord(tasks_paralelas)(callback)
```

### 2. **Chunk Size Din√°mico**
```python
chunk_size = calcular_chunk_size_dinamico(total_filas)
# Peque√±o: 50-100 filas ‚Üí chunk_size = 50
# Mediano: 100-500 filas ‚Üí chunk_size = 100
# Grande: 500-1000 filas ‚Üí chunk_size = 200
# Muy grande: >1000 filas ‚Üí chunk_size = 250
```

### 3. **Manejo de Estados**
- `pendiente` ‚Üí Archivo subido, esperando procesamiento
- `analizando_hdrs` ‚Üí Extrayendo headers
- `hdrs_analizados` ‚Üí Headers extra√≠dos
- `clasif_en_proceso` ‚Üí Clasificando headers
- `clasif_pendiente` ‚Üí Headers sin clasificar (requiere intervenci√≥n)
- `clasificado` ‚Üí Listo para procesamiento final
- `procesado` ‚Üí Completado exitosamente
- `con_errores_parciales` ‚Üí Completado con algunos errores
- `con_error` ‚Üí Error total

### 4. **Estad√≠sticas Consolidadas**
```python
{
    'empleados_actualizados': 150,
    'registros_creados': 1200,
    'registros_actualizados': 50,
    'errores_totales': 3,
    'chunks_procesados': 10,
    'tiempo_total': 45.2,
    'modo': 'chord_paralelo'
}
```

---

## ‚öôÔ∏è Configuraci√≥n Celery

### Queue Utilizada
```python
@shared_task(bind=True, queue='nomina_queue')
```

### Workers Necesarios
```bash
# En celery_worker.sh debe incluir:
celery -A sgm_backend worker \
    -Q nomina_queue,general,contabilidad_queue \
    -n nomina_worker@%h \
    --concurrency=4
```

---

## üß™ Testing

### Test Manual Recomendado

1. **Upload y An√°lisis**
```bash
# 1. Subir archivo de novedades desde frontend
# 2. Verificar logs en TarjetaActivityLogNomina:
SELECT * FROM nomina_tarjetaactivitylognomina 
WHERE tarjeta = 'novedades' 
ORDER BY timestamp DESC;

# Esperar:
# - process_start: procesamiento_inicial
# - process_start: analisis_headers
# - process_complete: analisis_headers (exito)
# - process_start: clasificacion_headers
# - process_complete: clasificacion_headers (exito/warning)
```

2. **Procesamiento Final Simple**
```bash
# Click "Procesar Final" en frontend
# Verificar:
# - process_start: actualizacion_empleados
# - process_complete: actualizacion_empleados
# - process_start: guardado_registros
# - process_complete: guardado_registros
```

3. **Procesamiento Optimizado**
```bash
# Con archivo >50 filas, click "Procesar Final Optimizado"
# Verificar en Flower o logs:
# - Chord con N chunks de empleados
# - Consolidaci√≥n de empleados
# - Chord con N chunks de registros
# - Finalizaci√≥n con estad√≠sticas
```

### Validaci√≥n de usuario_id

```python
# Verificar que todos los logs tengan el usuario correcto
logs = TarjetaActivityLogNomina.objects.filter(
    tarjeta='novedades'
).exclude(usuario__username='sistema')

for log in logs:
    assert log.usuario is not None
    assert log.usuario.id != 1  # No deber√≠a ser Pablo Castro gen√©rico
    print(f"‚úÖ Log {log.id}: usuario={log.usuario.username}")
```

---

## üìä Impacto del Refactoring

### Antes
- ‚ùå 11 tareas dispersas en tasks.py (5,289 l√≠neas)
- ‚ùå Sin logging dual
- ‚ùå Sin propagaci√≥n de usuario
- ‚ùå Dif√≠cil mantenimiento

### Despu√©s
- ‚úÖ M√≥dulo dedicado novedades.py (1,159 l√≠neas)
- ‚úÖ Dual logging completo (8 puntos de logging)
- ‚úÖ Propagaci√≥n de usuario_id en toda la cadena
- ‚úÖ Documentaci√≥n exhaustiva
- ‚úÖ C√≥digo organizado por workflow

### Progreso General
- **Tareas migradas:** 23 / 59 (39%)
- **M√≥dulos completados:** 4 / 12
  - ‚úÖ Libro de Remuneraciones (10 tareas)
  - ‚úÖ Movimientos del Mes (1 tarea)
  - ‚úÖ Archivos Analista (1 tarea)
  - ‚úÖ Novedades (11 tareas)

---

## üöÄ Deployment

```bash
# Reiniciar servicios
cd /root/SGM
docker compose restart celery_worker django

# Verificar workers registrados
docker compose exec celery_worker celery -A sgm_backend inspect registered

# Deber√≠an aparecer las 11 nuevas tareas:
# - nomina.tasks_refactored.novedades.analizar_headers_archivo_novedades
# - nomina.tasks_refactored.novedades.clasificar_headers_archivo_novedades_task
# - ... (9 m√°s)
```

---

## üìù Notas Importantes

1. **No se elimin√≥ tasks.py:** El monolito original se mantiene intacto para views legacy
2. **Tarjeta 'novedades' ya exist√≠a:** No requiri√≥ modificaci√≥n del modelo
3. **Usuario_id en chord callback:** Se propaga expl√≠citamente: `finalizar_procesamiento_novedades_task.s(usuario_id=Y)`
4. **Imports de utilities:** Se mantienen desde utils (no refactorizados a√∫n)
5. **Procesamiento paralelo:** Solo se activa para archivos >50 filas

---

## üéâ Conclusi√≥n

La extracci√≥n del m√≥dulo **Novedades** fue exitosa. Es el m√≥dulo m√°s complejo hasta ahora por incluir:
- ‚úÖ Workflow con chain + chord anidados
- ‚úÖ Procesamiento adaptativo (simple vs paralelo)
- ‚úÖ 11 tareas con dual logging completo
- ‚úÖ Propagaci√≥n correcta de usuario_id
- ‚úÖ Consolidaci√≥n de estad√≠sticas de m√∫ltiples chunks

**Status:** ‚úÖ COMPLETADO  
**Listo para:** Testing en producci√≥n  
**Pr√≥ximo m√≥dulo:** Consolidaci√≥n (8 tareas estimadas)
