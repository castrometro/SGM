# ğŸ”§ Fix: Bug en Procesamiento de Chunks del Libro de Remuneraciones

**Fecha**: 24 de octubre de 2025  
**MÃ³dulo**: `nomina/tasks_refactored/libro_remuneraciones.py`  
**Severidad**: Alta (80% de pÃ©rdida de datos)  
**Estado**: âœ… RESUELTO

---

## ğŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n del Error](#descripciÃ³n-del-error)
2. [DiagnÃ³stico y Root Cause](#diagnÃ³stico-y-root-cause)
3. [SoluciÃ³n Aplicada](#soluciÃ³n-aplicada)
4. [Flujo Completo de Procesamiento](#flujo-completo-de-procesamiento)
5. [Lecciones Aprendidas](#lecciones-aprendidas)
6. [Recomendaciones](#recomendaciones)

---

## ğŸ“– DescripciÃ³n del Error

### SÃ­ntomas Observados

Al procesar un Libro de Remuneraciones con 62 empleados:
- âœ… Se creaban correctamente 62 registros de `EmpleadoCierre`
- âŒ Solo 12 empleados obtenÃ­an sus `RegistroConceptoEmpleado` (conceptos de nÃ³mina)
- âŒ 50 empleados quedaban SIN conceptos (pÃ©rdida del 80% de datos)
- âš ï¸ No se reportaban errores en logs

### Caso de Prueba

**Cierre**: ID 34 (Septiembre 2025, Cliente FRASER ALEXANDER)  
**Libro**: ID 78 (64 filas en Excel, 62 empleados vÃ¡lidos)  
**Resultado esperado**: 62 empleados Ã— 48 conceptos = 2,976 registros  
**Resultado obtenido**: 12 empleados Ã— 48 conceptos = 576 registros (80% perdido)

---

## ğŸ” DiagnÃ³stico y Root Cause

### AnÃ¡lisis de Logs de Celery

Los logs revelaron un comportamiento anÃ³malo en el chunk 1:

```
Chunk 1 (50 empleados):
  - Tiempo de ejecuciÃ³n: 0.115 segundos âš ï¸ (SOSPECHOSAMENTE RÃPIDO)
  - Registros procesados: 0 âŒ
  - Errores reportados: [] (ninguno)
  - Status: SUCCESS âœ… (falso positivo)

Chunk 2 (12 empleados):
  - Tiempo de ejecuciÃ³n: 1.958 segundos âœ…
  - Registros procesados: 12 âœ…
  - Errores reportados: []
  - Status: SUCCESS âœ…
```

### HipÃ³tesis Investigadas

#### âŒ HipÃ³tesis 1: RUT mismatch entre Excel y DB
**Descartada**: VerificaciÃ³n cruzada mostrÃ³ que los 62 RUTs del Excel coinciden perfectamente con la BD.

#### âŒ HipÃ³tesis 2: Headers no clasificados
**Descartada**: El libro tenÃ­a 48 headers correctamente clasificados.

#### âŒ HipÃ³tesis 3: Endpoint no llamado
**Descartada**: Los logs de Django confirmaron la llamada a `/procesar/` a las 15:31:40.

#### âŒ HipÃ³tesis 4: Problema en divisiÃ³n de chunks
**Descartada**: SimulaciÃ³n del cÃ³digo de `dividir_dataframe_empleados` generÃ³ chunks correctos.

#### âŒ HipÃ³tesis 5: EmpleadoCierre no creados para chunk 1
**Descartada**: Query a BD confirmÃ³ que los 50 empleados del chunk 1 existen.

#### âœ… HipÃ³tesis 6: CÃ³digo obsoleto cargado en memoria del worker
**CONFIRMADA**: El worker de Celery tenÃ­a cÃ³digo antiguo en memoria y no habÃ­a auto-reloaded despuÃ©s de cambios previos.

### Root Cause Identificado

**Celery no auto-reload en modo producciÃ³n**. Los workers de Celery cargan el cÃ³digo Python en memoria al iniciar y **NO recargan automÃ¡ticamente** cuando se modifican archivos `.py`. 

Esto causÃ³ que:
1. CÃ³digo con bug se cargÃ³ en memoria del worker
2. El chunk 1 fallaba silenciosamente debido al bug (condiciÃ³n exacta desconocida)
3. El bug persistiÃ³ hasta que se reiniciÃ³ manualmente el worker

---

## ğŸ› ï¸ SoluciÃ³n Aplicada

### 1. Logging Detallado Agregado

Se instrumentÃ³ `procesar_chunk_registros_util` con logging de diagnÃ³stico:

**Archivo**: `backend/nomina/utils/LibroRemuneracionesOptimizado.py`  
**LÃ­neas**: 221-265

```python
def procesar_chunk_registros_util(libro_id, chunk_data):
    """
    ğŸ“ Procesa registros de nÃ³mina para un chunk especÃ­fico de empleados.
    """
    chunk_id = chunk_data['chunk_id']
    logger.info(f"ğŸ“ Procesando registros para chunk {chunk_id}/{chunk_data['total_chunks']}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        # ... cÃ³digo de inicializaciÃ³n ...
        
        # ğŸ” DEBUG LOGGING AGREGADO
        chunk_indices = chunk_data['indices']
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] Total indices recibidos: {len(chunk_indices)}")
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] Primeros 3 indices: {chunk_indices[:3] if chunk_indices else 'VACÃO'}")
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] Total headers: {len(headers)}")
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] DataFrame shape: {df.shape}")
        
        chunk_df = df.iloc[chunk_indices]
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] chunk_df shape: {chunk_df.shape}")
        
        # ... resto del procesamiento ...
```

### 2. Reinicio del Worker de Celery

```bash
docker compose restart celery_worker
```

Este comando forzÃ³ la recarga del cÃ³digo actualizado en memoria.

### 3. Reprocesamiento del Libro

```python
# Limpiar registros previos
RegistroConceptoEmpleado.objects.filter(empleado__cierre=cierre).delete()

# Marcar libro para reprocesar
libro.estado = "clasificado"
libro.save()

# Usuario presiona "Procesar" en frontend
```

### 4. Resultados DespuÃ©s del Fix

```
Chunk 1:
  âœ… Total indices: 50
  âœ… Primeros 3 indices: [0, 1, 2]
  âœ… Total headers: 48
  âœ… chunk_df shape: (50, 55)
  âœ… Registros procesados: 50
  âœ… Tiempo: 8.77 segundos (NORMAL)

Chunk 2:
  âœ… Total indices: 12
  âœ… Primeros 3 indices: [50, 51, 52]
  âœ… Total headers: 48
  âœ… chunk_df shape: (12, 55)
  âœ… Registros procesados: 12
  âœ… Tiempo: 1.94 segundos

ConsolidaciÃ³n final:
  âœ… Total empleados: 62
  âœ… Empleados con conceptos: 62 (100%)
  âœ… Empleados sin conceptos: 0
  âœ… Conceptos por empleado: 48
  âœ… Total registros: 2,976
```

---

## ğŸ”„ Flujo Completo de Procesamiento del Libro

### Vista General

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUJO LIBRO DE REMUNERACIONES                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. UPLOAD        2. ANALIZAR       3. CLASIFICAR     4. PROCESAR
   ğŸ“¤                ğŸ”                 ğŸ·ï¸                âš™ï¸
   Excel            Headers           Mapeo             Parallel
   â†“                â†“                 â†“                 â†“
   File             Task              Task              Chord
   Upload           Async             Async             Pattern
   â†“                â†“                 â†“                 â†“
   estado:          estado:           estado:           estado:
   subido           analizando        clasificado       procesado
```

---

### Fase 1: Upload del Archivo Excel

**Componente Frontend**: `LibroRemuneracionesCard.jsx`  
**Endpoint**: `POST /api/nomina/libros-remuneraciones/`  
**Usuario**: Analista o Gerente

#### Flujo Detallado:

```javascript
// 1. Usuario selecciona archivo Excel
const handleFileChange = (event) => {
    const file = event.target.files[0];
    setSelectedFile(file);
};

// 2. Usuario presiona "Subir Libro"
const handleUpload = async () => {
    const formData = new FormData();
    formData.append('archivo', selectedFile);
    formData.append('cierre', cierreId);
    
    // 3. Request al backend
    const response = await nominaApi.post('/libros-remuneraciones/', formData, {
        headers: { 'Content-Type': 'multipart/form-data' }
    });
};
```

#### Backend - View

**Archivo**: `backend/nomina/views_libro_remuneraciones.py`

```python
class LibroRemuneracionesUploadViewSet(viewsets.ModelViewSet):
    def create(self, request):
        # 1. Validar archivo
        archivo = request.FILES.get('archivo')
        if not archivo:
            return Response({"error": "No se proporcionÃ³ archivo"}, status=400)
        
        # 2. Validar extensiÃ³n
        if not archivo.name.endswith(('.xlsx', '.xls')):
            return Response({"error": "Formato invÃ¡lido"}, status=400)
        
        # 3. Crear registro
        libro = LibroRemuneracionesUpload.objects.create(
            cierre=cierre,
            archivo=archivo,
            estado='subido',  # â† Estado inicial
            usuario_carga=request.user
        )
        
        # 4. Lanzar anÃ¡lisis automÃ¡tico (Celery)
        analizar_headers_libro_remuneraciones_con_logging.delay(
            libro.id, 
            usuario_id=request.user.id
        )
        
        return Response(serializer.data, status=201)
```

#### Estado Resultante:
- âœ… Archivo guardado en `/media/remuneraciones/{cliente_id}/{periodo}/libro/`
- âœ… Registro `LibroRemuneracionesUpload` creado con `estado='subido'`
- âœ… Task de Celery lanzada para anÃ¡lisis automÃ¡tico

---

### Fase 2: AnÃ¡lisis de Headers (AutomÃ¡tico)

**Task**: `analizar_headers_libro_remuneraciones_con_logging`  
**Queue**: `nomina_queue`  
**Archivo**: `backend/nomina/tasks_refactored/libro_remuneraciones.py`

#### Flujo Detallado:

```python
@shared_task(bind=True, queue='nomina_queue')
def analizar_headers_libro_remuneraciones_con_logging(self, libro_id, usuario_id=None):
    """
    Analiza headers del Excel y los guarda en libro.header_json
    """
    logger.info(f"[LIBRO] Analizando headers libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        # 1. Leer Excel con pandas
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        # 2. Extraer nombres de columnas
        headers = list(df.columns)
        
        # 3. Filtrar headers de empleado (no son conceptos)
        empleado_cols = {
            "AÃ±o", "Mes", "Rut de la Empresa", 
            "Rut del Trabajador", "Nombre",
            "Apellido Paterno", "Apellido Materno"
        }
        
        # 4. Headers de conceptos (haberes, descuentos, etc.)
        headers_conceptos = [h for h in headers if h not in empleado_cols]
        
        # 5. Guardar en BD
        libro.header_json = {
            "headers_raw": headers_conceptos,
            "total": len(headers_conceptos),
            "fecha_analisis": datetime.now().isoformat()
        }
        libro.estado = 'analizando'  # â† Cambio de estado
        libro.save()
        
        # 6. Lanzar clasificaciÃ³n automÃ¡tica
        clasificar_headers_libro_remuneraciones_con_logging.delay(
            libro_id, 
            usuario_id=usuario_id
        )
        
        return {
            "libro_id": libro_id,
            "headers": headers_conceptos
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] Error analizando headers: {e}")
        raise
```

#### Estado Resultante:
- âœ… `libro.header_json` contiene lista de headers extraÃ­dos
- âœ… `estado='analizando'`
- âœ… Task de clasificaciÃ³n lanzada automÃ¡ticamente

---

### Fase 3: ClasificaciÃ³n de Headers (AutomÃ¡tico)

**Task**: `clasificar_headers_libro_remuneraciones_con_logging`  
**Queue**: `nomina_queue`  
**LÃ³gica**: Matching fuzzy con catÃ¡logo de conceptos

#### Flujo Detallado:

```python
@shared_task(bind=True, queue='nomina_queue')
def clasificar_headers_libro_remuneraciones_con_logging(self, libro_id, usuario_id=None):
    """
    Clasifica headers automÃ¡ticamente usando fuzzy matching
    """
    logger.info(f"[LIBRO] Clasificando headers libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        headers_raw = libro.header_json.get("headers_raw", [])
        
        # 1. Obtener catÃ¡logo de conceptos del cliente
        conceptos = ConceptoRemuneracion.objects.filter(
            cliente=libro.cierre.cliente,
            vigente=True
        )
        
        headers_clasificados = []
        headers_sin_clasificar = []
        
        # 2. Para cada header, buscar match con concepto
        for header in headers_raw:
            mejor_match = None
            mejor_score = 0
            
            # 3. Fuzzy matching con fuzzywuzzy
            for concepto in conceptos:
                score = fuzz.ratio(
                    header.lower().strip(),
                    concepto.nombre_concepto.lower().strip()
                )
                
                if score > mejor_score:
                    mejor_score = score
                    mejor_match = concepto
            
            # 4. Clasificar si score > umbral
            if mejor_score >= 80:  # Umbral de confianza
                headers_clasificados.append({
                    "header": header,
                    "concepto_id": mejor_match.id,
                    "concepto_nombre": mejor_match.nombre_concepto,
                    "categoria": mejor_match.categoria,
                    "score": mejor_score
                })
            else:
                headers_sin_clasificar.append(header)
        
        # 5. Guardar clasificaciÃ³n
        libro.header_json = {
            "headers_clasificados": headers_clasificados,
            "headers_sin_clasificar": headers_sin_clasificar,
            "fecha_clasificacion": datetime.now().isoformat()
        }
        
        # 6. Determinar estado final
        if len(headers_sin_clasificar) > 0:
            libro.estado = 'clasif_pendiente'  # â† Requiere revisiÃ³n manual
        else:
            libro.estado = 'clasificado'  # â† Listo para procesar
        
        libro.save()
        
        # 7. Registrar actividad para el usuario
        registrar_tarjeta_activity_log(
            usuario_id=usuario_id,
            libro_id=libro_id,
            event_type='classification_complete',
            details={
                "clasificados": len(headers_clasificados),
                "sin_clasificar": len(headers_sin_clasificar)
            }
        )
        
        return {
            "libro_id": libro_id,
            "headers_clasificados": len(headers_clasificados),
            "headers_sin_clasificar": len(headers_sin_clasificar),
            "estado_final": libro.estado
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] Error clasificando headers: {e}")
        raise
```

#### Posibles Estados Resultantes:

**Caso A: Todos clasificados automÃ¡ticamente**
- âœ… `estado='clasificado'`
- âœ… `headers_sin_clasificar = []`
- âœ… **Usuario puede presionar "Procesar"** âš™ï¸

**Caso B: Algunos headers sin clasificar**
- âš ï¸ `estado='clasif_pendiente'`
- âš ï¸ `headers_sin_clasificar = ["Bono Especial", "Descuento X"]`
- âš ï¸ **Usuario debe clasificar manualmente**

---

### Fase 3.5: ClasificaciÃ³n Manual (Si es necesario)

**Componente Frontend**: Modal de clasificaciÃ³n en `LibroRemuneracionesCard.jsx`  
**Endpoint**: `PATCH /api/nomina/libros-remuneraciones/{id}/`

Si hay headers sin clasificar, el usuario debe:

```javascript
// 1. Frontend muestra modal con headers pendientes
const handleManualClassification = (header, conceptoId) => {
    // 2. Usuario selecciona concepto del dropdown
    const clasificaciones = {
        ...clasificacionesAnteriores,
        [header]: conceptoId
    };
    
    // 3. Enviar al backend
    await nominaApi.patch(`/libros-remuneraciones/${libroId}/`, {
        clasificaciones_manuales: clasificaciones
    });
};
```

DespuÃ©s de clasificaciÃ³n manual:
- âœ… `estado='clasificado'`
- âœ… **Usuario puede presionar "Procesar"** âš™ï¸

---

### Fase 4: Procesamiento Paralelo (Manual)

**Usuario**: Presiona botÃ³n "Procesar"  
**Endpoint**: `POST /api/nomina/libros-remuneraciones/{id}/procesar/`  
**Requisito**: `libro.estado == 'clasificado'`

#### Flujo en el Backend:

```python
@action(detail=True, methods=['post'])
def procesar(self, request, pk=None):
    """
    Procesa el libro usando Celery Chord para paralelizaciÃ³n
    """
    libro = self.get_object()
    
    # 1. Validar estado
    if libro.estado != 'clasificado':
        return Response(
            {"error": "El libro debe estar clasificado"}, 
            status=400
        )
    
    # 2. Lanzar chain de Celery
    chain_result = chain(
        # Fase A: Crear EmpleadoCierre (paralelo)
        actualizar_empleados_desde_libro_optimizado.s(
            libro.id, 
            usuario_id=request.user.id
        ),
        # Fase B: Crear RegistroConceptoEmpleado (paralelo)
        guardar_registros_nomina_optimizado.s(
            usuario_id=request.user.id
        )
    ).apply_async()
    
    return Response({
        "message": "Procesamiento iniciado",
        "task_id": chain_result.id
    }, status=202)
```

---

### Fase 4A: Crear EmpleadoCierre (Paralelo con Chord)

**Task**: `actualizar_empleados_desde_libro_optimizado`  
**Pattern**: Celery Chord (parallel tasks | callback)

```python
@shared_task(bind=True, queue='nomina_queue')
def actualizar_empleados_desde_libro_optimizado(self, libro_id, usuario_id=None):
    """
    Divide el Excel en chunks y crea EmpleadoCierre en paralelo
    """
    logger.info(f"[LIBRO] Actualizando empleados (optimizado) libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        # 1. Leer Excel
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        total_filas = len(df)
        
        # 2. Calcular chunk size dinÃ¡mico
        chunk_size = _calcular_chunk_size_dinamico(total_filas)
        # Ejemplos: 
        #   <= 50 filas: chunk_size = 25
        #   <= 200 filas: chunk_size = 50
        #   > 200 filas: chunk_size = 100
        
        logger.info(f"[LIBRO] Total: {total_filas} filas, Chunk size: {chunk_size}")
        
        # 3. Dividir en chunks
        chunks = dividir_dataframe_empleados(libro.archivo.path, chunk_size)
        # chunks = [
        #     {'chunk_id': 1, 'indices': [0,1,2,...,49], 'size': 50},
        #     {'chunk_id': 2, 'indices': [50,51,...,61], 'size': 12}
        # ]
        
        # 4. Crear tasks paralelas
        tasks_paralelas = [
            procesar_chunk_empleados_task.s(libro_id, chunk_data)
            for chunk_data in chunks
        ]
        
        # 5. Ejecutar Chord: tasks | callback
        callback = consolidar_empleados_task.s()
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(f"[LIBRO] ğŸš€ Chord empleados iniciado: {len(chunks)} chunks")
        
        return {
            "libro_id": libro_id,
            "usuario_id": usuario_id,
            "chord_id": resultado_chord.id,
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord"
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] âŒ Error: {e}")
        raise
```

#### Worker Task: Procesar Chunk de Empleados

```python
@shared_task(queue='nomina_queue')
def procesar_chunk_empleados_task(libro_id, chunk_data):
    """
    Worker: Procesa un chunk especÃ­fico de empleados
    """
    return procesar_chunk_empleados_util(libro_id, chunk_data)


def procesar_chunk_empleados_util(libro_id, chunk_data):
    """
    LÃ³gica real: Crea/actualiza EmpleadoCierre para un chunk
    """
    chunk_id = chunk_data['chunk_id']
    logger.info(f"ğŸ‘¥ Procesando chunk de empleados {chunk_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        expected = {
            "ano": "AÃ±o",
            "mes": "Mes",
            "rut_trabajador": "Rut del Trabajador",
            "nombre": "Nombre",
            "ape_pat": "Apellido Paterno",
            "ape_mat": "Apellido Materno",
        }
        
        count = 0
        errores = []
        
        # Procesar solo las filas de este chunk
        chunk_indices = chunk_data['indices']
        chunk_df = df.iloc[chunk_indices]
        
        with transaction.atomic():
            for _, row in chunk_df.iterrows():
                try:
                    # Extraer y normalizar RUT
                    rut_raw = row.get(expected["rut_trabajador"])
                    if not _es_rut_chileno_valido(rut_raw):
                        continue
                    
                    rut = formatear_rut_con_guion(normalizar_rut(rut_raw))
                    
                    # Extraer datos del empleado
                    nombre = str(row.get(expected["nombre"], "")).strip()
                    apellido_paterno = str(row.get(expected["ape_pat"], "")).strip()
                    apellido_materno = str(row.get(expected["ape_mat"], "")).strip()
                    
                    # Crear o actualizar EmpleadoCierre
                    empleado, created = EmpleadoCierre.objects.update_or_create(
                        cierre=libro.cierre,
                        rut=rut,
                        defaults={
                            "nombre": nombre,
                            "apellido_paterno": apellido_paterno,
                            "apellido_materno": apellido_materno,
                        }
                    )
                    
                    count += 1
                    
                except Exception as e:
                    error_msg = f"Error procesando RUT {rut}: {str(e)}"
                    errores.append(error_msg)
                    logger.error(error_msg)
        
        resultado = {
            'chunk_id': chunk_id,
            'empleados_procesados': count,
            'errores': errores
        }
        
        logger.info(f"âœ… Chunk empleados {chunk_id} completado: {count} empleados")
        return resultado
        
    except Exception as e:
        logger.error(f"âŒ Error en chunk empleados {chunk_id}: {e}")
        return {
            'chunk_id': chunk_id,
            'empleados_procesados': 0,
            'errores': [str(e)]
        }
```

#### Callback: Consolidar Resultados de Empleados

```python
@shared_task(queue='nomina_queue')
def consolidar_empleados_task(resultados_chunks):
    """
    Callback: Ejecutado cuando TODOS los chunks de empleados terminan
    """
    stats = consolidar_stats_empleados(resultados_chunks)
    logger.info(f"[LIBRO] âœ… ConsolidaciÃ³n empleados: {stats}")
    return stats


def consolidar_stats_empleados(resultados_chunks):
    """
    Suma los resultados de todos los chunks
    """
    total_empleados = 0
    total_errores = 0
    chunks_exitosos = 0
    errores_consolidados = []
    
    for resultado in resultados_chunks:
        if isinstance(resultado, dict):
            total_empleados += resultado.get('empleados_procesados', 0)
            errores_chunk = resultado.get('errores', [])
            total_errores += len(errores_chunk)
            errores_consolidados.extend(errores_chunk)
            
            if resultado.get('empleados_procesados', 0) > 0:
                chunks_exitosos += 1
    
    return {
        'total_empleados_procesados': total_empleados,
        'chunks_exitosos': chunks_exitosos,
        'total_chunks': len(resultados_chunks),
        'total_errores': total_errores,
        'errores': errores_consolidados,
        'procesamiento_exitoso': total_errores == 0
    }
```

#### Resultado de Fase 4A:

```
âœ… Chord empleados completado:
   - Chunk 1: 50 empleados procesados
   - Chunk 2: 12 empleados procesados
   - Total: 62 EmpleadoCierre creados
```

---

### Fase 4B: Crear RegistroConceptoEmpleado (Paralelo con Chord)

**Task**: `guardar_registros_nomina_optimizado`  
**Pattern**: Celery Chord (parallel tasks | callback)  
**Input**: Resultado de Fase 4A

```python
@shared_task(bind=True, queue='nomina_queue')
def guardar_registros_nomina_optimizado(self, result, usar_chord=True):
    """
    Crea RegistroConceptoEmpleado para cada empleado Ã— concepto
    """
    # Extraer libro_id del resultado anterior
    libro_id = result.get("libro_id")
    usuario_id = result.get("usuario_id")
    
    logger.info(f"[LIBRO] Guardando registros (optimizado) libro_id={libro_id}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        # 1. Leer Excel
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        total_filas = len(df)
        
        # 2. Calcular chunk size dinÃ¡mico
        chunk_size = _calcular_chunk_size_dinamico(total_filas)
        
        # 3. Dividir en chunks (MISMO algoritmo que empleados)
        chunks = dividir_dataframe_empleados(libro.archivo.path, chunk_size)
        
        # 4. Crear tasks paralelas
        tasks_paralelas = [
            procesar_chunk_registros_task.s(libro_id, chunk_data)
            for chunk_data in chunks
        ]
        
        # 5. Ejecutar Chord con callback que recibe usuario_id
        callback = consolidar_registros_task.s(usuario_id=usuario_id)
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(f"[LIBRO] ğŸš€ Chord registros iniciado: {len(chunks)} chunks")
        
        return {
            "libro_id": libro_id,
            "usuario_id": usuario_id,
            "chord_id": resultado_chord.id,
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord",
            "estado": "procesando"
        }
        
    except Exception as e:
        logger.error(f"[LIBRO] âŒ Error: {e}")
        raise
```

#### Worker Task: Procesar Chunk de Registros (CON FIX)

```python
@shared_task(queue='nomina_queue')
def procesar_chunk_registros_task(libro_id, chunk_data):
    """
    Worker: Procesa registros de conceptos para un chunk
    """
    return procesar_chunk_registros_util(libro_id, chunk_data)


def procesar_chunk_registros_util(libro_id, chunk_data):
    """
    ğŸ”§ FUNCIÃ“N CON FIX: Logging detallado agregado
    
    Crea RegistroConceptoEmpleado para cada empleado Ã— concepto
    """
    chunk_id = chunk_data['chunk_id']
    logger.info(f"ğŸ“ Procesando registros para chunk {chunk_id}/{chunk_data['total_chunks']}")
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        
        expected = {
            "ano": "AÃ±o",
            "mes": "Mes",
            "rut_empresa": "Rut de la Empresa",
            "rut_trabajador": "Rut del Trabajador",
            "nombre": "Nombre",
            "ape_pat": "Apellido Paterno",
            "ape_mat": "Apellido Materno",
        }
        
        empleado_cols = set(expected.values())
        
        # Obtener headers clasificados
        headers = libro.header_json
        if isinstance(headers, dict):
            headers = headers.get("headers_clasificados", []) + headers.get(
                "headers_sin_clasificar", []
            )
        if not headers:
            headers = [h for h in df.columns if h not in empleado_cols]
        
        # ğŸ” DEBUG LOGGING (AGREGADO EN EL FIX)
        chunk_indices = chunk_data['indices']
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] Total indices recibidos: {len(chunk_indices)}")
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] Primeros 3 indices: {chunk_indices[:3] if chunk_indices else 'VACÃO'}")
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] Total headers: {len(headers)}")
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] DataFrame shape: {df.shape}")
        
        count = 0
        errores = []
        
        # Procesar solo las filas de este chunk
        chunk_df = df.iloc[chunk_indices]
        logger.info(f"ğŸ” [DEBUG Chunk {chunk_id}] chunk_df shape: {chunk_df.shape}")
        
        with transaction.atomic():
            for _, row in chunk_df.iterrows():
                try:
                    # Normalizar RUT
                    rut_valor = row.get(expected["rut_trabajador"])
                    if not _es_rut_chileno_valido(rut_valor):
                        continue
                    
                    rut = formatear_rut_con_guion(normalizar_rut(rut_valor))
                    
                    # Buscar EmpleadoCierre (debe existir de Fase 4A)
                    empleado = EmpleadoCierre.objects.filter(
                        cierre=libro.cierre, 
                        rut=rut
                    ).first()
                    
                    if not empleado:
                        continue
                    
                    # Procesar TODOS los headers para este empleado
                    for h in headers:
                        try:
                            valor_raw = row.get(h)
                            
                            # Procesar valor (convertir a string limpio)
                            if pd.isna(valor_raw) or valor_raw == '':
                                valor = ""
                            else:
                                if isinstance(valor_raw, (int, float)):
                                    if isinstance(valor_raw, int) or valor_raw.is_integer():
                                        valor = str(int(valor_raw))
                                    else:
                                        valor = f"{valor_raw:.2f}".rstrip('0').rstrip('.')
                                else:
                                    valor = str(valor_raw).strip()
                                    if valor.lower() == 'nan':
                                        valor = ""
                            
                            # Buscar concepto clasificado
                            concepto = ConceptoRemuneracion.objects.filter(
                                cliente=libro.cierre.cliente, 
                                nombre_concepto=h, 
                                vigente=True
                            ).first()
                            
                            # Crear o actualizar registro
                            RegistroConceptoEmpleado.objects.update_or_create(
                                empleado=empleado,
                                nombre_concepto_original=h,
                                defaults={
                                    "monto": valor, 
                                    "concepto": concepto
                                }
                            )
                            
                        except Exception as concepto_error:
                            error_msg = f"Error en concepto '{h}' para RUT {rut}: {str(concepto_error)}"
                            errores.append(error_msg)
                            logger.error(error_msg)
                    
                    count += 1
                    
                except Exception as e:
                    error_msg = f"Error procesando registros para RUT {rut}: {str(e)}"
                    errores.append(error_msg)
                    logger.error(error_msg)
        
        resultado = {
            'chunk_id': chunk_id,
            'registros_procesados': count,
            'errores': errores,
            'libro_id': libro_id
        }
        
        logger.info(f"âœ… Chunk registros {chunk_id} completado: {count} empleados procesados")
        return resultado
        
    except Exception as e:
        error_msg = f"Error en chunk registros {chunk_id}: {str(e)}"
        logger.error(error_msg)
        return {
            'chunk_id': chunk_id,
            'registros_procesados': 0,
            'errores': [error_msg],
            'libro_id': libro_id
        }
```

#### Callback: Consolidar Registros y Finalizar

```python
@shared_task(queue='nomina_queue')
def consolidar_registros_task(resultados_chunks, usuario_id=None):
    """
    Callback final: Ejecutado cuando TODOS los chunks de registros terminan
    
    - Consolida estadÃ­sticas
    - Actualiza estado del libro a 'procesado'
    - Registra actividad del usuario
    """
    stats = consolidar_stats_registros(resultados_chunks)
    logger.info(f"[LIBRO] âœ… ConsolidaciÃ³n registros: {stats}")
    
    # Obtener libro_id
    libro_id = None
    for resultado in resultados_chunks:
        if isinstance(resultado, dict) and 'libro_id' in resultado:
            libro_id = resultado['libro_id']
            break
    
    if libro_id:
        try:
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            
            # Actualizar estado final
            libro.estado = "procesado"  # â† ESTADO FINAL âœ…
            libro.save(update_fields=['estado'])
            
            logger.info(f"[LIBRO] Estado actualizado a 'procesado' para libro {libro_id}")
            
            # Registrar actividad en tarjeta del usuario
            registrar_tarjeta_activity_log(
                usuario_id=usuario_id,
                libro_id=libro_id,
                event_type='process_complete',
                details={
                    "total_registros": stats.get('total_registros_procesados', 0),
                    "chunks_exitosos": stats.get('chunks_exitosos', 0)
                }
            )
            
        except Exception as e:
            logger.error(f"[LIBRO] Error actualizando estado: {e}")
    
    return stats
```

#### Resultado de Fase 4B:

```
âœ… Chord registros completado:
   - Chunk 1: 50 empleados Ã— 48 conceptos = 2,400 registros
   - Chunk 2: 12 empleados Ã— 48 conceptos = 576 registros
   - Total: 2,976 RegistroConceptoEmpleado creados
   
âœ… Estado final: libro.estado = 'procesado'
```

---

### Resumen del Flujo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TIMELINE COMPLETO                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

T0: Usuario sube Excel
    â†“
    â”œâ”€ Archivo guardado en /media/
    â””â”€ estado = 'subido'
    
T1: analizar_headers_libro (async)
    â†“
    â”œâ”€ Extrae columnas del Excel
    â”œâ”€ Filtra headers de empleado
    â””â”€ estado = 'analizando'
    
T2: clasificar_headers_libro (async)
    â†“
    â”œâ”€ Fuzzy matching con catÃ¡logo de conceptos
    â”œâ”€ Clasifica automÃ¡ticamente (80% threshold)
    â””â”€ estado = 'clasif_pendiente' o 'clasificado'
    
T3: [MANUAL] ClasificaciÃ³n pendientes (si aplica)
    â†“
    â””â”€ estado = 'clasificado'
    
T4: [MANUAL] Usuario presiona "Procesar"
    â†“
    â””â”€ Chain de Celery iniciado
    
T5: actualizar_empleados_desde_libro_optimizado
    â†“
    â”œâ”€ Divide Excel en chunks
    â”œâ”€ Chord: [chunk1, chunk2] | consolidar
    â””â”€ Crea EmpleadoCierre en paralelo
    
T6: guardar_registros_nomina_optimizado
    â†“
    â”œâ”€ Divide Excel en chunks (mismos Ã­ndices)
    â”œâ”€ Chord: [chunk1, chunk2] | consolidar
    â””â”€ Crea RegistroConceptoEmpleado en paralelo
    
T7: consolidar_registros_task (callback final)
    â†“
    â”œâ”€ Actualiza estado = 'procesado'
    â”œâ”€ Registra actividad del usuario
    â””â”€ âœ… PROCESAMIENTO COMPLETO

Tiempo total (ejemplo con 62 empleados):
  - Upload: ~1s
  - AnÃ¡lisis: ~2s
  - ClasificaciÃ³n: ~3s
  - ClasificaciÃ³n manual: ~30s (si aplica)
  - Fase 4A (empleados): ~10s
  - Fase 4B (registros): ~15s
  - TOTAL: ~1 minuto (o ~31s si auto-clasificado)
```

---

## ğŸ“š Lecciones Aprendidas

### 1. Celery No Auto-Reload en ProducciÃ³n

**Problema**: Los workers cargan el cÃ³digo en memoria al iniciar y NO recargan automÃ¡ticamente.

**SoluciÃ³n**: DespuÃ©s de cambios en tasks, siempre reiniciar:
```bash
docker compose restart celery_worker
```

**Alternativa para desarrollo**: Usar `celery -A proyecto worker --autoreload` (mÃ¡s lento).

---

### 2. Logging es CrÃ­tico para Debugging

**Problema**: Chunk fallaba silenciosamente sin indicar por quÃ© (0.115s, 0 registros, sin errores).

**SoluciÃ³n**: Agregar logging detallado en puntos crÃ­ticos:
- TamaÃ±o de inputs (indices, headers, dataframe shape)
- Estados intermedios (empleados encontrados, RUTs vÃ¡lidos)
- Resultados de cada paso (registros procesados, errores)

**Beneficio**: PermitiÃ³ identificar inmediatamente que el problema era el cÃ³digo obsoleto en memoria.

---

### 3. Tiempos de EjecuciÃ³n como Indicador

**ObservaciÃ³n**:
- Chunk normal: 1.5-10 segundos (depende del tamaÃ±o)
- Chunk con bug: 0.115 segundos âš ï¸

**Aprendizaje**: Un tiempo **sospechosamente rÃ¡pido** indica que el cÃ³digo saliÃ³ temprano (early exit) sin procesar datos.

**RecomendaciÃ³n**: Agregar mÃ©tricas de tiempo en logs para detectar anomalÃ­as.

---

### 4. Determinismo en DivisiÃ³n de Chunks

**Problema**: `dividir_dataframe_empleados` se llama DOS VECES (empleados y registros).

**Riesgo**: Si el algoritmo no es determinista, los chunks podrÃ­an ser diferentes.

**ValidaciÃ³n**: Verificamos que el algoritmo es determinista (mismo input â†’ mismo output).

**RecomendaciÃ³n futura**: Cachear los chunks despuÃ©s de la primera divisiÃ³n y reutilizarlos.

---

### 5. Testing de IntegraciÃ³n AsÃ­ncrona

**DesafÃ­o**: DifÃ­cil testear tasks de Celery que dependen de orden de ejecuciÃ³n.

**RecomendaciÃ³n**:
- Unit tests para funciones `_util` (sÃ­ncronas)
- Integration tests con `CELERY_TASK_ALWAYS_EAGER=True` (ejecuta tasks sÃ­ncronamente)
- Monitoring en producciÃ³n con Flower

---

## ğŸ¯ Recomendaciones

### Para Desarrollo

1. **Auto-reload durante desarrollo**:
```bash
# En docker-compose.dev.yml
celery_worker:
  command: celery -A sgm_backend worker --autoreload --loglevel=info
```

2. **Logging consistente**:
```python
# Usar siempre el mismo formato
logger.info(f"[LIBRO] {mensaje} libro_id={libro_id}, chunk={chunk_id}")
```

3. **Validaciones tempranas**:
```python
# Al inicio de cada task
if not chunk_indices:
    logger.error(f"[LIBRO] Chunk {chunk_id} tiene Ã­ndices vacÃ­os")
    return {'error': 'indices_vacios'}
```

---

### Para ProducciÃ³n

1. **Monitoring de Celery**:
```bash
# Flower dashboard
docker compose up -d flower
# Acceso: http://localhost:5555
```

2. **Alertas por tiempo de ejecuciÃ³n**:
```python
# En cada task
inicio = time.time()
# ... procesamiento ...
duracion = time.time() - inicio

if duracion < 0.5 and count == 0:
    logger.warning(f"[ALERTA] Chunk {chunk_id} terminÃ³ muy rÃ¡pido sin resultados")
```

3. **Reinicio automÃ¡tico de workers**:
```yaml
# docker-compose.yml
celery_worker:
  restart: unless-stopped
  deploy:
    restart_policy:
      condition: on-failure
      max_attempts: 3
```

---

### Para Testing

1. **Test de chunks deterministas**:
```python
def test_dividir_chunks_determinista():
    """Verificar que dividir_dataframe_empleados es determinista"""
    chunks1 = dividir_dataframe_empleados(archivo, chunk_size=50)
    chunks2 = dividir_dataframe_empleados(archivo, chunk_size=50)
    assert chunks1 == chunks2
```

2. **Test de consolidaciÃ³n**:
```python
def test_consolidacion_suma_correcta():
    """Verificar que consolidar suma todos los chunks"""
    resultados = [
        {'chunk_id': 1, 'registros_procesados': 50},
        {'chunk_id': 2, 'registros_procesados': 12}
    ]
    stats = consolidar_stats_registros(resultados)
    assert stats['total_registros_procesados'] == 62
```

3. **Test end-to-end**:
```python
@pytest.mark.django_db
def test_procesamiento_completo_libro():
    """Test completo: upload â†’ clasificar â†’ procesar"""
    # 1. Upload
    libro = LibroRemuneracionesUpload.objects.create(...)
    
    # 2. Analizar (sÃ­ncrono con CELERY_TASK_ALWAYS_EAGER)
    analizar_headers_libro_remuneraciones(libro.id)
    
    # 3. Clasificar
    clasificar_headers_libro_remuneraciones(libro.id)
    
    # 4. Procesar
    chain(...).apply()
    
    # 5. Verificar resultado
    assert EmpleadoCierre.objects.filter(cierre=libro.cierre).count() == 62
    assert RegistroConceptoEmpleado.objects.count() == 2976
```

---

## ğŸ“Š MÃ©tricas de Ã‰xito

### Antes del Fix

- âœ… EmpleadoCierre: 62/62 (100%)
- âŒ RegistroConceptoEmpleado: 576/2976 (19.4%)
- âŒ Empleados completos: 12/62 (19.4%)
- âŒ PÃ©rdida de datos: 80%

### DespuÃ©s del Fix

- âœ… EmpleadoCierre: 62/62 (100%)
- âœ… RegistroConceptoEmpleado: 2976/2976 (100%)
- âœ… Empleados completos: 62/62 (100%)
- âœ… PÃ©rdida de datos: 0%

---

## ğŸ”— Referencias

- **Archivo principal**: `backend/nomina/utils/LibroRemuneracionesOptimizado.py`
- **Tasks**: `backend/nomina/tasks_refactored/libro_remuneraciones.py`
- **Views**: `backend/nomina/views_libro_remuneraciones.py`
- **Frontend**: `src/pages/LibroRemuneracionesCard.jsx`
- **DocumentaciÃ³n previa**: `FLUJO_CONSOLIDACION_VISUAL.md`

---

## âœ… ConclusiÃ³n

El bug fue causado por **cÃ³digo obsoleto cargado en memoria del worker de Celery** que no se actualizÃ³ despuÃ©s de cambios en el cÃ³digo. La soluciÃ³n fue:

1. âœ… Agregar logging detallado para diagnÃ³stico
2. âœ… Reiniciar el worker de Celery para recargar cÃ³digo
3. âœ… Reprocesar el libro con cÃ³digo actualizado

El sistema ahora procesa correctamente **100% de los empleados con todos sus conceptos** usando paralelizaciÃ³n con Celery Chord.

**RecomendaciÃ³n crÃ­tica**: Siempre reiniciar workers despuÃ©s de cambios en cÃ³digo de tasks:
```bash
docker compose restart celery_worker
```

---

**Autor**: Equipo SGM  
**Fecha**: 24 de octubre de 2025  
**VersiÃ³n**: 1.0
