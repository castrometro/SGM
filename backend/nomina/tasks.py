# nomina/tasks.py
from .utils.LibroRemuneraciones import (
    obtener_headers_libro_remuneraciones,
    clasificar_headers_libro_remuneraciones,
    actualizar_empleados_desde_libro_util,
    guardar_registros_nomina_util,
)
# ‚ú® NUEVOS IMPORTS PARA OPTIMIZACI√ìN CON CHORD
from .utils.LibroRemuneracionesOptimizado import (
    dividir_dataframe_empleados,
    procesar_chunk_empleados_util,
    procesar_chunk_registros_util,
    consolidar_stats_empleados,
    consolidar_stats_registros,
)
from .utils.NovedadesRemuneraciones import (
    obtener_headers_archivo_novedades,
    clasificar_headers_archivo_novedades,
    actualizar_empleados_desde_novedades,
    guardar_registros_novedades,
)
from .utils.GenerarIncidencias import normalizar_rut, formatear_rut_con_guion
from .utils.MovimientoMes import procesar_archivo_movimientos_mes_util
from .utils.ArchivosAnalista import procesar_archivo_analista_util
from celery import shared_task, chain, chord, group
from .models import (
    LibroRemuneracionesUpload,
    MovimientosMesUpload,
    ArchivoAnalistaUpload,
    ArchivoNovedadesUpload,
)
import logging
import pandas as pd
from django.utils import timezone
import json
from decimal import Decimal

logger = logging.getLogger(__name__)

# Registrar tareas definidas fuera de tasks.py (autodiscovery de Celery)
try:
    # Import por efecto colateral: registra @shared_task en nomina.utils.DetectarIncidenciasConsolidadas
    from .utils import DetectarIncidenciasConsolidadas as _incidencias_tasks  # noqa: F401
    logger.debug("‚úÖ M√≥dulo de incidencias V2 importado para registro de Celery")
except Exception as _e:
    logger.warning(f"‚ö†Ô∏è No se pudo importar incidencias V2 para registro de Celery: {_e}")
def _json_safe(value):
    """Convierte estructuras anidadas a tipos serializables por JSON (psycopg2 json).
    - Decimal -> float
    - pandas.Timestamp -> ISO string
    - dict/list/tuple -> procesa recursivamente
    """
    # Tipos primitivos
    if value is None or isinstance(value, (bool, int, float, str)):
        return value
    # Decimales
    if isinstance(value, Decimal):
        try:
            return float(value)
        except Exception:
            return str(value)
    # pandas Timestamp o similares con isoformat
    try:
        import pandas as _pd  # local para evitar dependencias en import-time
        if isinstance(value, _pd.Timestamp):
            return value.isoformat()
    except Exception:
        pass
    if hasattr(value, 'isoformat'):
        try:
            return value.isoformat()
        except Exception:
            pass
    # Estructuras
    if isinstance(value, dict):
        return {k: _json_safe(v) for k, v in value.items()}
    if isinstance(value, (list, tuple, set)):
        return [ _json_safe(v) for v in list(value) ]
    # Fallback
    try:
        return json.loads(json.dumps(value))
    except Exception:
        return str(value)


def calcular_chunk_size_dinamico(empleados_count):
    """
    üßÆ Calcula el tama√±o de chunk √≥ptimo basado en el n√∫mero de empleados
    
    Args:
        empleados_count: N√∫mero total de empleados a procesar
        
    Returns:
        int: Tama√±o de chunk optimizado
    """
    if empleados_count <= 50:
        return 25  # Chunks peque√±os para pocos empleados
    elif empleados_count <= 200:
        return 50  # Chunks medianos para empresas peque√±as-medianas
    elif empleados_count <= 500:
        return 100  # Chunks grandes para empresas medianas
    elif empleados_count <= 1000:
        return 150  # Chunks muy grandes para empresas grandes
    else:
        return 200  # Chunks extremos para corporaciones


@shared_task
def analizar_headers_libro_remuneraciones(libro_id):
    logger.info(f"Procesando libro de remuneraciones id={libro_id}")
    libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
    libro.estado = "analizando_hdrs"
    libro.save()

    try:
        headers = obtener_headers_libro_remuneraciones(libro.archivo.path)
        libro.header_json = headers
        libro.estado = "hdrs_analizados"
        libro.save()
        logger.info(f"Procesamiento exitoso libro id={libro_id}")
        # ¬°Retornamos libro_id y headers!
        return {"libro_id": libro_id, "headers": headers}
    except Exception as e:
        libro.estado = "con_error"
        libro.save()
        logger.error(f"Error procesando libro id={libro_id}: {e}")
        raise


@shared_task
def clasificar_headers_libro_remuneraciones_task(result):
    """
    Tarea de clasificaci√≥n de headers mejorada con logging
    Mantiene compatibilidad con el resultado de la tarea anterior
    """
    from .utils.mixins import UploadLogNominaMixin
    from .models_logging import registrar_actividad_tarjeta_nomina
    
    # Extraer datos del resultado
    libro_id = result["libro_id"]
    upload_log_id = result.get("upload_log_id", None)
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        cierre = libro.cierre
        cliente = cierre.cliente
        
        # Inicializar mixin solo si tenemos upload_log_id
        mixin = UploadLogNominaMixin() if upload_log_id else None

        # 1. Marcamos estado "en proceso de clasificaci√≥n"
        libro.estado = "clasif_en_proceso"
        libro.save()
        
        if mixin and upload_log_id:
            mixin.actualizar_upload_log(upload_log_id, estado='clasif_en_proceso')

        # 2. Registrar inicio de clasificaci√≥n
        if upload_log_id:
            from .models_logging import UploadLogNomina
            upload_log = UploadLogNomina.objects.get(id=upload_log_id)
            registrar_actividad_tarjeta_nomina(
                cierre_id=cierre.id,
                tarjeta="libro_remuneraciones",
                accion="classification_start",
                descripcion="Inicio de clasificaci√≥n de conceptos",
                upload_log=upload_log
            )

        # 3. Obtener headers
        headers = (
            libro.header_json
            if isinstance(libro.header_json, list)
            else result["headers"]
        )

        # 4. Ejecutar clasificaci√≥n
        headers_clasificados, headers_sin_clasificar = (
            clasificar_headers_libro_remuneraciones(headers, cliente)
        )

        # 5. Guardar resultados
        libro.header_json = {
            "headers_clasificados": headers_clasificados,
            "headers_sin_clasificar": headers_sin_clasificar,
        }

        # 6. Determinar estado final
        if headers_sin_clasificar:
            libro.estado = "clasif_pendiente"
            estado_desc = f"Clasificaci√≥n parcial: {len(headers_clasificados)} clasificados, {len(headers_sin_clasificar)} pendientes"
        else:
            libro.estado = "clasificado"
            estado_desc = f"Clasificaci√≥n completa: {len(headers_clasificados)} conceptos clasificados"

        libro.save()
        
        # 7. Actualizar upload log con estad√≠sticas
        if mixin and upload_log_id:
            mixin.actualizar_upload_log(
                upload_log_id, 
                estado=libro.estado,
                resumen={
                    **libro.upload_log.resumen,
                    "headers_clasificados": len(headers_clasificados),
                    "headers_sin_clasificar": len(headers_sin_clasificar),
                    "fase": "clasificacion_completada"
                }
            )

        # 8. Registrar resultado
        if upload_log_id:
            registrar_actividad_tarjeta_nomina(
                cierre_id=cierre.id,
                tarjeta="libro_remuneraciones",
                accion="classification_complete",
                descripcion=estado_desc,
                resultado="exito",
                detalles={
                    "headers_clasificados": len(headers_clasificados),
                    "headers_sin_clasificar": len(headers_sin_clasificar),
                    "clasificacion_completa": len(headers_sin_clasificar) == 0
                },
                upload_log=upload_log
            )

        logger.info(
            f"Libro {libro_id}: {len(headers_clasificados)} headers clasificados, "
            f"{len(headers_sin_clasificar)} sin clasificar"
        )
        
        return {
            "libro_id": libro_id,
            "upload_log_id": upload_log_id,
            "headers_clasificados": len(headers_clasificados),
            "headers_sin_clasificar": len(headers_sin_clasificar),
        }
        
    except Exception as e:
        error_msg = f"Error clasificando headers para libro id={libro_id}: {e}"
        logger.error(error_msg)
        
        # Actualizar estados de error
        try:
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            libro.estado = "con_error"
            libro.save()
        except Exception as ex:
            logger.error(f"Error guardando estado 'con_error' para libro id={libro_id}: {ex}")
        
        # Actualizar upload log si existe
        if mixin and upload_log_id:
            mixin.marcar_como_error(upload_log_id, error_msg)
        
        # Registrar error en actividades si tenemos upload_log_id
        if upload_log_id:
            try:
                registrar_actividad_tarjeta_nomina(
                    cierre_id=libro.cierre.id,
                    tarjeta="libro_remuneraciones",
                    accion="classification_complete",
                    descripcion=f"Error en clasificaci√≥n: {str(e)}",
                    resultado="error",
                    detalles={"error": str(e)},
                    upload_log=upload_log if upload_log_id else None
                )
            except Exception:
                pass  # No fallar si no se puede registrar la actividad
        
        raise
        raise


@shared_task
def actualizar_empleados_desde_libro(result):
    """Task para actualizar empleados desde libro - solo maneja try/except"""
    libro_id = result.get("libro_id") if isinstance(result, dict) else result
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        count = actualizar_empleados_desde_libro_util(libro)
        logger.info(f"Actualizados {count} empleados desde libro {libro_id}")
        return {"libro_id": libro_id, "empleados_actualizados": count}
    except Exception as e:
        logger.error(f"Error actualizando empleados para libro id={libro_id}: {e}")
        raise


@shared_task
def guardar_registros_nomina(result):
    """Task para guardar registros de n√≥mina - solo maneja try/except"""
    libro_id = result.get("libro_id") if isinstance(result, dict) else result
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        count = guardar_registros_nomina_util(libro)
        
        # ‚úÖ ACTUALIZAR ESTADO AL FINAL
        libro.estado = "procesado"
        libro.save()
        
        return {
            "libro_id": libro_id, 
            "registros_actualizados": count,
            "estado": "procesado"
        }
    except Exception as e:
        logger.error(f"Error guardando registros de n√≥mina para libro id={libro_id}: {e}")
        # Marcar como error
        try:
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            libro.estado = "con_error"
            libro.save()
        except:
            pass
        raise


@shared_task
def procesar_movimientos_mes(movimiento_id, upload_log_id=None):
    """Procesa un archivo de movimientos del mes con logging completo"""
    from .utils.mixins import UploadLogNominaMixin
    from .models_logging import registrar_actividad_tarjeta_nomina, UploadLogNomina
    
    logger.info(f"Procesando archivo movimientos mes id={movimiento_id} con upload_log={upload_log_id}")
    
    mixin = UploadLogNominaMixin() if upload_log_id else None
    upload_log = None
    
    try:
        # Obtener referencias
        movimiento = MovimientosMesUpload.objects.get(id=movimiento_id)
        if upload_log_id:
            upload_log = UploadLogNomina.objects.get(id=upload_log_id)
        
        # Cambiar estado a procesando
        movimiento.estado = 'en_proceso'
        movimiento.save()
        
        if mixin and upload_log:
            mixin.actualizar_upload_log(upload_log_id, estado='procesando', resumen={
                'fase': 'procesamiento_iniciado',
                'movimiento_id': movimiento_id
            })
        
        # Registrar inicio de procesamiento
        registrar_actividad_tarjeta_nomina(
            cierre_id=movimiento.cierre.id,
            tarjeta="movimientos_mes",
            accion="process_start",
            descripcion="Iniciando procesamiento de archivo de movimientos del mes",
            detalles={
                "movimiento_id": movimiento_id,
                "upload_log_id": upload_log_id,
                "archivo_nombre": movimiento.archivo.name if movimiento.archivo else "N/A"
            },
            upload_log=upload_log
        )
        
        # Procesar archivo
        resultados = procesar_archivo_movimientos_mes_util(movimiento)
        
        # Guardar resultados
        movimiento.resultados_procesamiento = resultados
        
        # Determinar estado final
        if resultados.get('errores'):
            if any(v > 0 for k, v in resultados.items() if k != 'errores' and isinstance(v, int)):
                estado_final = 'con_errores_parciales'
                resultado_actividad = 'warning'
            else:
                estado_final = 'con_error'
                resultado_actividad = 'error'
        else:
            estado_final = 'procesado'
            resultado_actividad = 'exito'
        
        movimiento.estado = estado_final
        movimiento.save()
        
        # Actualizar UploadLog
        if mixin and upload_log:
            mixin.actualizar_upload_log(upload_log_id, 
                estado='completado' if estado_final == 'procesado' else 'error',
                resumen={
                    'fase': 'procesamiento_completado',
                    'estado_final': estado_final,
                    'resultados': resultados,
                    'registros_procesados': sum(v for k, v in resultados.items() if k != 'errores' and isinstance(v, int))
                }
            )
        
        # Registrar finalizaci√≥n
        descripcion = f"Procesamiento completado con estado: {estado_final}"
        if resultados.get('errores'):
            descripcion += f". Errores: {len(resultados['errores'])}"
        
        registrar_actividad_tarjeta_nomina(
            cierre_id=movimiento.cierre.id,
            tarjeta="movimientos_mes",
            accion="process_complete",
            descripcion=descripcion,
            resultado=resultado_actividad,
            detalles={
                "movimiento_id": movimiento_id,
                "upload_log_id": upload_log_id,
                "estado_final": estado_final,
                "resultados": resultados,
                "registros_totales": sum(v for k, v in resultados.items() if k != 'errores' and isinstance(v, int)),
                "errores_count": len(resultados.get('errores', []))
            },
            upload_log=upload_log
        )
        
        logger.info(f"Procesamiento exitoso movimientos mes id={movimiento_id}, estado: {estado_final}")
        return resultados
        
    except Exception as e:
        error_msg = f"Error procesando movimientos mes id={movimiento_id}: {e}"
        logger.error(error_msg)
        
        # Actualizar estados de error
        try:
            movimiento = MovimientosMesUpload.objects.get(id=movimiento_id)
            movimiento.estado = 'con_error'
            movimiento.resultados_procesamiento = {'errores': [str(e)]}
            movimiento.save()
        except:
            pass
        
        # Marcar UploadLog como error
        if mixin and upload_log_id:
            mixin.marcar_como_error(upload_log_id, error_msg)
        
        # Registrar error en actividades
        if upload_log_id:
            try:
                registrar_actividad_tarjeta_nomina(
                    cierre_id=movimiento.cierre.id,
                    tarjeta="movimientos_mes",
                    accion="process_complete",
                    descripcion=f"Error en procesamiento: {str(e)}",
                    resultado="error",
                    detalles={
                        "movimiento_id": movimiento_id,
                        "upload_log_id": upload_log_id,
                        "error": str(e)
                    },
                    upload_log=upload_log
                )
            except Exception:
                pass  # No fallar si no se puede registrar la actividad
        
        raise


# Nuevas tasks para archivos del analista

@shared_task
def procesar_archivo_analista(archivo_id):
    """Procesa un archivo subido por el analista (finiquitos, incidencias o ingresos)"""
    logger.info(f"Procesando archivo analista id={archivo_id}")
    
    try:
        archivo = ArchivoAnalistaUpload.objects.get(id=archivo_id)
        archivo.estado = 'en_proceso'
        archivo.save()
        
        resultados = procesar_archivo_analista_util(archivo)
        
        # Determinar estado final
        if resultados.get('errores'):
            if resultados.get('procesados', 0) > 0:
                archivo.estado = 'procesado'  # Parcialmente exitoso
            else:
                archivo.estado = 'con_error'  # Totalmente fallido
        else:
            archivo.estado = 'procesado'
        
        archivo.save()
        
        logger.info(f"Procesamiento exitoso archivo analista id={archivo_id}: {resultados['procesados']} registros procesados")
        return resultados
        
    except Exception as e:
        archivo = ArchivoAnalistaUpload.objects.get(id=archivo_id)
        archivo.estado = 'con_error'
        archivo.save()
        logger.error(f"Error procesando archivo analista id={archivo_id}: {e}")
        raise


@shared_task
def procesar_archivo_novedades(archivo_id):
    """Procesa un archivo de novedades - solo hasta clasificaci√≥n inicial"""
    logger.info(f"Iniciando procesamiento inicial archivo de novedades id={archivo_id}")
    
    try:
        archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
        
        # ‚úÖ SOLO ejecutar an√°lisis y clasificaci√≥n inicial - NO las tareas finales
        workflow = chain(
            analizar_headers_archivo_novedades.s(archivo_id),
            clasificar_headers_archivo_novedades_task.s()
        )
        
        # Ejecutar la cadena parcial
        workflow.apply_async()
        
        logger.info(f"Cadena de an√°lisis y clasificaci√≥n iniciada para archivo novedades id={archivo_id}")
        return {"archivo_id": archivo_id, "estado": "cadena_iniciada"}
        
    except Exception as e:
        try:
            archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
            archivo.estado = 'con_error'
            archivo.save()
        except Exception as ex:
            logger.error(f"Error guardando estado 'con_error' para archivo novedades id={archivo_id}: {ex}")
        
        logger.error(f"Error iniciando procesamiento archivo de novedades id={archivo_id}: {e}")
        raise


@shared_task
def analizar_headers_archivo_novedades(archivo_id):
    """Analiza headers de un archivo de novedades"""
    logger.info(f"Procesando archivo de novedades id={archivo_id}")
    archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
    archivo.estado = "analizando_hdrs"
    archivo.save()

    try:
        headers = obtener_headers_archivo_novedades(archivo.archivo.path)
        archivo.header_json = headers
        archivo.estado = "hdrs_analizados"
        archivo.save()
        logger.info(f"Procesamiento exitoso archivo novedades id={archivo_id}")
        # Retornamos archivo_id y headers
        return {"archivo_id": archivo_id, "headers": headers}
    except Exception as e:
        archivo.estado = "con_error"
        archivo.save()
        logger.error(f"Error procesando archivo novedades id={archivo_id}: {e}")
        raise


@shared_task
def clasificar_headers_archivo_novedades_task(result):
    """Clasifica headers de un archivo de novedades"""
    archivo_id = result["archivo_id"]
    try:
        archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
        cierre = archivo.cierre
        cliente = cierre.cliente

        # Marcamos estado "en proceso de clasificaci√≥n"
        archivo.estado = "clasif_en_proceso"
        archivo.save()

        headers = (
            archivo.header_json
            if isinstance(archivo.header_json, list)
            else result["headers"]
        )

        headers_clasificados, headers_sin_clasificar = (
            clasificar_headers_archivo_novedades(headers, cliente)
        )

        # Escribe el resultado final en el campo JSON
        archivo.header_json = {
            "headers_clasificados": headers_clasificados,
            "headers_sin_clasificar": headers_sin_clasificar,
        }

        # Cambia el estado seg√∫n si quedan pendientes o no
        if headers_sin_clasificar:
            archivo.estado = "clasif_pendiente"
        else:
            archivo.estado = "clasificado"
            
            # ‚úÖ CAMBIO: NO ejecutar autom√°ticamente el procesamiento final
            # El usuario debe decidir cu√°ndo procesar mediante el bot√≥n "Procesar Final"
            logger.info(f"Archivo novedades {archivo_id}: Todos los headers clasificados autom√°ticamente. Estado: 'clasificado' - esperando acci√≥n del usuario")

        archivo.save()
        logger.info(
            f"Archivo novedades {archivo_id}: {len(headers_clasificados)} headers clasificados, "
            f"{len(headers_sin_clasificar)} sin clasificar"
        )
        return {
            "archivo_id": archivo_id,
            "headers_clasificados": len(headers_clasificados),
            "headers_sin_clasificar": len(headers_sin_clasificar),
        }
    except Exception as e:
        logger.error(f"Error clasificando headers para archivo novedades id={archivo_id}: {e}")
        try:
            archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
            archivo.estado = "con_error"
            archivo.save()
        except Exception as ex:
            logger.error(
                f"Error guardando estado 'con_error' para archivo novedades id={archivo_id}: {ex}"
            )
        raise


@shared_task
def actualizar_empleados_desde_novedades_task(result):
    """Task para actualizar empleados desde archivo de novedades"""
    archivo_id = result.get("archivo_id") if isinstance(result, dict) else result
    try:
        archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
        count = actualizar_empleados_desde_novedades(archivo)
        logger.info(f"Actualizados {count} empleados desde archivo novedades {archivo_id}")
        return {"archivo_id": archivo_id, "empleados_actualizados": count}
    except Exception as e:
        logger.error(f"Error actualizando empleados para archivo novedades id={archivo_id}: {e}")
        raise


@shared_task
def guardar_registros_novedades_task(result):
    """Task para guardar registros de novedades"""
    archivo_id = result.get("archivo_id") if isinstance(result, dict) else result
    try:
        archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
        count = guardar_registros_novedades(archivo)
        
        # ‚úÖ ACTUALIZAR ESTADO AL FINAL
        archivo.estado = "procesado"
        archivo.save()
        
        return {
            "archivo_id": archivo_id, 
            "registros_actualizados": count,
            "estado": "procesado"
        }
    except Exception as e:
        logger.error(f"Error guardando registros de novedades para archivo id={archivo_id}: {e}")
        # Marcar como error
        try:
            archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
            archivo.estado = "con_error"
            archivo.save()
        except:
            pass
        raise


# ===== üöÄ TASKS OPTIMIZADAS PARA NOVEDADES CON CELERY CHORD =====

@shared_task
def procesar_chunk_empleados_novedades_task(archivo_id, chunk_data):
    """
    üë• Task para procesar un chunk espec√≠fico de empleados de novedades en paralelo
    
    Args:
        archivo_id: ID del ArchivoNovedadesUpload
        chunk_data: Datos del chunk a procesar
        
    Returns:
        Dict: Estad√≠sticas del procesamiento del chunk
    """
    from .utils.NovedadesOptimizado import procesar_chunk_empleados_novedades_util, validar_chunk_data
    
    logger.info(f"üîÑ Iniciando procesamiento de chunk empleados novedades {chunk_data.get('chunk_id')}")
    
    # Validar datos del chunk
    if not validar_chunk_data(chunk_data):
        return {
            'chunk_id': chunk_data.get('chunk_id', 0),
            'empleados_actualizados': 0,
            'errores': ['Datos del chunk inv√°lidos'],
            'archivo_id': archivo_id,
            'success': False
        }
    
    try:
        resultado = procesar_chunk_empleados_novedades_util(archivo_id, chunk_data)
        logger.info(f"‚úÖ Chunk empleados novedades {chunk_data.get('chunk_id')} completado exitosamente")
        return resultado
    except Exception as e:
        error_msg = f"Error en chunk empleados novedades {chunk_data.get('chunk_id')}: {str(e)}"
        logger.error(error_msg)
        return {
            'chunk_id': chunk_data.get('chunk_id', 0),
            'empleados_actualizados': 0,
            'errores': [error_msg],
            'tiempo_procesamiento': 0,
            'registros_procesados': 0,
            'archivo_id': archivo_id,
            'success': False
        }


@shared_task
def procesar_chunk_registros_novedades_task(archivo_id, chunk_data):
    """
    üìù Task para procesar un chunk espec√≠fico de registros de novedades en paralelo
    
    Args:
        archivo_id: ID del ArchivoNovedadesUpload
        chunk_data: Datos del chunk a procesar
        
    Returns:
        Dict: Estad√≠sticas del procesamiento del chunk
    """
    from .utils.NovedadesOptimizado import procesar_chunk_registros_novedades_util, validar_chunk_data
    
    logger.info(f"üíæ Iniciando procesamiento de chunk registros novedades {chunk_data.get('chunk_id')}")
    
    # Validar datos del chunk
    if not validar_chunk_data(chunk_data):
        return {
            'chunk_id': chunk_data.get('chunk_id', 0),
            'registros_creados': 0,
            'registros_actualizados': 0,
            'errores': ['Datos del chunk inv√°lidos'],
            'archivo_id': archivo_id,
            'success': False
        }
    
    try:
        resultado = procesar_chunk_registros_novedades_util(archivo_id, chunk_data)
        logger.info(f"‚úÖ Chunk registros novedades {chunk_data.get('chunk_id')} completado exitosamente")
        return resultado
    except Exception as e:
        error_msg = f"Error en chunk registros novedades {chunk_data.get('chunk_id')}: {str(e)}"
        logger.error(error_msg)
        return {
            'chunk_id': chunk_data.get('chunk_id', 0),
            'registros_creados': 0,
            'registros_actualizados': 0,
            'errores': [error_msg],
            'tiempo_procesamiento': 0,
            'registros_procesados': 0,
            'archivo_id': archivo_id,
            'success': False
        }


@shared_task
def consolidar_empleados_novedades_task(resultados_chunks):
    """
    üìä Consolida los resultados de m√∫ltiples chunks de empleados de novedades
    
    Args:
        resultados_chunks: Lista de resultados de chunks de empleados
        
    Returns:
        Dict: Estad√≠sticas consolidadas
    """
    from .utils.NovedadesOptimizado import consolidar_stats_novedades
    
    logger.info(f"üìä Consolidando resultados de {len(resultados_chunks)} chunks de empleados novedades")
    
    try:
        consolidado = consolidar_stats_novedades(resultados_chunks, 'empleados')
        
        # Obtener archivo_id del primer resultado
        archivo_id = resultados_chunks[0].get('archivo_id') if resultados_chunks else None
        
        # Agregar informaci√≥n espec√≠fica para la siguiente fase
        consolidado.update({
            'fase': 'empleados_completada',
            'archivo_id': archivo_id,
            'listo_para_registros': True
        })
        
        logger.info(f"‚úÖ Consolidaci√≥n empleados completada: {consolidado.get('empleados_actualizados', 0)} empleados actualizados")
        
        return consolidado
        
    except Exception as e:
        logger.error(f"‚ùå Error consolidando empleados novedades: {e}")
        return {
            'fase': 'empleados_error',
            'archivo_id': resultados_chunks[0].get('archivo_id') if resultados_chunks else None,
            'errores': [str(e)],
            'success': False
        }


@shared_task
def finalizar_procesamiento_novedades_task(resultados_chunks):
    """
    üéØ Finaliza el procesamiento de novedades y actualiza el estado del archivo
    
    Args:
        resultados_chunks: Lista de resultados de chunks de registros
        
    Returns:
        Dict: Resultado final del procesamiento
    """
    from .utils.NovedadesOptimizado import consolidar_stats_novedades
    from .models_logging import registrar_actividad_tarjeta_nomina
    
    logger.info(f"üéØ Finalizando procesamiento de novedades con {len(resultados_chunks)} chunks")
    
    try:
        # Consolidar estad√≠sticas de registros
        consolidado = consolidar_stats_novedades(resultados_chunks, 'registros')
        archivo_id = consolidado.get('archivo_id') or (resultados_chunks[0].get('archivo_id') if resultados_chunks else None)
        
        if not archivo_id:
            raise ValueError("No se pudo obtener archivo_id de los resultados")
        
        # Obtener archivo y actualizar estado
        archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
        
        # Determinar estado final basado en errores
        errores_totales = consolidado.get('errores_totales', 0)
        registros_creados = consolidado.get('registros_creados', 0)
        registros_actualizados = consolidado.get('registros_actualizados', 0)
        
        if errores_totales > 0 and (registros_creados > 0 or registros_actualizados > 0):
            estado_final = "con_errores_parciales"
            resultado_actividad = "warning"
        elif errores_totales > 0:
            estado_final = "con_error"
            resultado_actividad = "error"
        else:
            estado_final = "procesado"
            resultado_actividad = "exito"
        
        # Actualizar estado del archivo
        archivo.estado = estado_final
        archivo.save()
        
        # Registrar actividad de finalizaci√≥n
        try:
            registrar_actividad_tarjeta_nomina(
                cierre_id=archivo.cierre.id,
                tarjeta="novedades",
                accion="procesamiento_paralelo_completado",
                descripcion=f"Procesamiento paralelo completado: {registros_creados} registros creados, {registros_actualizados} actualizados",
                resultado=resultado_actividad,
                detalles={
                    "archivo_id": archivo_id,
                    "estado_final": estado_final,
                    "estadisticas": consolidado,
                    "modo": "chord_paralelo"
                }
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error registrando actividad final: {e}")
        
        resultado_final = {
            'archivo_id': archivo_id,
            'estado_final': estado_final,
            'estadisticas_consolidadas': consolidado,
            'timestamp_finalizacion': timezone.now().isoformat(),
            'success': True
        }
        
        logger.info(f"üéØ Procesamiento novedades finalizado exitosamente:")
        logger.info(f"  - Estado final: {estado_final}")
        logger.info(f"  - Registros creados: {registros_creados}")
        logger.info(f"  - Registros actualizados: {registros_actualizados}")
        logger.info(f"  - Errores totales: {errores_totales}")
        
        return resultado_final
        
    except Exception as e:
        logger.error(f"‚ùå Error finalizando procesamiento novedades: {e}")
        
        # Intentar marcar archivo como error
        try:
            if 'archivo_id' in locals():
                archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
                archivo.estado = "con_error"
                archivo.save()
        except:
            pass
        
        return {
            'archivo_id': archivo_id if 'archivo_id' in locals() else None,
            'estado_final': 'con_error',
            'errores': [str(e)],
            'success': False
        }


@shared_task
def actualizar_empleados_desde_novedades_task_optimizado(result):
    """
    üöÄ Versi√≥n optimizada que usa Celery Chord para procesar empleados en chunks paralelos
    
    Args:
        result: Resultado de la task anterior (contiene archivo_id)
        
    Returns:
        Dict: Informaci√≥n del procesamiento o referencia al chord
    """
    from .utils.NovedadesOptimizado import dividir_dataframe_novedades, obtener_archivo_novedades_path
    
    archivo_id = result.get("archivo_id") if isinstance(result, dict) else result
    
    logger.info(f"üöÄ Iniciando actualizaci√≥n optimizada empleados novedades archivo {archivo_id}")
    
    try:
        archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
        
        # Obtener ruta del archivo
        ruta_archivo = obtener_archivo_novedades_path(archivo_id)
        if not ruta_archivo:
            raise ValueError(f"No se pudo obtener ruta del archivo {archivo_id}")
        
        # Leer archivo para calcular chunk size
        import pandas as pd
        df = pd.read_excel(ruta_archivo, engine="openpyxl")
        total_filas = len(df)
        
        # Calcular chunk size din√°mico
        chunk_size = calcular_chunk_size_dinamico(total_filas)
        
        logger.info(f"üìä Total filas novedades: {total_filas}, Chunk size calculado: {chunk_size}")
        
        # Para archivos peque√±os, usar procesamiento directo
        if total_filas <= 50:
            logger.info(f"üìù Archivo peque√±o ({total_filas} filas), usando procesamiento directo")
            from .utils.NovedadesRemuneraciones import actualizar_empleados_desde_novedades
            count = actualizar_empleados_desde_novedades(archivo)
            return {
                "archivo_id": archivo_id,
                "empleados_actualizados": count,
                "modo": "directo",
                "timestamp": timezone.now().isoformat()
            }
        
        # Dividir en chunks para procesamiento paralelo
        chunks = dividir_dataframe_novedades(ruta_archivo, chunk_size)
        
        if not chunks:
            logger.warning(f"‚ö†Ô∏è No se crearon chunks v√°lidos para archivo {archivo_id}")
            raise ValueError("No se pudieron crear chunks para el archivo")
        
        logger.info(f"üì¶ Creados {len(chunks)} chunks para procesamiento paralelo")
        
        # Crear tasks paralelas usando chord
        tasks_paralelas = [
            procesar_chunk_empleados_novedades_task.s(archivo_id, chunk_data) 
            for chunk_data in chunks
        ]
        
        # Ejecutar chord: tasks paralelas | callback
        callback = consolidar_empleados_novedades_task.s()
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(f"üöÄ Chord empleados iniciado para archivo {archivo_id}: {len(chunks)} chunks en paralelo")
        
        return {
            "archivo_id": archivo_id,
            "chord_id": str(resultado_chord),
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord_empleados",
            "timestamp": timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en empleados optimizado archivo {archivo_id}: {e}")
        
        # Marcar archivo como error
        try:
            archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
            archivo.estado = "con_error"
            archivo.save()
        except:
            pass
        
        raise


@shared_task
def guardar_registros_novedades_task_optimizado(result):
    """
    üöÄ Versi√≥n optimizada que usa Celery Chord para guardar registros en chunks paralelos
    
    Args:
        result: Resultado de la task anterior (contiene archivo_id)
        
    Returns:
        Dict: Informaci√≥n del procesamiento o referencia al chord
    """
    from .utils.NovedadesOptimizado import dividir_dataframe_novedades, obtener_archivo_novedades_path
    
    archivo_id = result.get("archivo_id") if isinstance(result, dict) else result
    
    logger.info(f"üöÄ Iniciando guardado optimizado registros novedades archivo {archivo_id}")
    
    try:
        archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
        
        # Verificar que el archivo est√© en estado correcto
        if archivo.estado not in ['clasificado', 'empleados_actualizados']:
            logger.warning(f"‚ö†Ô∏è Archivo en estado {archivo.estado}, continuando con registros")
        
        # Obtener ruta del archivo
        ruta_archivo = obtener_archivo_novedades_path(archivo_id)
        if not ruta_archivo:
            raise ValueError(f"No se pudo obtener ruta del archivo {archivo_id}")
        
        # Leer archivo para calcular chunk size
        import pandas as pd
        df = pd.read_excel(ruta_archivo, engine="openpyxl")
        total_filas = len(df)
        
        # Calcular chunk size din√°mico
        chunk_size = calcular_chunk_size_dinamico(total_filas)
        
        logger.info(f"üíæ Iniciando guardado de registros en chunks: {total_filas} filas, chunk size: {chunk_size}")
        
        # Para archivos peque√±os, usar procesamiento directo
        if total_filas <= 50:
            logger.info(f"üìù Archivo peque√±o ({total_filas} filas), guardado directo")
            from .utils.NovedadesRemuneraciones import guardar_registros_novedades
            count = guardar_registros_novedades(archivo)
            
            # Actualizar estado final
            archivo.estado = "procesado"
            archivo.save()
            
            return {
                "archivo_id": archivo_id,
                "registros_guardados": count,
                "estado_final": "procesado",
                "modo": "directo",
                "timestamp": timezone.now().isoformat()
            }
        
        # Dividir en chunks para procesamiento paralelo
        chunks = dividir_dataframe_novedades(ruta_archivo, chunk_size)
        
        if not chunks:
            logger.warning(f"‚ö†Ô∏è No se crearon chunks v√°lidos para registros archivo {archivo_id}")
            raise ValueError("No se pudieron crear chunks para registros")
        
        logger.info(f"üì¶ Creados {len(chunks)} chunks para guardado paralelo de registros")
        
        # Crear tasks paralelas para registros
        tasks_paralelas = [
            procesar_chunk_registros_novedades_task.s(archivo_id, chunk_data) 
            for chunk_data in chunks
        ]
        
        # Ejecutar chord para registros
        callback = finalizar_procesamiento_novedades_task.s()
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(f"üöÄ Chord registros iniciado para archivo {archivo_id}: {len(chunks)} chunks en paralelo")
        
        return {
            "archivo_id": archivo_id,
            "chord_id": str(resultado_chord),
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord_registros",
            "timestamp": timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en registros optimizado archivo {archivo_id}: {e}")
        
        # Marcar archivo como error
        try:
            archivo = ArchivoNovedadesUpload.objects.get(id=archivo_id)
            archivo.estado = "con_error"
            archivo.save()
        except:
            pass
        
        raise


# ===== TAREAS PARA SISTEMA DE INCIDENCIAS =====

@shared_task
def generar_incidencias_cierre_task(cierre_id):
    """
    üîç TASK: GENERACI√ìN DE INCIDENCIAS CONSOLIDADAS
    
    Genera incidencias comparando datos consolidados del mes actual con el mes anterior.
    Implementa las 4 reglas principales:
    1. Variaciones de valor header-empleado superior a ¬±30%
    2. Ausentismos del mes anterior que deber√≠an continuar
    3. Personas que ingresaron el mes anterior y no est√°n presentes  
    4. Personas que finiquitaron el mes anterior y siguen presentes
    """
    from .utils.DetectarIncidenciasConsolidadas import generar_incidencias_consolidadas_task
    from .models import CierreNomina
    
    logger.info(f"üîç Iniciando generaci√≥n de incidencias para cierre consolidado {cierre_id}")
    
    try:
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        # Verificar que el cierre est√© en un estado v√°lido para generar incidencias
        estados_validos = ['datos_consolidados', 'con_incidencias', 'incidencias_resueltas'] 
        if cierre.estado not in estados_validos:
            raise ValueError(f"El cierre debe estar en estado v√°lido para generar incidencias. Estado actual: {cierre.estado}, Estados v√°lidos: {estados_validos}")
        
        # Ejecutar detecci√≥n de incidencias
        resultado = generar_incidencias_consolidadas_task(cierre_id)
        
        if resultado['success']:
            logger.info(f"‚úÖ Incidencias generadas exitosamente para cierre {cierre_id}: {resultado['total_incidencias']} incidencias")
            return resultado
        else:
            logger.error(f"‚ùå Error en detecci√≥n de incidencias para cierre {cierre_id}: {resultado.get('error', 'Error desconocido')}")
            return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico generando incidencias para cierre {cierre_id}: {e}")
        try:
            cierre = CierreNomina.objects.get(id=cierre_id)
            # No cambiar estado en caso de error, mantener consolidado
        except:
            pass
        
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id
        }


# ===== NUEVAS TAREAS PARA SISTEMA DUAL DE INCIDENCIAS (CELERY CHORD) =====

@shared_task
def generar_incidencias_consolidados_v2_task(cierre_id, clasificaciones_seleccionadas=None):
    """
    üîÑ TASK: GENERACI√ìN DUAL DE INCIDENCIAS (CELERY CHORD)
    
    Implementa el sistema dual de detecci√≥n de incidencias:
    - Comparaci√≥n individual (elemento por elemento) para clasificaciones seleccionadas
    - Comparaci√≥n por suma total para todas las clasificaciones
    
    Utiliza Celery Chord para procesamiento paralelo optimizado.
    Performance target: ~183% improvement (8.2s ‚Üí 2.9s)
    """
    from .utils.DetectarIncidenciasConsolidadas import generar_incidencias_consolidados_v2
    from .models import CierreNomina
    
    logger.info(f"üîÑ Iniciando generaci√≥n dual de incidencias para cierre {cierre_id}")
    
    try:
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        # Verificar estado v√°lido
        estados_validos = ['datos_consolidados', 'con_incidencias', 'incidencias_resueltas'] 
        if cierre.estado not in estados_validos:
            raise ValueError(f"Estado inv√°lido: {cierre.estado}. V√°lidos: {estados_validos}")
        
        # Ejecutar detecci√≥n dual con Celery Chord
        resultado = generar_incidencias_consolidados_v2(
            cierre_id=cierre_id,
            clasificaciones_seleccionadas=clasificaciones_seleccionadas
        )
        
        if resultado['success']:
            logger.info(f"‚úÖ Sistema dual completado para cierre {cierre_id}:")
            logger.info(f"   üìã Incidencias individuales: {resultado.get('total_incidencias_individuales', 0)}")
            logger.info(f"   üìä Incidencias suma total: {resultado.get('total_incidencias_suma', 0)}")
            logger.info(f"   üéØ Total: {resultado.get('total_incidencias', 0)}")
            logger.info(f"   ‚è±Ô∏è Tiempo procesamiento: {resultado.get('tiempo_procesamiento', 'N/A')}")
            
            return resultado
        else:
            logger.error(f"‚ùå Error en sistema dual para cierre {cierre_id}: {resultado.get('error')}")
            return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico en sistema dual para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id
        }


@shared_task
def procesar_chunk_comparacion_individual_task(chunk_data, cierre_id, clasificaciones_seleccionadas):
    """
    üîç TASK: PROCESAMIENTO DE CHUNK INDIVIDUAL
    
    Procesa un chunk de empleados para comparaci√≥n individual (elemento por elemento).
    Parte del Celery Chord para procesamiento paralelo.
    """
    from .utils.DetectarIncidenciasConsolidadas import procesar_chunk_comparacion_individual
    
    try:
        resultado = procesar_chunk_comparacion_individual(
            chunk_data=chunk_data,
            cierre_id=cierre_id,
            clasificaciones_seleccionadas=clasificaciones_seleccionadas
        )
        
        logger.info(f"‚úÖ Chunk individual procesado: {len(resultado)} incidencias")
        return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando chunk individual: {e}")
        return []


@shared_task
def procesar_comparacion_suma_total_task(cierre_id):
    """
    üìä TASK: PROCESAMIENTO DE SUMA TOTAL
    
    Procesa la comparaci√≥n por suma total de todas las clasificaciones.
    Ejecuta en paralelo con las comparaciones individuales.
    """
    from .utils.DetectarIncidenciasConsolidadas import procesar_comparacion_suma_total
    
    try:
        resultado = procesar_comparacion_suma_total(cierre_id)
        
        logger.info(f"‚úÖ Suma total procesada: {len(resultado)} incidencias")
        return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando suma total: {e}")
        return []


@shared_task
def consolidar_resultados_incidencias_task(resultados_individuales, resultados_suma_total, cierre_id):
    """
    üéØ TASK: CONSOLIDACI√ìN DE RESULTADOS
    
    Consolida los resultados de todas las tareas paralelas del Celery Chord.
    Callback final que unifica todos los resultados.
    """
    from .utils.DetectarIncidenciasConsolidadas import consolidar_resultados_incidencias
    
    try:
        resultado_final = consolidar_resultados_incidencias(
            resultados_individuales=resultados_individuales,
            resultados_suma_total=resultados_suma_total,
            cierre_id=cierre_id
        )
        
        logger.info(f"üéØ Consolidaci√≥n completada para cierre {cierre_id}:")
        logger.info(f"   üìã Total incidencias: {resultado_final.get('total_incidencias', 0)}")
        logger.info(f"   üîÑ Individuales: {resultado_final.get('total_incidencias_individuales', 0)}")
        logger.info(f"   üìä Suma total: {resultado_final.get('total_incidencias_suma', 0)}")
        
        return resultado_final
        
    except Exception as e:
        logger.error(f"‚ùå Error consolidando resultados para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id
        }
    
    # C√ìDIGO ORIGINAL COMENTADO:
    # from .utils.GenerarIncidencias import generar_todas_incidencias
    # from .models import CierreNomina
    # 
    # logger.info(f"Iniciando generaci√≥n de incidencias para cierre {cierre_id}")
    # 
    # try:
    #     cierre = CierreNomina.objects.get(id=cierre_id)
    #     
    #     # Verificar que el cierre tenga los archivos necesarios procesados
    #     if not _verificar_archivos_listos_para_incidencias(cierre):
    #         raise ValueError("No todos los archivos est√°n procesados para generar incidencias")
    #     
    #     # FALTA: Verificar que discrepancias = 0 antes de continuar
    #     # FALTA: Implementar comparaci√≥n contra mes anterior espec√≠ficamente
    #     
    #     resultado = generar_todas_incidencias(cierre)
    #     
    #     logger.info(f"Incidencias generadas exitosamente para cierre {cierre_id}: {resultado['total_incidencias']} incidencias")
    #     return resultado
    #     
    # except Exception as e:
    #     logger.error(f"Error generando incidencias para cierre {cierre_id}: {e}")
    #     try:
    #         cierre = CierreNomina.objects.get(id=cierre_id)
    #         cierre.estado_incidencias = 'pendiente'
    #         cierre.save(update_fields=['estado_incidencias'])
    #     except:
    #         pass
    #     raise

def _verificar_archivos_listos_para_incidencias(cierre):
    """Verifica que los archivos necesarios est√©n procesados"""
    from .models import LibroRemuneracionesUpload, MovimientosMesUpload
    
    # Verificar libro de remuneraciones procesado
    libro = LibroRemuneracionesUpload.objects.filter(cierre=cierre).first()
    if not libro or libro.estado != 'procesado':
        logger.warning(f"Libro de remuneraciones no procesado para cierre {cierre.id}")
        return False
    
    # Verificar movimientos del mes procesados
    movimientos = MovimientosMesUpload.objects.filter(cierre=cierre).first()
    if not movimientos or movimientos.estado != 'procesado':
        logger.warning(f"Movimientos del mes no procesados para cierre {cierre.id}")
        return False
    
    return True


# ===== NUEVAS TAREAS CON SISTEMA DE LOGGING INTEGRADO =====

@shared_task
def analizar_headers_libro_remuneraciones_con_logging(libro_id, upload_log_id):
    """
    Analiza headers del libro de remuneraciones con logging integrado
    """
    from .models_logging import UploadLogNomina
    logger.info(f"Procesando libro de remuneraciones id={libro_id} con upload_log={upload_log_id}")
    
    try:
        # Obtener upload_log y actualizar estado
        upload_log = UploadLogNomina.objects.get(id=upload_log_id)
        upload_log.estado = "analizando_hdrs"
        upload_log.save()
        
        # Obtener libro
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        libro.estado = "analizando_hdrs"
        libro.save()
        
        # Procesar headers
        headers = obtener_headers_libro_remuneraciones(libro.archivo.path)
        libro.header_json = headers
        libro.estado = "hdrs_analizados"
        libro.save()
        
        # Actualizar upload_log
        upload_log.estado = "hdrs_analizados"
        upload_log.headers_detectados = headers
        upload_log.save()
        
        logger.info(f"Headers analizados exitosamente para libro {libro_id}")
        return {
            "libro_id": libro_id, 
            "upload_log_id": upload_log_id,
            "headers": headers
        }
        
    except Exception as e:
        logger.error(f"Error analizando headers libro {libro_id}: {e}")
        
        # Marcar errores en ambos modelos
        try:
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            libro.estado = "con_error"
            libro.save()
        except:
            pass
            
        try:
            upload_log = UploadLogNomina.objects.get(id=upload_log_id)
            upload_log.marcar_como_error(f"Error analizando headers: {str(e)}")
        except:
            pass
            
        raise


@shared_task  
def clasificar_headers_libro_remuneraciones_con_logging(result):
    """
    Clasifica headers del libro de remuneraciones con logging integrado
    """
    from .models_logging import UploadLogNomina
    
    libro_id = result["libro_id"]
    upload_log_id = result["upload_log_id"]
    
    logger.info(f"Clasificando headers para libro {libro_id} con upload_log {upload_log_id}")
    
    try:
        # Obtener modelos
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        upload_log = UploadLogNomina.objects.get(id=upload_log_id)
        cierre = libro.cierre
        cliente = cierre.cliente
        
        # Actualizar estados
        libro.estado = "clasif_en_proceso"
        libro.save()
        upload_log.estado = "clasif_en_proceso"
        upload_log.save()
        
        # Obtener headers
        headers = (
            libro.header_json
            if isinstance(libro.header_json, list)
            else result["headers"]
        )
        
        # Clasificar headers
        headers_clasificados, headers_sin_clasificar = (
            clasificar_headers_libro_remuneraciones(headers, cliente)
        )
        
        # Actualizar libro
        libro.header_json = {
            "headers_clasificados": headers_clasificados,
            "headers_sin_clasificar": headers_sin_clasificar,
        }
        
        # Determinar estado final
        if headers_sin_clasificar:
            libro.estado = "clasif_pendiente"
            upload_log.estado = "clasif_pendiente"
        else:
            libro.estado = "clasificado"
            upload_log.estado = "clasificado"
        
        libro.save()
        
        # Actualizar upload_log con resumen
        resumen_final = {
            "libro_id": libro_id,
            "headers_total": len(headers),
            "headers_clasificados": len(headers_clasificados),
            "headers_sin_clasificar": len(headers_sin_clasificar),
            "clasificados": headers_clasificados,
            "sin_clasificar": headers_sin_clasificar
        }
        
        upload_log.resumen = resumen_final
        upload_log.save()
        
        logger.info(
            f"Libro {libro_id}: {len(headers_clasificados)} headers clasificados, "
            f"{len(headers_sin_clasificar)} sin clasificar"
        )
        
        return {
            "libro_id": libro_id,
            "upload_log_id": upload_log_id,
            "headers_clasificados": len(headers_clasificados),
            "headers_sin_clasificar": len(headers_sin_clasificar),
            "estado_final": libro.estado
        }
        
    except Exception as e:
        logger.error(f"Error clasificando headers para libro {libro_id}: {e}")
        
        # Marcar errores en ambos modelos
        try:
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            libro.estado = "con_error"
            libro.save()
        except:
            pass
            
        try:
            upload_log = UploadLogNomina.objects.get(id=upload_log_id)
            upload_log.marcar_como_error(f"Error clasificando headers: {str(e)}")
        except:
            pass
            
        raise


# ======== TAREAS DE AN√ÅLISIS DE DATOS ========

@shared_task
def analizar_datos_cierre_task(cierre_id, tolerancia_variacion=30.0):
    """
    Tarea para analizar datos del cierre actual vs mes anterior
    y generar incidencias de variaci√≥n salarial
    """
    from .models import CierreNomina, AnalisisDatosCierre, IncidenciaVariacionSalarial
    from .models import EmpleadoCierre, AnalistaIngreso, AnalistaFiniquito, AnalistaIncidencia
    from django.utils import timezone
    from decimal import Decimal
    import logging
    
    logger = logging.getLogger(__name__)
    logger.info(f"Iniciando an√°lisis de datos para cierre {cierre_id}")
    
    try:
        # Obtener cierre actual
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        # Crear o actualizar an√°lisis
        analisis, created = AnalisisDatosCierre.objects.get_or_create(
            cierre=cierre,
            defaults={
                'tolerancia_variacion_salarial': Decimal(str(tolerancia_variacion)),
                'estado': 'procesando',
                'analista': cierre.analista_asignado if hasattr(cierre, 'analista_asignado') else None
            }
        )
        
        if not created:
            analisis.estado = 'procesando'
            analisis.tolerancia_variacion_salarial = Decimal(str(tolerancia_variacion))
            analisis.save()
        
        # 1. AN√ÅLISIS DE DATOS ACTUALES
        logger.info(f"Analizando datos actuales del cierre {cierre_id}")
        
        # Contar empleados actuales
        empleados_actuales = EmpleadoCierre.objects.filter(cierre=cierre).count()
        
        # Contar ingresos actuales
        ingresos_actuales = AnalistaIngreso.objects.filter(cierre=cierre).count()
        
        # Contar finiquitos actuales
        finiquitos_actuales = AnalistaFiniquito.objects.filter(cierre=cierre).count()
        
        # Contar ausentismos actuales por tipo
        ausentismos_actuales = AnalistaIncidencia.objects.filter(cierre=cierre)
        ausentismos_por_tipo_actual = {}
        ausentismos_total_actual = 0
        
        for ausentismo in ausentismos_actuales:
            tipo = ausentismo.tipo_ausentismo or 'Sin especificar'
            ausentismos_por_tipo_actual[tipo] = ausentismos_por_tipo_actual.get(tipo, 0) + 1
            ausentismos_total_actual += 1
        
        # 2. AN√ÅLISIS DE DATOS MES ANTERIOR
        logger.info(f"Analizando datos del mes anterior para cliente {cierre.cliente.id}")
        
        # Obtener cierre del mes anterior
        cierre_anterior = CierreNomina.objects.filter(
            cliente=cierre.cliente,
            periodo__lt=cierre.periodo
        ).order_by('-periodo').first()
        
        empleados_anterior = 0
        ingresos_anterior = 0
        finiquitos_anterior = 0
        ausentismos_por_tipo_anterior = {}
        ausentismos_total_anterior = 0
        
        if cierre_anterior:
            empleados_anterior = EmpleadoCierre.objects.filter(cierre=cierre_anterior).count()
            ingresos_anterior = AnalistaIngreso.objects.filter(cierre=cierre_anterior).count()
            finiquitos_anterior = AnalistaFiniquito.objects.filter(cierre=cierre_anterior).count()
            
            ausentismos_anteriores = AnalistaIncidencia.objects.filter(cierre=cierre_anterior)
            for ausentismo in ausentismos_anteriores:
                tipo = ausentismo.tipo_ausentismo or 'Sin especificar'
                ausentismos_por_tipo_anterior[tipo] = ausentismos_por_tipo_anterior.get(tipo, 0) + 1
                ausentismos_total_anterior += 1
        
        # 3. ACTUALIZAR AN√ÅLISIS CON DATOS RECOPILADOS
        analisis.cantidad_empleados_actual = empleados_actuales
        analisis.cantidad_ingresos_actual = ingresos_actuales
        analisis.cantidad_finiquitos_actual = finiquitos_actuales
        analisis.cantidad_ausentismos_actual = ausentismos_total_actual
        
        analisis.cantidad_empleados_anterior = empleados_anterior
        analisis.cantidad_ingresos_anterior = ingresos_anterior
        analisis.cantidad_finiquitos_anterior = finiquitos_anterior
        analisis.cantidad_ausentismos_anterior = ausentismos_total_anterior
        
        analisis.ausentismos_por_tipo_actual = ausentismos_por_tipo_actual
        analisis.ausentismos_por_tipo_anterior = ausentismos_por_tipo_anterior
        
        analisis.save()
        
        logger.info(f"Datos recopilados - Empleados: {empleados_actuales}, "
                   f"Ingresos: {ingresos_actuales}, Finiquitos: {finiquitos_actuales}, "
                   f"Ausentismos: {ausentismos_total_actual}")
        
        # 4. GENERAR INCIDENCIAS DE VARIACI√ìN SALARIAL
        logger.info(f"Generando incidencias de variaci√≥n salarial con tolerancia {tolerancia_variacion}%")
        
        # Limpiar incidencias existentes
        IncidenciaVariacionSalarial.objects.filter(cierre=cierre).delete()
        
        if cierre_anterior:
            # Obtener empleados de ambos meses
            empleados_actual = EmpleadoCierre.objects.filter(cierre=cierre).select_related('empleado')
            empleados_anterior = EmpleadoCierre.objects.filter(cierre=cierre_anterior).select_related('empleado')
            
            # Crear mapas por RUT
            empleados_actual_map = {emp.rut: emp for emp in empleados_actual}
            empleados_anterior_map = {emp.rut: emp for emp in empleados_anterior}
            
            incidencias_creadas = 0
            
            # Comparar empleados que existen en ambos meses
            for rut, empleado_actual in empleados_actual_map.items():
                if rut in empleados_anterior_map:
                    empleado_anterior = empleados_anterior_map[rut]
                    
                    sueldo_actual = empleado_actual.sueldo_base or 0
                    sueldo_anterior = empleado_anterior.sueldo_base or 0
                    
                    if sueldo_anterior > 0:  # Evitar divisi√≥n por cero
                        variacion = ((sueldo_actual - sueldo_anterior) / sueldo_anterior) * 100
                        
                        if abs(variacion) > tolerancia_variacion:
                            # Crear incidencia
                            IncidenciaVariacionSalarial.objects.create(
                                analisis=analisis,
                                cierre=cierre,
                                rut_empleado=rut,
                                nombre_empleado=empleado_actual.nombre_completo,
                                sueldo_base_actual=sueldo_actual,
                                sueldo_base_anterior=sueldo_anterior,
                                porcentaje_variacion=Decimal(str(round(variacion, 2))),
                                tipo_variacion='aumento' if variacion > 0 else 'disminucion',
                                estado='pendiente'
                            )
                            incidencias_creadas += 1
            
            logger.info(f"Creadas {incidencias_creadas} incidencias de variaci√≥n salarial")
        
        # 5. FINALIZAR AN√ÅLISIS
        analisis.estado = 'completado'
        analisis.fecha_completado = timezone.now()
        analisis.save()
        
        # 6. ACTUALIZAR ESTADO DEL CIERRE
        incidencias_variacion_count = IncidenciaVariacionSalarial.objects.filter(cierre=cierre).count()
        
        if incidencias_variacion_count > 0:
            # Hay incidencias de variaci√≥n salarial que resolver
            cierre.estado = 'validacion_incidencias'
            cierre.estado_incidencias = 'en_revision'
        else:
            # No hay incidencias de variaci√≥n salarial
            cierre.estado = 'listo_para_entrega'
            cierre.estado_incidencias = 'resueltas'
        
        cierre.save(update_fields=['estado', 'estado_incidencias'])
        
        logger.info(f"An√°lisis completado para cierre {cierre_id}. Estado: {cierre.estado}, Estado incidencias: {cierre.estado_incidencias}")
        
        return {
            "cierre_id": cierre_id,
            "analisis_id": analisis.id,
            "empleados_actual": empleados_actuales,
            "empleados_anterior": empleados_anterior,
            "ingresos_actual": ingresos_actuales,
            "finiquitos_actual": finiquitos_actuales,
            "ausentismos_actual": ausentismos_total_actual,
            "incidencias_variacion_creadas": incidencias_variacion_count,
            "estado": "completado",
            "estado_incidencias": cierre.estado_incidencias
        }
        
    except Exception as e:
        logger.error(f"Error en an√°lisis de datos para cierre {cierre_id}: {e}")
        
        # Marcar an√°lisis como error
        try:
            analisis = AnalisisDatosCierre.objects.get(cierre_id=cierre_id)
            analisis.estado = 'error'
            analisis.save()
        except:
            pass
            
        raise


# ===== TAREAS PARA SISTEMA DE DISCREPANCIAS (VERIFICACI√ìN DE DATOS) =====

@shared_task
def generar_discrepancias_cierre_task(cierre_id):
    """
    Tarea para generar discrepancias en la verificaci√≥n de datos de un cierre.
    Sistema simplificado - solo detecta y registra diferencias.
    """
    from .utils.GenerarDiscrepancias import generar_todas_discrepancias
    from .models import CierreNomina
    
    logger.info(f"Iniciando generaci√≥n de discrepancias para cierre {cierre_id}")
    
    try:
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        # Cambiar estado a verificacion_datos al iniciar
        cierre.estado = 'verificacion_datos'
        cierre.save(update_fields=['estado'])
        
        # Verificar que el cierre tenga los archivos necesarios procesados
        if not _verificar_archivos_listos_para_discrepancias(cierre):
            raise ValueError("No todos los archivos est√°n procesados para generar discrepancias")
        
        # Eliminar discrepancias anteriores si existen
        cierre.discrepancias.all().delete()
        
        # Generar nuevas discrepancias
        resultado = generar_todas_discrepancias(cierre)
        
        # Actualizar estado del cierre
        if resultado['total_discrepancias'] == 0:
            # Sin discrepancias - datos verificados
            cierre.estado = 'verificado_sin_discrepancias'
        else:
            # Con discrepancias - requiere correcci√≥n
            cierre.estado = 'con_discrepancias'
        
        cierre.save(update_fields=['estado'])
        
        logger.info(f"Discrepancias generadas exitosamente para cierre {cierre_id}: {resultado['total_discrepancias']} discrepancias")
        return resultado
        
    except Exception as e:
        logger.error(f"Error generando discrepancias para cierre {cierre_id}: {e}")
        try:
            cierre = CierreNomina.objects.get(id=cierre_id)
            cierre.estado = 'archivos_completos'  # Volver al estado anterior correcto
            cierre.save(update_fields=['estado'])
        except:
            pass
        raise

def _verificar_archivos_listos_para_discrepancias(cierre):
    """Verifica que los archivos necesarios est√©n procesados para generar discrepancias"""
    from .models import LibroRemuneracionesUpload, MovimientosMesUpload
    
    # Verificar libro de remuneraciones procesado
    libro = LibroRemuneracionesUpload.objects.filter(cierre=cierre).first()
    if not libro or libro.estado not in ['clasificado', 'procesado']:
        logger.warning(f"Libro de remuneraciones no procesado para cierre {cierre.id}")
        return False
    
    # Verificar movimientos del mes procesados
    movimientos = MovimientosMesUpload.objects.filter(cierre=cierre).first()
    if not movimientos or movimientos.estado != 'procesado':
        logger.warning(f"Movimientos del mes no procesados para cierre {cierre.id}")
        return False
    
    # Verificar al menos un archivo del analista procesado
    archivos_analista = cierre.archivos_analista.filter(estado='procesado')
    if archivos_analista.count() == 0:
        logger.warning(f"No hay archivos del analista procesados para cierre {cierre.id}")
        return False
    
    # Archivo de novedades es opcional - si existe debe estar procesado
    archivo_novedades = cierre.archivos_novedades.first()
    if archivo_novedades and archivo_novedades.estado not in ['clasificado', 'procesado']:
        logger.warning(f"Archivo de novedades no procesado para cierre {cierre.id}")
        return False
    
    return True


# ===== TAREAS PARA CONSOLIDACI√ìN DE INFORMACI√ìN =====

@shared_task
def consolidar_cierre_task(cierre_id, usuario_id=None):
    """
    üéØ TASK PARA CONSOLIDAR INFORMACI√ìN DE UN CIERRE
    
    Ejecuta el proceso de consolidaci√≥n despu√©s de resolver discrepancias.
    """
    from .utils.ConsolidarInformacion import consolidar_cierre_completo
    from .models import CierreNomina
    from django.contrib.auth import get_user_model
    
    User = get_user_model()
    usuario = None
    if usuario_id:
        try:
            usuario = User.objects.get(id=usuario_id)
        except User.DoesNotExist:
            pass
    
    logger.info(f"üöÄ Iniciando consolidaci√≥n para cierre {cierre_id}")
    
    try:
        # Ejecutar consolidaci√≥n
        resultado = consolidar_cierre_completo(cierre_id, usuario)
        
        logger.info(f"‚úÖ Consolidaci√≥n exitosa para cierre {cierre_id}: {resultado['estadisticas']}")
        return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Error en consolidaci√≥n de cierre {cierre_id}: {e}")
        
        # Marcar cierre con error
        try:
            cierre = CierreNomina.objects.get(id=cierre_id)
            cierre.estado_consolidacion = 'error_consolidacion'
            cierre.save(update_fields=['estado_consolidacion'])
        except:
            pass
            
        raise


@shared_task
def generar_incidencias_consolidadas_task(cierre_id):
    """
    Tarea para generar incidencias de un cierre consolidado
    """
    from .utils.DetectarIncidenciasConsolidadas import generar_incidencias_consolidadas_task as detectar_incidencias
    
    logger.info(f"üîç Iniciando detecci√≥n de incidencias consolidadas para cierre {cierre_id}")
    
    try:
        # Ejecutar detecci√≥n de incidencias
        resultado = detectar_incidencias(cierre_id)
        
        if resultado['success']:
            logger.info(f"‚úÖ Detecci√≥n exitosa para cierre {cierre_id}: {resultado['total_incidencias']} incidencias")
        else:
            logger.error(f"‚ùå Error en detecci√≥n para cierre {cierre_id}: {resultado.get('error', 'Error desconocido')}")
        
        return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico en detecci√≥n de incidencias para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e)
        }


@shared_task
def consolidar_datos_nomina_task_optimizado(cierre_id):
    """
    üöÄ TAREA OPTIMIZADA: CONSOLIDACI√ìN DE DATOS DE N√ìMINA CON CELERY CHORD
    
    Utiliza paralelizaci√≥n para optimizar el proceso de consolidaci√≥n:
    1. Ejecuta tareas en paralelo usando chord
    2. Consolida resultados al final
    3. Reduce significativamente el tiempo de procesamiento
    """
    import logging
    from django.utils import timezone
    from celery import chord
    
    logger = logging.getLogger(__name__)
    logger.info(f"üöÄ Iniciando consolidaci√≥n OPTIMIZADA de datos para cierre {cierre_id}")
    
    try:
        # 1. VERIFICAR PRERREQUISITOS
        from .models import CierreNomina, NominaConsolidada, HeaderValorEmpleado, MovimientoPersonal
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        logger.info(f"üìã Cierre obtenido: {cierre} - Estado: {cierre.estado}")
        
        # Verificar estado
        if cierre.estado not in ['verificado_sin_discrepancias', 'datos_consolidados']:
            raise ValueError(f"El cierre debe estar en 'verificado_sin_discrepancias' o 'datos_consolidados', actual: {cierre.estado}")
        
        # Cambiar estado a procesando
        cierre.estado = 'consolidando'
        cierre.save(update_fields=['estado'])
        
        # 2. VERIFICAR ARCHIVOS PROCESADOS
        libro = cierre.libros_remuneraciones.filter(estado='procesado').first()
        movimientos = cierre.movimientos_mes.filter(estado='procesado').first()
        
        if not libro:
            raise ValueError("No hay libro de remuneraciones procesado")
        if not movimientos:
            raise ValueError("No hay archivo de movimientos procesado")
            
        logger.info(f"üìö Libro: {libro.archivo.name}")
        logger.info(f"üîÑ Movimientos: {movimientos.archivo.name}")
        
        # 3. LIMPIAR CONSOLIDACI√ìN ANTERIOR (SI EXISTE)
        consolidaciones_eliminadas = cierre.nomina_consolidada.count()
        if consolidaciones_eliminadas > 0:
            logger.info(f"üóëÔ∏è Eliminando {consolidaciones_eliminadas} registros de consolidaci√≥n anterior...")
            # Limpiar tambi√©n MovimientoPersonal relacionados
            movimientos_eliminados = MovimientoPersonal.objects.filter(nomina_consolidada__cierre=cierre).count()
            MovimientoPersonal.objects.filter(nomina_consolidada__cierre=cierre).delete()
            
            cierre.nomina_consolidada.all().delete()
            logger.info(f"‚úÖ {consolidaciones_eliminadas} registros de consolidaci√≥n anterior eliminados exitosamente")
            logger.info(f"‚úÖ {movimientos_eliminados} movimientos de personal anteriores eliminados exitosamente")
        else:
            logger.info("‚ÑπÔ∏è No hay consolidaci√≥n anterior que eliminar")
        
        # 3.5. C√ÅLCULO DIN√ÅMICO DE CHUNKS
        from .models import EmpleadoCierre
        empleados_count = EmpleadoCierre.objects.filter(cierre=cierre).count()
        chunk_size = calcular_chunk_size_dinamico(empleados_count)
        logger.info(f"üìä Procesando {empleados_count} empleados con chunk size din√°mico: {chunk_size}")
        
        # 4. INICIAR ORQUESTACI√ìN: EMPLEADOS (paralelo interno) ‚Üí MOVIMIENTOS (secuencial) ‚Üí CONCEPTOS (final)
        logger.info("üéØ Iniciando orquestaci√≥n: empleados en paralelo interno; movimientos luego de forma secuencial; conceptos al final")

        flujo = chain(
            # Empleados del libro (usa su propio procesamiento batch interno)
            procesar_empleados_libro_paralelo.s(cierre_id, chunk_size),
            # Movimientos deben correr despu√©s de que existan las n√≥minas consolidadas
            procesar_movimientos_personal_paralelo.si(cierre_id),
            # Finalizaci√≥n: conceptos y cambio de estado
            finalizar_consolidacion_post_movimientos.si(cierre_id)
        )

        resultado_flujo = flujo.apply_async()

        logger.info(f"üîó Cadena de consolidaci√≥n iniciada para cierre {cierre_id}")
        logger.info("üìù Nota: Movimientos se ejecutan secuencialmente tras empleados; conceptos en la etapa final")

        return {
            'success': True,
            'cierre_id': cierre_id,
            'chain_id': getattr(resultado_flujo, 'id', None),
            'modo': 'optimizado_empleados_paralelo_movs_secuencial',
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en consolidaci√≥n optimizada para cierre {cierre_id}: {e}")
        
        # Revertir estado en caso de error
        try:
            cierre = CierreNomina.objects.get(id=cierre_id)
            cierre.estado = 'verificado_sin_discrepancias'
            cierre.save(update_fields=['estado'])
        except:
            pass
            
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id,
            'modo': 'optimizado_paralelo'
        }


@shared_task
def procesar_empleados_libro_paralelo(cierre_id, chunk_size=50):
    """
    üìö TAREA PARALELA: Procesar empleados del libro de remuneraciones
    
    Args:
        cierre_id: ID del cierre a procesar
        chunk_size: Tama√±o din√°mico de chunks para optimizar el procesamiento
    """
    import logging
    from django.utils import timezone
    from decimal import Decimal, InvalidOperation
    
    logger = logging.getLogger(__name__)
    logger.info(f"üìö [PARALELO] Procesando empleados del libro para cierre {cierre_id} (chunk_size: {chunk_size})")
    
    try:
        from .models import CierreNomina, NominaConsolidada, HeaderValorEmpleado
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        libro = cierre.libros_remuneraciones.filter(estado='procesado').first()
        
        if not libro:
            raise ValueError("No hay libro de remuneraciones procesado")
        
        empleados_consolidados = 0
        headers_consolidados = 0
        
        # Procesar empleados del libro en lotes para optimizar
        empleados = cierre.empleados.all()
        total_empleados = empleados.count()
        logger.info(f"üë• Procesando {total_empleados} empleados en lotes (chunk_size: {chunk_size})")
        
        # Procesar en lotes usando el chunk_size din√°mico
        for i in range(0, total_empleados, chunk_size):
            batch_empleados = empleados[i:i + chunk_size]
            
            # Crear registros de n√≥mina consolidada en lote
            nominas_batch = []
            headers_batch = []
            
            for empleado in batch_empleados:
                # Crear registro de n√≥mina consolidada
                nomina_consolidada = NominaConsolidada(
                    cierre=cierre,
                    rut_empleado=normalizar_rut(empleado.rut),  # ‚úÖ NORMALIZAR RUT
                    nombre_empleado=f"{empleado.nombre} {empleado.apellido_paterno} {empleado.apellido_materno}".strip(),
                    estado_empleado='activo',
                    dias_trabajados=empleado.dias_trabajados,
                    fecha_consolidacion=timezone.now(),
                    fuente_datos={
                        'libro_id': libro.id,
                        'consolidacion_version': '2.0_optimizada',
                        'procesamiento': 'paralelo'
                    }
                )
                nominas_batch.append(nomina_consolidada)
            
            # Bulk create n√≥minas con manejo de duplicados
            NominaConsolidada.objects.bulk_create(nominas_batch, ignore_conflicts=True)
            empleados_consolidados += len(nominas_batch)
            
            # Procesar headers para este lote
            for j, empleado in enumerate(batch_empleados):
                try:
                    # Obtener o crear la n√≥mina consolidada (manejo de duplicados)
                    nomina_consolidada, created = NominaConsolidada.objects.get_or_create(
                        cierre=cierre, 
                        rut_empleado=normalizar_rut(empleado.rut),
                        defaults={
                            'nombre_empleado': f"{empleado.nombre} {empleado.apellido_paterno} {empleado.apellido_materno}".strip(),
                            'estado_empleado': 'activo',
                            'dias_trabajados': empleado.dias_trabajados,
                            'fecha_consolidacion': timezone.now(),
                            'fuente_datos': {
                                'libro_id': libro.id,
                                'consolidacion_version': '2.0_optimizada',
                                'procesamiento': 'paralelo'
                            }
                        }
                    )
                
                    # Crear HeaderValorEmpleado para cada concepto
                    conceptos_empleado = empleado.registroconceptoempleado_set.all()
                    
                    for concepto in conceptos_empleado:
                        # Determinar si es num√©rico
                        valor_numerico = None
                        es_numerico = False
                        
                        if concepto.monto:
                            try:
                                # Limpiar el valor mejorado
                                valor_limpio = str(concepto.monto).replace('$', '').replace(',', '').strip()
                                if valor_limpio and (valor_limpio.replace('-', '').replace('.', '').isdigit()):
                                    valor_numerico = Decimal(valor_limpio)
                                    es_numerico = True
                            except (ValueError, InvalidOperation, AttributeError):
                                pass
                        
                        # Crear registro HeaderValorEmpleado
                        header = HeaderValorEmpleado(
                            nomina_consolidada=nomina_consolidada,
                            nombre_header=concepto.nombre_concepto_original,
                            concepto_remuneracion=concepto.concepto,
                            valor_original=concepto.monto or '',
                            valor_numerico=valor_numerico,
                            es_numerico=es_numerico,
                            fuente_archivo='libro_remuneraciones',
                            fecha_consolidacion=timezone.now()
                        )
                        headers_batch.append(header)
                        
                except Exception as e:
                    logger.error(f"‚ùå Error procesando empleado {empleado.rut}: {e}")
                    continue
            
            # Bulk create headers cada cierto n√∫mero para evitar memoria excesiva
            if len(headers_batch) >= 500:
                HeaderValorEmpleado.objects.bulk_create(headers_batch)
                headers_consolidados += len(headers_batch)
                headers_batch = []
                
            logger.info(f"üìä Progreso: {empleados_consolidados}/{total_empleados} empleados procesados")
        
        # Insertar headers restantes
        if headers_batch:
            HeaderValorEmpleado.objects.bulk_create(headers_batch)
            headers_consolidados += len(headers_batch)
        
        logger.info(f"‚úÖ [PARALELO] Empleados procesados: {empleados_consolidados}, Headers: {headers_consolidados}")
        
        return {
            'success': True,
            'task': 'procesar_empleados_libro',
            'empleados_consolidados': empleados_consolidados,
            'headers_consolidados': headers_consolidados,
            'cierre_id': cierre_id
        }
        
    except Exception as e:
        logger.error(f"‚ùå [PARALELO] Error procesando empleados para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'task': 'procesar_empleados_libro',
            'error': str(e),
            'cierre_id': cierre_id
        }


@shared_task
def procesar_movimientos_personal_paralelo(cierre_id):
    """
    üîÑ TAREA PARALELA: Procesar movimientos de personal
    """
    import logging
    from django.utils import timezone
    
    logger = logging.getLogger(__name__)
    logger.info(f"üîÑ [PARALELO] Procesando movimientos de personal para cierre {cierre_id}")
    
    try:
        from .models import (
            CierreNomina, NominaConsolidada, MovimientoPersonal,
            MovimientoAltaBaja, MovimientoAusentismo, MovimientoVacaciones,
            MovimientoVariacionSueldo, MovimientoVariacionContrato
        )
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        movimientos_creados = 0
        
        # Verificar si hay archivo de movimientos procesado
        movimientos_archivo = cierre.movimientos_mes.filter(estado='procesado').first()
        if not movimientos_archivo:
            logger.warning(f"‚ö†Ô∏è [PARALELO] No hay archivo de movimientos procesado para cierre {cierre_id}")
            return {
                'success': True,
                'task': 'procesar_movimientos_personal',
                'movimientos_creados': 0,
                'cierre_id': cierre_id,
                'info': 'No hay archivo de movimientos procesado'
            }
        
        logger.info(f"üìÅ [PARALELO] Archivo de movimientos encontrado: {movimientos_archivo.archivo.name}")
        
        # Crear lista para bulk_create y contadores
        movimientos_batch = []
        contador_altas_bajas = 0
        contador_ausentismos = 0
        contador_vacaciones = 0
        contador_variaciones_sueldo = 0
        contador_variaciones_contrato = 0
        
        # 1. Procesar ALTAS y BAJAS
        altas_bajas = MovimientoAltaBaja.objects.filter(cierre=cierre)
        logger.info(f"üìä [PARALELO] Encontrados {altas_bajas.count()} registros de altas/bajas en BD")
        
        for movimiento in altas_bajas:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(movimiento.rut)  # ‚úÖ NORMALIZAR RUT
                )
                
                # Actualizar estado del empleado
                if movimiento.alta_o_baja == 'ALTA':
                    nomina_consolidada.estado_empleado = 'nueva_incorporacion'
                elif movimiento.alta_o_baja == 'BAJA':
                    nomina_consolidada.estado_empleado = 'finiquito'
                
                nomina_consolidada.save(update_fields=['estado_empleado'])
                
                # Crear MovimientoPersonal
                fecha_evt = movimiento.fecha_ingreso if movimiento.alta_o_baja == 'ALTA' else movimiento.fecha_retiro
                categoria = 'ingreso' if movimiento.alta_o_baja == 'ALTA' else 'finiquito'
                mov_personal = MovimientoPersonal(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento=categoria,
                    motivo=movimiento.motivo,
                    descripcion=movimiento.motivo,
                    fecha_movimiento=fecha_evt,
                    fecha_inicio=fecha_evt,
                    fecha_fin=fecha_evt,
                    dias_evento=1,
                    dias_en_periodo=1,
                    categoria=categoria,
                    subtipo=None,
                    multi_mes=False,
                    observaciones=f"Tipo contrato: {movimiento.tipo_contrato}, Sueldo base: ${movimiento.sueldo_base:,.0f}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_paralela_v2'
                )
                movimientos_batch.append(mov_personal)
                contador_altas_bajas += 1
                
            except NominaConsolidada.DoesNotExist:
                # Crear empleado si no existe (caso de finiquitos)
                if movimiento.alta_o_baja == 'BAJA':
                    nomina_consolidada = NominaConsolidada.objects.create(
                        cierre=cierre,
                        rut_empleado=normalizar_rut(movimiento.rut),
                        nombre_empleado=movimiento.nombres_apellidos,
                        estado_empleado='finiquito',
                        fecha_consolidacion=timezone.now(),
                        fuente_datos={'movimiento_finiquito': True}
                    )
                    
                    mov_personal = MovimientoPersonal(
                        nomina_consolidada=nomina_consolidada,
                        tipo_movimiento='finiquito',
                        categoria='finiquito',
                        motivo=movimiento.motivo,
                        descripcion=movimiento.motivo,
                        fecha_movimiento=movimiento.fecha_retiro,
                        fecha_inicio=movimiento.fecha_retiro,
                        fecha_fin=movimiento.fecha_retiro,
                        dias_evento=1,
                        dias_en_periodo=1,
                        multi_mes=False,
                        fecha_deteccion=timezone.now(),
                        detectado_por_sistema='consolidacion_paralela_v2'
                    )
                    movimientos_batch.append(mov_personal)
                    contador_altas_bajas += 1
        
        logger.info(f"‚úÖ [PARALELO] {contador_altas_bajas} movimientos de altas/bajas procesados")
        
        # 2. Procesar AUSENTISMOS
        ausentismos = MovimientoAusentismo.objects.filter(cierre=cierre)
        logger.info(f"üìä [PARALELO] Encontrados {ausentismos.count()} registros de ausentismo en BD")
        
        for ausentismo in ausentismos:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(ausentismo.rut)  # ‚úÖ NORMALIZAR RUT
                )
                
                # Actualizar estado si es ausencia total
                if ausentismo.dias >= 30:
                    nomina_consolidada.estado_empleado = 'ausente_total'
                    nomina_consolidada.dias_ausencia = ausentismo.dias
                else:
                    nomina_consolidada.estado_empleado = 'ausente_parcial'
                    nomina_consolidada.dias_ausencia = ausentismo.dias
                
                nomina_consolidada.save(update_fields=['estado_empleado', 'dias_ausencia'])
                
                # Normalizaci√≥n de ausentismo
                descripcion = f"{ausentismo.tipo} - {ausentismo.motivo}".strip(' -')
                subtipo = ausentismo.tipo.lower().replace(' ', '_') if ausentismo.tipo else 'sin_justificar'
                fecha_inicio = ausentismo.fecha_inicio_ausencia
                fecha_fin = ausentismo.fecha_fin_ausencia
                dias_evento = (fecha_fin - fecha_inicio).days + 1 if fecha_inicio and fecha_fin else ausentismo.dias
                # Clipping al periodo
                try:
                    year, month = map(int, cierre.periodo.split('-'))
                    from datetime import date
                    import calendar
                    start_periodo = date(year, month, 1)
                    last_day = calendar.monthrange(year, month)[1]
                    end_periodo = date(year, month, last_day)
                    clip_start = fecha_inicio if fecha_inicio > start_periodo else start_periodo
                    clip_end = fecha_fin if fecha_fin < end_periodo else end_periodo
                    dias_en_periodo = (clip_end - clip_start).days + 1 if clip_end >= clip_start else 0
                    multi_mes_flag = (fecha_inicio < start_periodo) or (fecha_fin > end_periodo)
                except Exception:
                    dias_en_periodo = ausentismo.dias
                    multi_mes_flag = False
                from hashlib import sha1
                base_hash = f"{nomina_consolidada.rut_empleado}:ausencia:{subtipo}:{fecha_inicio}:{fecha_fin}".lower()
                hash_evento = sha1(base_hash.encode('utf-8')).hexdigest()
                hash_registro_periodo = sha1(f"{hash_evento}:{cierre.periodo}".encode('utf-8')).hexdigest()
                mov_personal = MovimientoPersonal(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='ausentismo',
                    categoria='ausencia',
                    subtipo=subtipo or 'sin_justificar',
                    motivo=descripcion,
                    descripcion=descripcion,
                    dias_ausencia=ausentismo.dias,
                    fecha_movimiento=fecha_inicio,
                    fecha_inicio=fecha_inicio,
                    fecha_fin=fecha_fin,
                    dias_evento=dias_evento,
                    dias_en_periodo=dias_en_periodo,
                    multi_mes=multi_mes_flag,
                    hash_evento=hash_evento,
                    hash_registro_periodo=hash_registro_periodo,
                    observaciones=f"Desde: {ausentismo.fecha_inicio_ausencia} hasta: {ausentismo.fecha_fin_ausencia}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_paralela_v2'
                )
                movimientos_batch.append(mov_personal)
                contador_ausentismos += 1
                
            except NominaConsolidada.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {ausentismo.rut} en ausentismo")
                continue
        
        logger.info(f"‚úÖ [PARALELO] {contador_ausentismos} movimientos de ausentismo procesados")
        
        # 3. Procesar VACACIONES
        vacaciones = MovimientoVacaciones.objects.filter(cierre=cierre)
        logger.info(f"üìä [PARALELO] Encontrados {vacaciones.count()} registros de vacaciones en BD")
        
        for vacacion in vacaciones:
            try:
                # üîç DEBUG: Agregar logging para debugging del matching
                rut_original = vacacion.rut
                rut_normalizado = normalizar_rut(vacacion.rut)
                logger.debug(f"üîç Buscando vacaci√≥n - RUT original: '{rut_original}' ‚Üí RUT normalizado: '{rut_normalizado}'")
                
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=rut_normalizado  # ‚úÖ NORMALIZAR RUT
                )
                
                # Vacaciones como subtipo ausencia
                fecha_inicio = vacacion.fecha_inicio
                fecha_fin = vacacion.fecha_fin_vacaciones
                dias_evento = (fecha_fin - fecha_inicio).days + 1 if fecha_inicio and fecha_fin else vacacion.cantidad_dias
                try:
                    year, month = map(int, cierre.periodo.split('-'))
                    from datetime import date
                    import calendar
                    start_periodo = date(year, month, 1)
                    last_day = calendar.monthrange(year, month)[1]
                    end_periodo = date(year, month, last_day)
                    clip_start = fecha_inicio if fecha_inicio > start_periodo else start_periodo
                    clip_end = fecha_fin if fecha_fin < end_periodo else end_periodo
                    dias_en_periodo = (clip_end - clip_start).days + 1 if clip_end >= clip_start else 0
                    multi_mes_flag = (fecha_inicio < start_periodo) or (fecha_fin > end_periodo)
                except Exception:
                    dias_en_periodo = vacacion.cantidad_dias
                    multi_mes_flag = False
                from hashlib import sha1
                base_hash = f"{nomina_consolidada.rut_empleado}:ausencia:vacaciones:{fecha_inicio}:{fecha_fin}".lower()
                hash_evento = sha1(base_hash.encode('utf-8')).hexdigest()
                hash_registro_periodo = sha1(f"{hash_evento}:{cierre.periodo}".encode('utf-8')).hexdigest()
                mov_personal = MovimientoPersonal(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='ausentismo',
                    categoria='ausencia',
                    subtipo='vacaciones',
                    motivo='Vacaciones',
                    descripcion='Vacaciones',
                    dias_ausencia=vacacion.cantidad_dias,
                    fecha_movimiento=fecha_inicio,
                    fecha_inicio=fecha_inicio,
                    fecha_fin=fecha_fin,
                    dias_evento=dias_evento,
                    dias_en_periodo=dias_en_periodo,
                    multi_mes=multi_mes_flag,
                    hash_evento=hash_evento,
                    hash_registro_periodo=hash_registro_periodo,
                    observaciones=f"Vacaciones desde: {vacacion.fecha_inicio} hasta: {vacacion.fecha_fin_vacaciones}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_paralela_v2'
                )
                movimientos_batch.append(mov_personal)
                contador_vacaciones += 1
                
            except NominaConsolidada.DoesNotExist:
                # üîç DEBUG: Agregar informaci√≥n detallada del error
                rut_normalizado = normalizar_rut(vacacion.rut)
                empleados_en_cierre = NominaConsolidada.objects.filter(cierre=cierre).count()
                existe_rut = NominaConsolidada.objects.filter(cierre=cierre, rut_empleado=rut_normalizado).exists()
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {vacacion.rut} ‚Üí {rut_normalizado} en vacaciones")
                logger.warning(f"   üìä Empleados en cierre: {empleados_en_cierre}, ¬øExiste RUT?: {existe_rut}")
                if existe_rut:
                    # Si existe el RUT pero no se encontr√≥, hay un problema en la query
                    empleado = NominaConsolidada.objects.get(cierre=cierre, rut_empleado=rut_normalizado)
                    logger.error(f"   üö® ERROR: RUT existe pero query fall√≥. Empleado encontrado: {empleado.id}")
                continue
        
        logger.info(f"‚úÖ [PARALELO] {contador_vacaciones} movimientos de vacaciones procesados")
        
        # 4. Procesar VARIACIONES DE SUELDO
        variaciones_sueldo = MovimientoVariacionSueldo.objects.filter(cierre=cierre)
        logger.info(f"üìä [PARALELO] Encontrados {variaciones_sueldo.count()} registros de variaciones de sueldo en BD")
        
        for variacion in variaciones_sueldo:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(variacion.rut)  # ‚úÖ NORMALIZAR RUT
                )
                
                # El modelo MovimientoVariacionSueldo no tiene campo 'motivo'. Usar descripci√≥n informativa
                motivo_text = f"Cambio de sueldo: de ${variacion.sueldo_base_anterior:,.0f} a ${variacion.sueldo_base_actual:,.0f}"
                mov_personal = MovimientoPersonal(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='cambio_sueldo',
                    motivo=motivo_text,
                    # Algunos modelos no tienen 'fecha_cambio' ‚Äî usar fallback seguro
                    fecha_movimiento=getattr(variacion, 'fecha_cambio', getattr(variacion, 'fecha_ingreso', timezone.now())),
                    observaciones=f"De ${variacion.sueldo_base_anterior:,.0f} a ${variacion.sueldo_base_actual:,.0f}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_paralela_v2'
                )
                movimientos_batch.append(mov_personal)
                contador_variaciones_sueldo += 1
                
            except NominaConsolidada.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {variacion.rut} en variaci√≥n sueldo")
                continue
        
        logger.info(f"‚úÖ [PARALELO] {contador_variaciones_sueldo} movimientos de variaci√≥n de sueldo procesados")
        
        # 5. Procesar VARIACIONES DE CONTRATO
        variaciones_contrato = MovimientoVariacionContrato.objects.filter(cierre=cierre)
        logger.info(f"üìä [PARALELO] Encontrados {variaciones_contrato.count()} registros de variaciones de contrato en BD")
        
        for variacion in variaciones_contrato:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(variacion.rut)  # ‚úÖ NORMALIZAR RUT
                )
                
                # El modelo MovimientoVariacionContrato no tiene campo 'motivo' ni 'tipo_contrato_nuevo'
                motivo_text = f"Cambio de contrato: de {variacion.tipo_contrato_anterior} a {variacion.tipo_contrato_actual}"
                mov_personal = MovimientoPersonal(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='cambio_contrato',
                    motivo=motivo_text,
                    # Algunos modelos no tienen 'fecha_cambio' ‚Äî usar fallback seguro
                    fecha_movimiento=getattr(variacion, 'fecha_cambio', getattr(variacion, 'fecha_ingreso', timezone.now())),
                    observaciones=f"De {variacion.tipo_contrato_anterior} a {variacion.tipo_contrato_actual}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_paralela_v2'
                )
                movimientos_batch.append(mov_personal)
                contador_variaciones_contrato += 1
                
            except NominaConsolidada.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {variacion.rut} en variaci√≥n contrato")
                continue
        
        logger.info(f"‚úÖ [PARALELO] {contador_variaciones_contrato} movimientos de variaci√≥n de contrato procesados")
        
        # Bulk create movimientos en lotes
        if movimientos_batch:
            batch_size = 100
            for i in range(0, len(movimientos_batch), batch_size):
                batch = movimientos_batch[i:i + batch_size]
                MovimientoPersonal.objects.bulk_create(batch)
                movimientos_creados += len(batch)
        
        # Resumen detallado
        total_tipos = contador_altas_bajas + contador_ausentismos + contador_vacaciones + contador_variaciones_sueldo + contador_variaciones_contrato
        logger.info(f"üìã [PARALELO] RESUMEN DETALLADO DE MOVIMIENTOS:")
        logger.info(f"    ‚¨ÜÔ∏è Altas/Bajas: {contador_altas_bajas}")
        logger.info(f"    üè• Ausentismos: {contador_ausentismos}")
        logger.info(f"    üèñÔ∏è Vacaciones: {contador_vacaciones}")
        logger.info(f"    üí∞ Variaciones Sueldo: {contador_variaciones_sueldo}")
        logger.info(f"    üìë Variaciones Contrato: {contador_variaciones_contrato}")
        logger.info(f"    üìä TOTAL TIPOS: {total_tipos}")
        logger.info(f"    ‚úÖ TOTAL CREADOS: {movimientos_creados}")
        
        if total_tipos != movimientos_creados:
            logger.warning(f"‚ö†Ô∏è [PARALELO] DISCREPANCIA: {total_tipos} tipos procesados vs {movimientos_creados} movimientos creados")
        
        return {
            'success': True,
            'task': 'procesar_movimientos_personal',
            'movimientos_creados': movimientos_creados,
            'cierre_id': cierre_id
        }
        
    except Exception as e:
        logger.error(f"‚ùå [PARALELO] Error procesando movimientos para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'task': 'procesar_movimientos_personal',
            'error': str(e),
            'cierre_id': cierre_id
        }


@shared_task
def procesar_conceptos_consolidados_paralelo(cierre_id):
    """
    üí∞ TAREA PARALELA: Procesar conceptos consolidados y calcular totales
    """
    import logging
    from django.utils import timezone
    from decimal import Decimal
    
    logger = logging.getLogger(__name__)
    logger.info(f"üí∞ [PARALELO] Procesando conceptos consolidados para cierre {cierre_id}")
    
    try:
        from .models import CierreNomina, NominaConsolidada, HeaderValorEmpleado, ConceptoConsolidado
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        conceptos_consolidados = 0
        
        # Obtener todas las n√≥minas consolidadas
        nominas = NominaConsolidada.objects.filter(cierre=cierre).prefetch_related(
            'header_valores__concepto_remuneracion'
        )
        
        conceptos_batch = []
        
        logger.info(f"üìä Procesando conceptos para {nominas.count()} empleados")
        
        for nomina_consolidada in nominas:
            # Obtener todos los headers para este empleado
            headers_empleado = nomina_consolidada.header_valores.filter(
                es_numerico=True,
                concepto_remuneracion__isnull=False
            )
            
            # Totales por nuevas categor√≠as
            total_haberes_imponibles = Decimal('0')
            total_haberes_no_imponibles = Decimal('0')
            total_dctos_legales = Decimal('0')
            total_otros_dctos = Decimal('0')
            total_impuestos = Decimal('0')
            total_horas_extras = Decimal('0')
            total_aportes_patronales = Decimal('0')
            
            # Agrupar por concepto y sumar
            conceptos_agrupados = {}
            
            for header in headers_empleado:
                concepto_nombre = header.concepto_remuneracion.nombre_concepto
                clasificacion = header.concepto_remuneracion.clasificacion
                
                if concepto_nombre not in conceptos_agrupados:
                    conceptos_agrupados[concepto_nombre] = {
                        'clasificacion': clasificacion,
                        'monto_total': Decimal('0'),
                        'cantidad': 0,
                        'concepto_obj': header.concepto_remuneracion
                    }
                
                conceptos_agrupados[concepto_nombre]['monto_total'] += header.valor_numerico
                conceptos_agrupados[concepto_nombre]['cantidad'] += 1
                
                # Sumar a la categor√≠a correspondiente seg√∫n clasificaci√≥n proporcionada por analista
                if clasificacion == 'haberes_imponibles':
                    total_haberes_imponibles += header.valor_numerico
                elif clasificacion == 'haberes_no_imponibles':
                    total_haberes_no_imponibles += header.valor_numerico
                elif clasificacion == 'horas_extras':
                    # acumular cantidad de horas (usar cantidad si disponible)
                    try:
                        total_horas_extras += Decimal(header.valor_numerico) if header.valor_numerico is not None else Decimal('0')
                    except Exception:
                        total_horas_extras += Decimal('0')
                elif clasificacion == 'descuentos_legales':
                    total_dctos_legales += header.valor_numerico
                elif clasificacion == 'otros_descuentos':
                    total_otros_dctos += header.valor_numerico
                elif clasificacion == 'impuestos':
                    total_impuestos += header.valor_numerico
                elif clasificacion == 'aportes_patronales':
                    total_aportes_patronales += header.valor_numerico
            
            # Crear ConceptoConsolidado para cada concepto agrupado
            for concepto_nombre, datos in conceptos_agrupados.items():
                # Mapear clasificaci√≥n a tipo_concepto
                clasificacion_mapping = {
                    'haberes_imponibles': 'haber_imponible',
                    'haberes_no_imponibles': 'haber_no_imponible',
                    'descuentos_legales': 'descuento_legal',
                    'otros_descuentos': 'otro_descuento',
                    'aportes_patronales': 'aporte_patronal',
                    'impuestos': 'impuesto'
                }
                
                tipo_concepto = clasificacion_mapping.get(datos['clasificacion'], 'informativo')
                codigo_concepto = str(datos['concepto_obj'].id) if datos['concepto_obj'] else None
                
                concepto_consolidado = ConceptoConsolidado(
                    nomina_consolidada=nomina_consolidada,
                    codigo_concepto=codigo_concepto,
                    nombre_concepto=concepto_nombre,
                    tipo_concepto=tipo_concepto,
                    monto_total=datos['monto_total'],
                    cantidad=datos['cantidad'],
                    es_numerico=True,
                    fuente_archivo='libro_remuneraciones'
                )
                conceptos_batch.append(concepto_consolidado)
            
            # Actualizar nuevos campos por categor√≠a en la n√≥mina consolidada
            nomina_consolidada.haberes_imponibles = total_haberes_imponibles
            nomina_consolidada.haberes_no_imponibles = total_haberes_no_imponibles
            nomina_consolidada.dctos_legales = total_dctos_legales
            nomina_consolidada.otros_dctos = total_otros_dctos
            nomina_consolidada.impuestos = total_impuestos
            nomina_consolidada.horas_extras_cantidad = total_horas_extras
            nomina_consolidada.aportes_patronales = total_aportes_patronales
            nomina_consolidada.save(update_fields=['haberes_imponibles', 'haberes_no_imponibles', 'dctos_legales', 'otros_dctos', 'impuestos', 'horas_extras_cantidad', 'aportes_patronales'])
        
        # Bulk create conceptos
        if conceptos_batch:
            batch_size = 200
            for i in range(0, len(conceptos_batch), batch_size):
                batch = conceptos_batch[i:i + batch_size]
                ConceptoConsolidado.objects.bulk_create(batch)
                conceptos_consolidados += len(batch)
        
        logger.info(f"‚úÖ [PARALELO] Conceptos consolidados procesados: {conceptos_consolidados}")
        
        return {
            'success': True,
            'task': 'procesar_conceptos_consolidados',
            'conceptos_consolidados': conceptos_consolidados,
            'cierre_id': cierre_id
        }
        
    except Exception as e:
        logger.error(f"‚ùå [PARALELO] Error procesando conceptos para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'task': 'procesar_conceptos_consolidados',
            'error': str(e),
            'cierre_id': cierre_id
        }


@shared_task
def finalizar_consolidacion_post_movimientos(cierre_id):
    """
    üéØ TAREA FINAL: Ejecuta el procesamiento de conceptos y cierra la consolidaci√≥n
    Debe correrse despu√©s de empleados y movimientos.
    """
    from django.utils import timezone
    logger = logging.getLogger(__name__)
    logger.info(f"üéØ [FINAL] Ejecutando conceptos y finalizando consolidaci√≥n para cierre {cierre_id}")

    try:
        from .models import CierreNomina

        # Procesar conceptos
        resultado_conceptos = procesar_conceptos_consolidados_paralelo(cierre_id)
        conceptos_consolidados = 0
        if resultado_conceptos.get('success', False):
            conceptos_consolidados = resultado_conceptos.get('conceptos_consolidados', 0)
            logger.info(f"‚úÖ [FINAL] Conceptos consolidados: {conceptos_consolidados}")
        else:
            logger.error(f"‚ùå [FINAL] Error procesando conceptos: {resultado_conceptos.get('error', 'Error desconocido')}")

        # Poner estado final del cierre
        cierre = CierreNomina.objects.get(id=cierre_id)
        cierre.estado = 'datos_consolidados'
        cierre.fecha_consolidacion = timezone.now()
        cierre.save(update_fields=['estado', 'fecha_consolidacion'])

        return {
            'success': True,
            'cierre_id': cierre_id,
            'conceptos_consolidados': conceptos_consolidados,
            'nuevo_estado': cierre.estado
        }
    except Exception as e:
        logger.error(f"‚ùå [FINAL] Error finalizando consolidaci√≥n para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id
        }


@shared_task
def consolidar_resultados_finales(resultados_paralelos, cierre_id):
    """
    üéØ TAREA FINAL: Consolidar resultados de todas las tareas paralelas
    """
    import logging
    from django.utils import timezone
    
    logger = logging.getLogger(__name__)
    logger.info(f"üéØ [FINAL] Consolidando resultados finales para cierre {cierre_id}")
    
    try:
        from .models import CierreNomina
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        # Procesar resultados de todas las tareas paralelas
        empleados_consolidados = 0
        headers_consolidados = 0
        movimientos_creados = 0
        conceptos_consolidados = 0
        
        errores = []
        tareas_exitosas = 0
        
        for resultado in resultados_paralelos:
            if resultado.get('success', False):
                tareas_exitosas += 1
                task_name = resultado.get('task', 'unknown')
                
                if task_name == 'procesar_empleados_libro':
                    empleados_consolidados = resultado.get('empleados_consolidados', 0)
                    headers_consolidados = resultado.get('headers_consolidados', 0)
                elif task_name == 'procesar_movimientos_personal':
                    movimientos_creados = resultado.get('movimientos_creados', 0)
                elif task_name == 'procesar_conceptos_consolidados':
                    conceptos_consolidados = resultado.get('conceptos_consolidados', 0)
                    
                logger.info(f"‚úÖ Tarea {task_name} completada exitosamente")
            else:
                error = resultado.get('error', 'Error desconocido')
                task_name = resultado.get('task', 'unknown')
                errores.append(f"{task_name}: {error}")
                logger.error(f"‚ùå Error en tarea {task_name}: {error}")
        
        # Verificar si todas las tareas fueron exitosas
        if len(errores) > 0:
            logger.error(f"‚ùå Consolidaci√≥n fall√≥. Errores en {len(errores)} tareas:")
            for error in errores:
                logger.error(f"  - {error}")
            
            # Cambiar estado a error
            cierre.estado = 'error_consolidacion'
            cierre.save(update_fields=['estado'])
            
            return {
                'success': False,
                'cierre_id': cierre_id,
                'errores': errores,
                'tareas_exitosas': tareas_exitosas,
                'total_tareas': len(resultados_paralelos)
            }
        
        # üéØ EJECUTAR PROCESAMIENTO DE CONCEPTOS CONSOLIDADOS
        # (Ahora que los empleados ya est√°n consolidados)
        logger.info("üí∞ [FINAL] Iniciando procesamiento de conceptos consolidados...")
        try:
            resultado_conceptos = procesar_conceptos_consolidados_paralelo(cierre_id)
            if resultado_conceptos.get('success', False):
                conceptos_consolidados = resultado_conceptos.get('conceptos_consolidados', 0)
                logger.info(f"‚úÖ [FINAL] Conceptos consolidados procesados: {conceptos_consolidados}")
            else:
                logger.error(f"‚ùå [FINAL] Error procesando conceptos: {resultado_conceptos.get('error', 'Error desconocido')}")
                conceptos_consolidados = 0
        except Exception as e:
            logger.error(f"‚ùå [FINAL] Excepci√≥n procesando conceptos: {e}")

        # üéØ AGREGAR RESUMEN POR TIPO DE CONCEPTO (guardar en fuente_datos)
        try:
            from .utils.aggregate_by_tipo_concepto import aggregate_by_tipo
            from .models import NominaConsolidada

            logger.info("üîé [FINAL] Generando resumen por tipo_concepto para cada n√≥mina consolidada...")
            nominas = NominaConsolidada.objects.filter(cierre=cierre)
            resumen_global = {}
            for nom in nominas:
                try:
                    resumen = aggregate_by_tipo(nom)
                    # Guardar en fuente_datos['resumen_totales']
                    fuente = nom.fuente_datos or {}
                    fuente['resumen_totales'] = {k: str(v) for k, v in resumen.items()}
                    nom.fuente_datos = fuente
                    nom.save(update_fields=['fuente_datos'])

                    # Agregar al resumen global para logging
                    resumen_global[nom.rut_empleado] = resumen
                except Exception as ex:
                    logger.error(f"‚ö†Ô∏è Error al agregar resumen para nomina {nom.id}: {ex}")

            logger.info(f"‚úÖ [FINAL] Resumen por tipo_concepto generado para {len(resumen_global)} n√≥minas")
        except Exception as e:
            logger.error(f"‚ùå [FINAL] Error generando resumen por tipo_concepto: {e}")
            conceptos_consolidados = 0
        
        # FINALIZAR CONSOLIDACI√ìN EXITOSA
        cierre.estado = 'datos_consolidados'
        cierre.fecha_consolidacion = timezone.now()
        cierre.save(update_fields=['estado', 'fecha_consolidacion'])
        
        logger.info(f"‚úÖ [FINAL] Consolidaci√≥n OPTIMIZADA completada exitosamente:")
        logger.info(f"   üìä {empleados_consolidados} empleados consolidados")
        logger.info(f"   üìã {headers_consolidados} headers-valores creados")
        logger.info(f"   üîÑ {movimientos_creados} movimientos de personal creados")
        logger.info(f"   üí∞ {conceptos_consolidados} conceptos consolidados creados")
        logger.info(f"   ‚ö° Todas las tareas ejecutadas en PARALELO")
        
        return {
            'success': True,
            'cierre_id': cierre_id,
            'modo': 'optimizado_paralelo',
            'empleados_consolidados': empleados_consolidados,
            'headers_consolidados': headers_consolidados,
            'movimientos_creados': movimientos_creados,
            'conceptos_consolidados': conceptos_consolidados,
            'tareas_exitosas': tareas_exitosas,
            'tiempo_finalizacion': timezone.now().isoformat(),
            'nuevo_estado': cierre.estado
        }
        
    except Exception as e:
        logger.error(f"‚ùå [FINAL] Error cr√≠tico consolidando resultados para cierre {cierre_id}: {e}")
        
        # Revertir estado en caso de error cr√≠tico
        try:
            cierre = CierreNomina.objects.get(id=cierre_id)
            cierre.estado = 'verificado_sin_discrepancias'
            cierre.save(update_fields=['estado'])
        except:
            pass
            
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id,
            'modo': 'optimizado_paralelo'
        }


@shared_task
def consolidar_datos_nomina_task(cierre_id, modo='optimizado'):
    """
    üîÑ TAREA: CONSOLIDACI√ìN DE DATOS DE N√ìMINA
    
    Ahora con soporte para dos modos:
    - 'optimizado': Usa Celery Chord para procesamiento paralelo (RECOMENDADO)
    - 'secuencial': Versi√≥n original con procesamiento secuencial
    
    Toma un cierre en estado 'verificado_sin_discrepancias' y:
    1. Lee los archivos procesados (Libro, Movimientos, Analista)
    2. Genera registros de NominaConsolidada
    3. Genera registros de HeaderValorEmpleado (mapeo 1:1 Excel)
    4. Genera registros de MovimientoPersonal
    5. Cambia estado a 'datos_consolidados'
    """
    import logging
    from django.utils import timezone
    
    logger = logging.getLogger(__name__)
    logger.info(f"üîÑ Iniciando consolidaci√≥n de datos para cierre {cierre_id} - Modo: {modo}")
    
    # Elegir versi√≥n seg√∫n el modo
    if modo == 'optimizado':
        logger.info("üöÄ Usando versi√≥n OPTIMIZADA con Celery Chord")
        return consolidar_datos_nomina_task_optimizado(cierre_id)
    else:
        logger.info("‚è≥ Usando versi√≥n SECUENCIAL (original)")
        return consolidar_datos_nomina_task_secuencial(cierre_id)


@shared_task
def consolidar_datos_nomina_task_secuencial(cierre_id):
    """
    üîÑ TAREA: CONSOLIDACI√ìN DE DATOS DE N√ìMINA
    
    Toma un cierre en estado 'verificado_sin_discrepancias' y:
    1. Lee los archivos procesados (Libro, Movimientos, Analista)
    2. Genera registros de NominaConsolidada
    3. Genera registros de HeaderValorEmpleado (mapeo 1:1 Excel)
    4. Genera registros de MovimientoPersonal
    5. Cambia estado a 'datos_consolidados'
    """
    import logging
    from django.utils import timezone
    from decimal import Decimal, InvalidOperation
    
    logger = logging.getLogger(__name__)
    logger.info(f"üîÑ Iniciando consolidaci√≥n de datos para cierre {cierre_id}")
    
    try:
        # 1. OBTENER EL CIERRE
        from .models import CierreNomina, NominaConsolidada, HeaderValorEmpleado, MovimientoPersonal
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        logger.info(f"üìã Cierre obtenido: {cierre} - Estado: {cierre.estado}")
        
        # Verificar estado
        if cierre.estado not in ['verificado_sin_discrepancias', 'datos_consolidados']:
            raise ValueError(f"El cierre debe estar en 'verificado_sin_discrepancias' o 'datos_consolidados', actual: {cierre.estado}")
        
        # Cambiar estado a procesando
        cierre.estado = 'consolidando'
        cierre.save(update_fields=['estado'])
        
        # 2. OBTENER ARCHIVOS PROCESADOS
        libro = cierre.libros_remuneraciones.filter(estado='procesado').first()
        movimientos = cierre.movimientos_mes.filter(estado='procesado').first()
        
        if not libro:
            raise ValueError("No hay libro de remuneraciones procesado")
        if not movimientos:
            raise ValueError("No hay archivo de movimientos procesado")
            
        logger.info(f"üìö Libro: {libro.archivo.name}")
        logger.info(f"üîÑ Movimientos: {movimientos.archivo.name}")
        
        # 3. LIMPIAR CONSOLIDACI√ìN ANTERIOR (SI EXISTE)
        consolidaciones_eliminadas = cierre.nomina_consolidada.count()
        if consolidaciones_eliminadas > 0:
            logger.info(f"üóëÔ∏è Eliminando {consolidaciones_eliminadas} registros de consolidaci√≥n anterior...")
            # Limpiar tambi√©n MovimientoPersonal relacionados
            from .models import MovimientoPersonal
            movimientos_eliminados = MovimientoPersonal.objects.filter(nomina_consolidada__cierre=cierre).count()
            MovimientoPersonal.objects.filter(nomina_consolidada__cierre=cierre).delete()
            
            cierre.nomina_consolidada.all().delete()
            logger.info(f"‚úÖ {consolidaciones_eliminadas} registros de consolidaci√≥n anterior eliminados exitosamente")
            logger.info(f"‚úÖ {movimientos_eliminados} movimientos de personal anteriores eliminados exitosamente")
        else:
            logger.info("‚ÑπÔ∏è No hay consolidaci√≥n anterior que eliminar")
        
        logger.info("üîÑ Iniciando nueva consolidaci√≥n desde cero...")
        
        # 4. CONSOLIDAR DATOS DEL LIBRO DE REMUNERACIONES
        empleados_consolidados = 0
        headers_consolidados = 0
        
        # Obtener empleados del libro
        empleados = cierre.empleados.all()
        logger.info(f"üë• Procesando {empleados.count()} empleados")
        
        for empleado in empleados:
            # Crear registro de n√≥mina consolidada
            nomina_consolidada = NominaConsolidada.objects.create(
                cierre=cierre,
                rut_empleado=normalizar_rut(empleado.rut),
                nombre_empleado=f"{empleado.nombre} {empleado.apellido_paterno} {empleado.apellido_materno}".strip(),
                estado_empleado='activo',  # TODO: Detectar estado seg√∫n movimientos
                dias_trabajados=empleado.dias_trabajados,
                fecha_consolidacion=timezone.now(),
                fuente_datos={
                    'libro_id': libro.id,
                    'movimientos_id': movimientos.id,
                    'consolidacion_version': '1.0'
                }
            )
            
            # Obtener registros de conceptos del empleado
            conceptos_empleado = empleado.registroconceptoempleado_set.all()
            
            # Crear HeaderValorEmpleado para cada concepto
            for concepto in conceptos_empleado:
                # Determinar si es num√©rico
                valor_numerico = None
                es_numerico = False
                
                if concepto.monto:
                    try:
                        # Limpiar el valor (remover $, comas, pero MANTENER puntos decimales)
                        valor_limpio = str(concepto.monto).replace('$', '').replace(',', '').strip()
                        # Verificar si es un n√∫mero v√°lido (entero o decimal)
                        if valor_limpio and (valor_limpio.replace('-', '').replace('.', '').isdigit()):
                            valor_numerico = Decimal(valor_limpio)
                            es_numerico = True
                    except (ValueError, InvalidOperation, AttributeError):
                        pass
                
                # Crear registro HeaderValorEmpleado
                HeaderValorEmpleado.objects.create(
                    nomina_consolidada=nomina_consolidada,
                    nombre_header=concepto.nombre_concepto_original,
                    concepto_remuneracion=concepto.concepto,
                    valor_original=concepto.monto or '',
                    valor_numerico=valor_numerico,
                    es_numerico=es_numerico,
                    fuente_archivo='libro_remuneraciones',
                    fecha_consolidacion=timezone.now()
                )
                headers_consolidados += 1
            
            empleados_consolidados += 1
            
            if empleados_consolidados % 100 == 0:
                logger.info(f"üìä Progreso: {empleados_consolidados} empleados consolidados")
        
        # 5. CONSOLIDAR MOVIMIENTOS DE PERSONAL
        movimientos_creados = 0
        
        # Importar modelos de movimientos
        from .models import (
            MovimientoAltaBaja, MovimientoAusentismo, MovimientoVacaciones,
            MovimientoVariacionSueldo, MovimientoVariacionContrato, ConceptoConsolidado
        )
        
        logger.info("üîÑ Iniciando consolidaci√≥n de movimientos de personal...")
        
        # 5.1 Procesar ALTAS y BAJAS (Ingresos y Finiquitos)
        altas_bajas = MovimientoAltaBaja.objects.filter(cierre=cierre)
        logger.info(f"üìä Procesando {altas_bajas.count()} movimientos de altas/bajas")
        
        for movimiento in altas_bajas:
            # Buscar o crear el empleado consolidado
            nomina_consolidada = None
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(movimiento.rut)
                )
            except NominaConsolidada.DoesNotExist:
                # Si no existe, crear uno nuevo para este movimiento
                    # Crear n√≥mina consolidada usando los nuevos campos por categor√≠a.
                    # Sueldo base se considera haberes imponibles por defecto.
                    nomina_consolidada = NominaConsolidada.objects.create(
                            cierre=cierre,
                            rut_empleado=normalizar_rut(movimiento.rut),
                            nombre_empleado=movimiento.nombres_apellidos,
                            cargo=movimiento.cargo,
                            centro_costo=movimiento.centro_de_costo,
                            estado_empleado='nueva_incorporacion' if movimiento.alta_o_baja == 'ALTA' else 'finiquito',
                            dias_trabajados=movimiento.dias_trabajados,
                            haberes_imponibles=movimiento.sueldo_base or Decimal('0'),
                            haberes_no_imponibles=Decimal('0'),
                            dctos_legales=Decimal('0'),
                            otros_dctos=Decimal('0'),
                            impuestos=Decimal('0'),
                            horas_extras_cantidad=Decimal('0'),
                            aportes_patronales=Decimal('0'),
                            fecha_consolidacion=timezone.now(),
                            fuente_datos={
                                'libro_id': libro.id,
                                'movimientos_id': movimientos.id,
                                'consolidacion_version': '1.0',
                                'movimiento_tipo': 'alta_baja'
                            }
                        )
            
            # Actualizar estado seg√∫n el movimiento
            if movimiento.alta_o_baja == 'ALTA':
                nomina_consolidada.estado_empleado = 'nueva_incorporacion'
            elif movimiento.alta_o_baja == 'BAJA':
                nomina_consolidada.estado_empleado = 'finiquito'
            
            nomina_consolidada.save(update_fields=['estado_empleado'])
            
            # Crear MovimientoPersonal
            MovimientoPersonal.objects.create(
                nomina_consolidada=nomina_consolidada,
                tipo_movimiento='ingreso' if movimiento.alta_o_baja == 'ALTA' else 'finiquito',
                motivo=movimiento.motivo,
                fecha_movimiento=movimiento.fecha_ingreso if movimiento.alta_o_baja == 'ALTA' else movimiento.fecha_retiro,
                observaciones=f"Tipo contrato: {movimiento.tipo_contrato}, Sueldo base: ${movimiento.sueldo_base:,.0f}",
                fecha_deteccion=timezone.now(),
                detectado_por_sistema='consolidacion_automatica_v1'
            )
            
            movimientos_creados += 1
        
        # 5.2 Procesar AUSENTISMOS
        ausentismos = MovimientoAusentismo.objects.filter(cierre=cierre)
        logger.info(f"üìä Procesando {ausentismos.count()} movimientos de ausentismo")
        
        for ausentismo in ausentismos:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(ausentismo.rut)
                )
                
                # Actualizar estado si es ausencia total
                if ausentismo.dias >= 30:  # M√°s de 30 d√≠as = ausencia total
                    nomina_consolidada.estado_empleado = 'ausente_total'
                    nomina_consolidada.dias_ausencia = ausentismo.dias
                else:
                    nomina_consolidada.estado_empleado = 'ausente_parcial'
                    nomina_consolidada.dias_ausencia = ausentismo.dias
                
                nomina_consolidada.save(update_fields=['estado_empleado', 'dias_ausencia'])
                
                # Crear MovimientoPersonal
                MovimientoPersonal.objects.create(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='ausentismo',
                    motivo=f"{ausentismo.tipo} - {ausentismo.motivo}",
                    dias_ausencia=ausentismo.dias,
                    fecha_movimiento=ausentismo.fecha_inicio_ausencia,
                    observaciones=f"Desde: {ausentismo.fecha_inicio_ausencia} hasta: {ausentismo.fecha_fin_ausencia}. {ausentismo.observaciones}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_automatica_v1'
                )
                
                movimientos_creados += 1
                
            except NominaConsolidada.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {normalizar_rut(ausentismo.rut)} en ausentismo")
                continue
        
        # 5.3 Procesar VACACIONES
        vacaciones = MovimientoVacaciones.objects.filter(cierre=cierre)
        logger.info(f"üìä Procesando {vacaciones.count()} movimientos de vacaciones")
        
        for vacacion in vacaciones:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(vacacion.rut)
                )
                
                # Crear MovimientoPersonal para vacaciones
                MovimientoPersonal.objects.create(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='ausentismo',
                    motivo='Vacaciones',
                    dias_ausencia=vacacion.cantidad_dias,
                    fecha_movimiento=vacacion.fecha_inicio,
                    observaciones=f"Vacaciones desde: {vacacion.fecha_inicio} hasta: {vacacion.fecha_fin_vacaciones}. Retorno: {vacacion.fecha_retorno}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_automatica_v1'
                )
                
                movimientos_creados += 1
                
            except NominaConsolidada.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {normalizar_rut(vacacion.rut)} en vacaciones")
                continue
        
        # 5.4 Procesar VARIACIONES DE SUELDO
        variaciones_sueldo = MovimientoVariacionSueldo.objects.filter(cierre=cierre)
        logger.info(f"üìä Procesando {variaciones_sueldo.count()} variaciones de sueldo")
        
        for variacion in variaciones_sueldo:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(variacion.rut)
                )
                
                # Crear MovimientoPersonal
                MovimientoPersonal.objects.create(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='cambio_datos',
                    motivo=f"Variaci√≥n de sueldo: {variacion.porcentaje_reajuste}%",
                    fecha_movimiento=cierre.fecha_creacion.date(),
                    observaciones=f"Sueldo anterior: ${variacion.sueldo_base_anterior:,.0f} ‚Üí Sueldo actual: ${variacion.sueldo_base_actual:,.0f} (Variaci√≥n: ${variacion.variacion_pesos:,.0f})",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_automatica_v1'
                )
                
                movimientos_creados += 1
                
            except NominaConsolidada.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {normalizar_rut(variacion.rut)} en variaci√≥n de sueldo")
                continue
        
        # 5.5 Procesar VARIACIONES DE CONTRATO
        variaciones_contrato = MovimientoVariacionContrato.objects.filter(cierre=cierre)
        logger.info(f"üìä Procesando {variaciones_contrato.count()} variaciones de contrato")
        
        for variacion in variaciones_contrato:
            try:
                nomina_consolidada = NominaConsolidada.objects.get(
                    cierre=cierre, 
                    rut_empleado=normalizar_rut(variacion.rut)
                )
                
                # Crear MovimientoPersonal
                MovimientoPersonal.objects.create(
                    nomina_consolidada=nomina_consolidada,
                    tipo_movimiento='cambio_datos',
                    motivo='Cambio de tipo de contrato',
                    fecha_movimiento=cierre.fecha_creacion.date(),
                    observaciones=f"Contrato anterior: {variacion.tipo_contrato_anterior} ‚Üí Contrato actual: {variacion.tipo_contrato_actual}",
                    fecha_deteccion=timezone.now(),
                    detectado_por_sistema='consolidacion_automatica_v1'
                )
                
                movimientos_creados += 1
                
            except NominaConsolidada.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è No se encontr√≥ empleado consolidado para RUT {normalizar_rut(variacion.rut)} en variaci√≥n de contrato")
                continue
        
        # 5.6 CONSOLIDAR CONCEPTOS POR EMPLEADO
        logger.info("üí∞ Consolidando conceptos y totales por empleado...")
        conceptos_consolidados = 0
        
        for nomina_consolidada in NominaConsolidada.objects.filter(cierre=cierre):
            # Obtener todos los headers para este empleado
            headers_empleado = HeaderValorEmpleado.objects.filter(nomina_consolidada=nomina_consolidada)
            
            total_haberes = Decimal('0')
            total_descuentos = Decimal('0')
            
            # Agrupar por concepto y sumar
            conceptos_agrupados = {}
            
            for header in headers_empleado:
                if header.concepto_remuneracion and header.valor_numerico:
                    concepto_nombre = header.concepto_remuneracion.nombre_concepto  # Corregido: usar nombre_concepto
                    clasificacion = header.concepto_remuneracion.clasificacion
                    
                    if concepto_nombre not in conceptos_agrupados:
                        conceptos_agrupados[concepto_nombre] = {
                            'clasificacion': clasificacion,
                            'monto_total': Decimal('0'),
                            'cantidad': 0,
                            'concepto_obj': header.concepto_remuneracion
                        }
                    
                    conceptos_agrupados[concepto_nombre]['monto_total'] += header.valor_numerico
                    conceptos_agrupados[concepto_nombre]['cantidad'] += 1
                    
                    # Sumar a la categor√≠a correspondiente seg√∫n clasificaci√≥n proporcionada por analista
                    if clasificacion == 'haberes_imponibles':
                        total_haberes_imponibles += header.valor_numerico
                    elif clasificacion == 'haberes_no_imponibles':
                        total_haberes_no_imponibles += header.valor_numerico
                    elif clasificacion == 'horas_extras':
                        try:
                            total_horas_extras += Decimal(header.valor_numerico) if header.valor_numerico is not None else Decimal('0')
                        except Exception:
                            total_horas_extras += Decimal('0')
                    elif clasificacion == 'descuentos_legales':
                        total_dctos_legales += header.valor_numerico
                    elif clasificacion == 'otros_descuentos':
                        total_otros_dctos += header.valor_numerico
                    elif clasificacion == 'impuestos':
                        total_impuestos += header.valor_numerico
                    elif clasificacion == 'aportes_patronales':
                        total_aportes_patronales += header.valor_numerico
            
            # Crear ConceptoConsolidado para cada concepto agrupado
            for concepto_nombre, datos in conceptos_agrupados.items():
                # Mapear clasificaci√≥n a tipo_concepto
                clasificacion_mapping = {
                    'haberes_imponibles': 'haber_imponible',
                    'haberes_no_imponibles': 'haber_no_imponible',
                    'descuentos_legales': 'descuento_legal',
                    'otros_descuentos': 'otro_descuento',
                    'impuestos': 'impuesto',
                    'aportes_patronales': 'aporte_patronal',
                }
                
                tipo_concepto = clasificacion_mapping.get(datos['clasificacion'], 'informativo')
                # ConceptoRemuneracion no tiene campo codigo, usar id o dejarlo None
                codigo_concepto = str(datos['concepto_obj'].id) if datos['concepto_obj'] else None
                
                ConceptoConsolidado.objects.create(
                    nomina_consolidada=nomina_consolidada,
                    codigo_concepto=codigo_concepto,
                    nombre_concepto=concepto_nombre,
                    tipo_concepto=tipo_concepto,
                    monto_total=datos['monto_total'],
                    cantidad=datos['cantidad'],
                    es_numerico=True,
                    fuente_archivo='libro_remuneraciones'
                )
                conceptos_consolidados += 1
            
            # Actualizar nuevos campos por categor√≠a en la n√≥mina consolidada
            nomina_consolidada.haberes_imponibles = total_haberes_imponibles
            nomina_consolidada.haberes_no_imponibles = total_haberes_no_imponibles
            nomina_consolidada.dctos_legales = total_dctos_legales
            nomina_consolidada.otros_dctos = total_otros_dctos
            nomina_consolidada.impuestos = total_impuestos
            nomina_consolidada.horas_extras_cantidad = total_horas_extras
            nomina_consolidada.aportes_patronales = total_aportes_patronales
            nomina_consolidada.save(update_fields=['haberes_imponibles', 'haberes_no_imponibles', 'dctos_legales', 'otros_dctos', 'impuestos', 'horas_extras_cantidad', 'aportes_patronales'])
        
        logger.info(f"üí∞ {conceptos_consolidados} conceptos consolidados creados")
        
        # 6. FINALIZAR CONSOLIDACI√ìN
        cierre.estado = 'datos_consolidados'
        cierre.fecha_consolidacion = timezone.now()
        cierre.save(update_fields=['estado', 'fecha_consolidacion'])
        
        logger.info(f"‚úÖ Consolidaci√≥n completada:")
        logger.info(f"   üìä {empleados_consolidados} empleados consolidados")
        logger.info(f"   üìã {headers_consolidados} headers-valores creados")
        logger.info(f"   üîÑ {movimientos_creados} movimientos de personal creados")
        logger.info(f"   üí∞ {conceptos_consolidados} conceptos consolidados creados")
        
        return {
            'success': True,
            'cierre_id': cierre_id,
            'empleados_consolidados': empleados_consolidados,
            'headers_consolidados': headers_consolidados,
            'movimientos_creados': movimientos_creados,
            'conceptos_consolidados': conceptos_consolidados,
            'nuevo_estado': cierre.estado
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en consolidaci√≥n para cierre {cierre_id}: {e}")
        
        # Revertir estado en caso de error
        try:
            cierre = CierreNomina.objects.get(id=cierre_id)
            cierre.estado = 'verificado_sin_discrepancias'
            cierre.save(update_fields=['estado'])
        except:
            pass
            
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id
        }


# ===== üöÄ NUEVO SISTEMA PARALELO DE INCIDENCIAS =====

@shared_task
def generar_incidencias_cierre_paralelo(cierre_id, clasificaciones_seleccionadas):
    """
    üöÄ TASK PRINCIPAL: Sistema dual de procesamiento paralelo
    
    Coordina dos procesamientos simult√°neos:
    1. Filtrado: Solo clasificaciones seleccionadas
    2. Completo: Todas las clasificaciones
    3. Comparaci√≥n: Validaci√≥n cruzada de resultados
    """
    logger.info(f"üöÄ Iniciando procesamiento paralelo dual (legacy) para cierre {cierre_id}")
    try:
        sel_len = len(clasificaciones_seleccionadas)
    except Exception:
        sel_len = 0
    logger.info(f"üìã Clasificaciones seleccionadas: {sel_len}")

    # üîÅ Redirecci√≥n autom√°tica a V2 si no hay clasificaciones seleccionadas
    if not clasificaciones_seleccionadas:
        try:
            from .utils.DetectarIncidenciasConsolidadas import generar_incidencias_consolidados_v2
            logger.info("üîÅ Sin clasificaciones seleccionadas ‚Üí redirigiendo a sistema dual V2 (configuraci√≥n autom√°tica, umbral 30%)")
            job = generar_incidencias_consolidados_v2.delay(cierre_id)
            return {
                'success': True,
                'cierre_id': str(cierre_id),
                'redirigido_a': 'dual_v2',
                'task_id_v2': job.id,
                'mensaje': 'Legacy redirigido a V2 por falta de clasificaciones seleccionadas'
            }
        except Exception as e:
            logger.error(f"‚ùå Error redirigiendo a V2: {e}")
            # Si falla la redirecci√≥n, seguir con el flujo legacy para no romper
    
    try:
        from .models import CierreNomina
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        # Obtener todas las clasificaciones disponibles para este cierre
        todas_clasificaciones = obtener_clasificaciones_cierre(cierre_id)
        logger.info(f"üìä Total clasificaciones disponibles: {len(todas_clasificaciones)}")
        
        # Crear chunks para procesamiento eficiente
        chunks_seleccionadas = crear_chunks(clasificaciones_seleccionadas, chunk_size=6)
        chunks_todas = crear_chunks(todas_clasificaciones, chunk_size=6)
        
        logger.info(f"üîÄ Chunks filtrado: {len(chunks_seleccionadas)}, Chunks completo: {len(chunks_todas)}")
        
        # Ejecutar ambos procesamientos en paralelo usando chord
        # NO PODEMOS USAR .get() dentro de una tarea Celery
        # En su lugar, configuramos el chord para que se ejecute de forma as√≠ncrona
        
        logger.info("üîÑ Configurando ejecuci√≥n as√≠ncrona con callback...")
        
        # 1. Configurar procesamiento filtrado
        if chunks_seleccionadas:
            logger.info("üîç Configurando procesamiento filtrado...")
            chord_filtrado = chord([
                procesar_chunk_clasificaciones.s(cierre_id, chunk, 'filtrado', idx)
                for idx, chunk in enumerate(chunks_seleccionadas)
            ])(consolidar_resultados_filtrados.s(cierre_id))
        else:
            # Si no hay clasificaciones seleccionadas, crear resultado vac√≠o
            chord_filtrado = None
        
        # 2. Configurar procesamiento completo
        logger.info("üìä Configurando procesamiento completo...")
        chord_completo = chord([
            procesar_chunk_clasificaciones.s(cierre_id, chunk, 'completo', idx)
            for idx, chunk in enumerate(chunks_todas)
        ])(consolidar_resultados_completos.s(cierre_id))
        
        # 3. Retornar informaci√≥n de la configuraci√≥n sin esperar resultados
        resultado_inmediato = {
            'success': True,
            'cierre_id': cierre_id,
            'timestamp': timezone.now().isoformat(),
            'configuracion_completada': True,
            'chunks_filtrado': len(chunks_seleccionadas) if chunks_seleccionadas else 0,
            'chunks_completo': len(chunks_todas),
            'clasificaciones_seleccionadas': len(clasificaciones_seleccionadas),
            'clasificaciones_totales': len(todas_clasificaciones),
            'chord_filtrado_id': chord_filtrado.id if chord_filtrado else None,
            'chord_completo_id': chord_completo.id,
            'mensaje': 'Procesamiento configurado y ejecut√°ndose de forma as√≠ncrona'
        }
        
        logger.info(f"‚úÖ Configuraci√≥n completada para cierre {cierre_id}")
        logger.info(f"üìä Chunks configurados: {len(chunks_seleccionadas) if chunks_seleccionadas else 0} filtrado, {len(chunks_todas)} completo")
        
        return resultado_inmediato
        
    except Exception as e:
        logger.error(f"‚ùå Error configurando procesamiento paralelo para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id,
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def obtener_resultado_procesamiento_dual(cierre_id, chord_filtrado_id=None, chord_completo_id=None):
    """
    üéØ TASK: Obtener resultado final del procesamiento dual
    
    Esta tarea se puede llamar para obtener el estado final del procesamiento
    despu√©s de que los chords hayan terminado.
    """
    from celery.result import AsyncResult
    
    logger.info(f"üîç Obteniendo resultados finales para cierre {cierre_id}")
    
    try:
        resultados_finales = {}
        
        # Obtener resultado filtrado si existe
        if chord_filtrado_id:
            result_filtrado = AsyncResult(chord_filtrado_id)
            if result_filtrado.ready():
                resultados_finales['filtrado'] = result_filtrado.result
            else:
                resultados_finales['filtrado'] = {'pendiente': True}
        else:
            resultados_finales['filtrado'] = {
                'success': True,
                'total_incidencias': 0,
                'mensaje': 'No se procesaron clasificaciones filtradas'
            }
        
        # Obtener resultado completo
        if chord_completo_id:
            result_completo = AsyncResult(chord_completo_id)
            if result_completo.ready():
                resultados_finales['completo'] = result_completo.result
            else:
                resultados_finales['completo'] = {'pendiente': True}
        
        # Generar reporte final
        resultado_filtrado = resultados_finales.get('filtrado', {})
        resultado_completo = resultados_finales.get('completo', {})
        
        reporte_final = {
            'success': True,
            'cierre_id': cierre_id,
            'timestamp': timezone.now().isoformat(),
            'total_incidencias': (resultado_filtrado.get('total_incidencias', 0) + 
                                resultado_completo.get('total_incidencias', 0)),
            'total_incidencias_individuales': resultado_filtrado.get('total_incidencias', 0),
            'total_incidencias_suma': resultado_completo.get('total_incidencias', 0),
            'resultados_detallados': resultados_finales,
            'ambos_listos': (not resultado_filtrado.get('pendiente', False) and 
                           not resultado_completo.get('pendiente', False))
        }
        
        logger.info(f"üìä Reporte dual generado para cierre {cierre_id}")
        logger.info(f"‚úÖ Total incidencias: {reporte_final['total_incidencias']}")
        
        return reporte_final
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo resultados duales para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id
        }


@shared_task
def procesar_chunk_clasificaciones(cierre_id, chunk_clasificaciones, tipo_procesamiento, chunk_idx):
    """
    üîß TASK: Procesa un chunk de clasificaciones
    
    Args:
        cierre_id: ID del cierre
        chunk_clasificaciones: Lista de IDs de clasificaciones a procesar
        tipo_procesamiento: 'filtrado' o 'completo'
        chunk_idx: √çndice del chunk para tracking
    """
    logger.info(f"üîß Procesando chunk {chunk_idx} tipo '{tipo_procesamiento}' con {len(chunk_clasificaciones)} clasificaciones")
    
    try:
        resultados_chunk = []
        
        for clasificacion_id in chunk_clasificaciones:
            logger.debug(f"   üìù Procesando clasificaci√≥n {clasificacion_id}")
            
            # Ejecutar la l√≥gica de detecci√≥n de incidencias para esta clasificaci√≥n
            resultado_clasificacion = procesar_incidencias_clasificacion_individual(
                cierre_id, 
                clasificacion_id, 
                tipo_procesamiento
            )
            
            resultados_chunk.append({
                'clasificacion_id': clasificacion_id,
                'resultado': resultado_clasificacion,
                'timestamp': timezone.now().isoformat()
            })
        
        logger.info(f"‚úÖ Chunk {chunk_idx} procesado: {len(resultados_chunk)} clasificaciones")
        
        return {
            'chunk_idx': chunk_idx,
            'tipo_procesamiento': tipo_procesamiento,
            'clasificaciones_procesadas': len(resultados_chunk),
            'resultados': resultados_chunk,
            'success': True
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando chunk {chunk_idx}: {e}")
        return {
            'chunk_idx': chunk_idx,
            'tipo_procesamiento': tipo_procesamiento,
            'success': False,
            'error': str(e)
        }


@shared_task
def consolidar_resultados_filtrados(resultados_chunks, cierre_id):
    """
    üìä TASK: Consolida resultados del procesamiento filtrado
    """
    logger.info(f"üìä Consolidando resultados filtrados para cierre {cierre_id}")
    return consolidar_resultados_chunks(resultados_chunks, cierre_id, 'filtrado')


@shared_task
def consolidar_resultados_completos(resultados_chunks, cierre_id):
    """
    üìä TASK: Consolida resultados del procesamiento completo
    """
    logger.info(f"üìä Consolidando resultados completos para cierre {cierre_id}")
    return consolidar_resultados_chunks(resultados_chunks, cierre_id, 'completo')


@shared_task
def procesar_resultado_vacio(cierre_id, tipo_procesamiento):
    """
    üìù TASK: Genera resultado vac√≠o cuando no hay clasificaciones seleccionadas
    """
    logger.info(f"üìù Generando resultado vac√≠o para tipo '{tipo_procesamiento}'")
    return {
        'cierre_id': cierre_id,
        'tipo_procesamiento': tipo_procesamiento,
        'total_incidencias': 0,
        'incidencias_por_regla': {
            'variaciones_conceptos': 0,
            'ausentismos_continuos': 0,
            'ingresos_faltantes': 0,
            'finiquitos_presentes': 0
        },
        'clasificaciones_procesadas': 0,
        'success': True,
        'timestamp': timezone.now().isoformat()
    }


@shared_task
def comparar_y_generar_reporte_final(cierre_id, resultado_filtrado, resultado_completo, 
                                   total_seleccionadas, total_disponibles):
    """
    üéØ TASK FINAL: Compara ambos resultados y genera reporte unificado
    """
    logger.info(f"üéØ Generando reporte final para cierre {cierre_id}")
    logger.info(f"üìä Seleccionadas: {total_seleccionadas}, Disponibles: {total_disponibles}")
    
    try:
        # Calcular diferencias y m√©tricas
        comparacion = {
            'cierre_id': cierre_id,
            'timestamp': timezone.now().isoformat(),
            'configuracion': {
                'clasificaciones_seleccionadas': total_seleccionadas,
                'clasificaciones_disponibles': total_disponibles,
                'modo_procesamiento': 'paralelo_dual'
            },
            'resultados': {
                'filtrado': resultado_filtrado,
                'completo': resultado_completo
            },
            'analisis_comparativo': calcular_diferencias_resultados(resultado_filtrado, resultado_completo),
            'metricas_rendimiento': {
                'cobertura_porcentaje': (total_seleccionadas / total_disponibles * 100) if total_disponibles > 0 else 0,
                'clasificaciones_no_procesadas': total_disponibles - total_seleccionadas
            }
        }
        
        # Guardar comparaci√≥n en base de datos
        guardar_comparacion_incidencias(cierre_id, comparacion)
        
        # Actualizar estado del cierre
        actualizar_estado_cierre_post_procesamiento(cierre_id, comparacion)
        
        logger.info(f"‚úÖ Reporte final generado exitosamente para cierre {cierre_id}")
        
        return {
            'success': True,
            'cierre_id': cierre_id,
            'comparacion': comparacion,
            'resumen': {
                'incidencias_filtrado': resultado_filtrado.get('total_incidencias', 0),
                'incidencias_completo': resultado_completo.get('total_incidencias', 0),
                'cobertura_porcentaje': comparacion['metricas_rendimiento']['cobertura_porcentaje']
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error generando reporte final para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id
        }


# ===== üõ†Ô∏è FUNCIONES AUXILIARES =====

def obtener_clasificaciones_cierre(cierre_id):
    """Obtiene todas las clasificaciones disponibles para un cierre"""
    try:
        from .models import CierreNomina
        # Aqu√≠ implementar√≠as la l√≥gica espec√≠fica para obtener clasificaciones
        # Por ahora devuelvo un ejemplo
        return list(range(1, 21))  # Ejemplo: 20 clasificaciones
    except Exception as e:
        logger.error(f"Error obteniendo clasificaciones: {e}")
        return []


def crear_chunks(lista, chunk_size=6):
    """Divide una lista en chunks del tama√±o especificado"""
    if not lista:
        return []
    return [lista[i:i + chunk_size] for i in range(0, len(lista), chunk_size)]


def procesar_incidencias_clasificacion_individual(cierre_id, clasificacion_id, tipo_procesamiento):
    """Procesa incidencias para una clasificaci√≥n individual"""
    try:
        # Aqu√≠ va la l√≥gica espec√≠fica de detecci√≥n de incidencias
        # Por ahora simulo el procesamiento
        logger.debug(f"Procesando incidencias para clasificaci√≥n {clasificacion_id} (tipo: {tipo_procesamiento})")
        
        return {
            'clasificacion_id': clasificacion_id,
            'incidencias_encontradas': 0,  # Simulado
            'reglas_aplicadas': ['variaciones_conceptos', 'ausentismos_continuos'],
            'success': True
        }
    except Exception as e:
        logger.error(f"Error procesando clasificaci√≥n {clasificacion_id}: {e}")
        return {
            'clasificacion_id': clasificacion_id,
            'success': False,
            'error': str(e)
        }


def consolidar_resultados_chunks(resultados_chunks, cierre_id, tipo_procesamiento):
    """Consolida los resultados de m√∫ltiples chunks"""
    try:
        total_incidencias = 0
        clasificaciones_procesadas = 0
        incidencias_por_regla = {
            'variaciones_conceptos': 0,
            'ausentismos_continuos': 0,
            'ingresos_faltantes': 0,
            'finiquitos_presentes': 0
        }
        
        chunks_exitosos = 0
        
        for chunk_resultado in resultados_chunks:
            if chunk_resultado.get('success', False):
                chunks_exitosos += 1
                clasificaciones_procesadas += chunk_resultado.get('clasificaciones_procesadas', 0)
                # Aqu√≠ sumar√≠as las incidencias espec√≠ficas seg√∫n tu l√≥gica
        
        logger.info(f"üìä Consolidaci√≥n {tipo_procesamiento}: {chunks_exitosos} chunks, {clasificaciones_procesadas} clasificaciones")
        
        return {
            'cierre_id': cierre_id,
            'tipo_procesamiento': tipo_procesamiento,
            'total_incidencias': total_incidencias,
            'incidencias_por_regla': incidencias_por_regla,
            'clasificaciones_procesadas': clasificaciones_procesadas,
            'chunks_exitosos': chunks_exitosos,
            'success': True,
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error consolidando resultados {tipo_procesamiento}: {e}")
        return {
            'success': False,
            'error': str(e),
            'tipo_procesamiento': tipo_procesamiento
        }


def calcular_diferencias_resultados(resultado_filtrado, resultado_completo):
    """Calcula las diferencias entre el procesamiento filtrado y completo"""
    try:
        return {
            'diferencia_incidencias': (
                resultado_completo.get('total_incidencias', 0) - 
                resultado_filtrado.get('total_incidencias', 0)
            ),
            'diferencia_clasificaciones': (
                resultado_completo.get('clasificaciones_procesadas', 0) - 
                resultado_filtrado.get('clasificaciones_procesadas', 0)
            ),
            'coherencia_resultados': (
                resultado_filtrado.get('success', False) and 
                resultado_completo.get('success', False)
            )
        }
    except Exception as e:
        logger.error(f"Error calculando diferencias: {e}")
        return {'error': str(e)}


def guardar_comparacion_incidencias(cierre_id, comparacion):
    """Guarda la comparaci√≥n en la base de datos"""
    try:
        # Aqu√≠ implementar√≠as el guardado en tu modelo de base de datos
        logger.info(f"üíæ Guardando comparaci√≥n para cierre {cierre_id}")
        pass
    except Exception as e:
        logger.error(f"Error guardando comparaci√≥n: {e}")


def actualizar_estado_cierre_post_procesamiento(cierre_id, comparacion):
    """Actualiza el estado del cierre despu√©s del procesamiento"""
    try:
        from .models import CierreNomina
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        if comparacion['resultados']['completo'].get('success') and comparacion['resultados']['filtrado'].get('success'):
            cierre.estado = 'con_incidencias'
            cierre.save(update_fields=['estado'])
            logger.info(f"‚úÖ Estado del cierre {cierre_id} actualizado a 'con_incidencias'")
        
    except Exception as e:
        logger.error(f"Error actualizando estado del cierre: {e}")


# ===== üöÄ NUEVO SISTEMA PARALELO DE DISCREPANCIAS =====

@shared_task
def generar_discrepancias_cierre_paralelo(cierre_id, usuario_id=None):
    """
    üöÄ TASK PRINCIPAL: Sistema paralelo de generaci√≥n de discrepancias
    
    Coordina dos procesamientos simult√°neos usando Celery Chord:
    1. Chunk 1: Discrepancias Libro vs Novedades
    2. Chunk 2: Discrepancias Movimientos vs Analista
    3. Consolidaci√≥n: Unificaci√≥n y actualizaci√≥n del estado
    
    Args:
        cierre_id: ID del cierre a verificar
        usuario_id: ID del usuario que ejecuta la verificaci√≥n (para auditor√≠a)
    """
    logger.info(f"üöÄ Iniciando generaci√≥n paralela de discrepancias para cierre {cierre_id}")
    
    try:
        from .models import CierreNomina, HistorialVerificacionCierre
        from django.contrib.auth import get_user_model
        from django.utils import timezone
        from django.db import models
        
        User = get_user_model()
        cierre = CierreNomina.objects.get(id=cierre_id)
        
        # üìä CREAR REGISTRO DE AUDITOR√çA
        # Obtener el n√∫mero de intento (contando intentos anteriores + 1)
        ultimo_intento = HistorialVerificacionCierre.objects.filter(
            cierre=cierre
        ).aggregate(
            max_intento=models.Max('numero_intento')
        )['max_intento'] or 0
        
        nuevo_intento = ultimo_intento + 1
        
        # Obtener usuario ejecutor si se proporciona
        usuario_ejecutor = None
        if usuario_id:
            try:
                usuario_ejecutor = User.objects.get(id=usuario_id)
                logger.info(f"üë§ Usuario ejecutor: {usuario_ejecutor.nombre} ({usuario_ejecutor.correo_bdo})")
            except User.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è Usuario {usuario_id} no encontrado")
        
        # Crear el registro de historial
        historial = HistorialVerificacionCierre.objects.create(
            cierre=cierre,
            numero_intento=nuevo_intento,
            usuario_ejecutor=usuario_ejecutor,
            estado_verificacion='procesando',  # ‚úÖ Estado correcto del modelo
            task_id=generar_discrepancias_cierre_paralelo.request.id if hasattr(generar_discrepancias_cierre_paralelo, 'request') else None,
            archivos_analizados={
                'inicio_procesamiento': timezone.now().isoformat(),
                'chunk_types': ['libro_vs_novedades', 'movimientos_vs_analista']
            }
        )
        
        logger.info(f"üìã Creado historial de verificaci√≥n #{nuevo_intento} para cierre {cierre_id}")
        
        # Cambiar estado a verificacion_datos al iniciar
        cierre.estado = 'verificacion_datos'
        cierre.save(update_fields=['estado'])
        
        # Eliminar discrepancias anteriores si existen
        cierre.discrepancias.all().delete()
        logger.info(f"üßπ Discrepancias anteriores limpiadas para cierre {cierre_id}")
        
        # Actualizar historial como procesando
        historial.estado_verificacion = 'procesando'
        historial.save(update_fields=['estado_verificacion'])
        
        # Ejecutar procesamientos en paralelo usando chord
        chord_paralelo = chord([
            procesar_discrepancias_chunk.s(cierre_id, 'libro_vs_novedades', historial.id),
            procesar_discrepancias_chunk.s(cierre_id, 'movimientos_vs_analista', historial.id)
        ])(consolidar_discrepancias_finales.s(cierre_id, historial.id))
        
        logger.info(f"üìä Chord de discrepancias iniciado para cierre {cierre_id}")
        return {
            'success': True,
            'cierre_id': cierre_id,
            'chord_id': str(chord_paralelo),
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en generaci√≥n paralela de discrepancias para cierre {cierre_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id,
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def procesar_discrepancias_chunk(cierre_id, tipo_chunk, historial_id):
    """
    üîß TASK: Procesa un chunk espec√≠fico de discrepancias
    
    Args:
        cierre_id: ID del cierre
        tipo_chunk: 'libro_vs_novedades' o 'movimientos_vs_analista'
        historial_id: ID del historial de verificaci√≥n para auditor√≠a
    """
    logger.info(f"üîß Procesando chunk '{tipo_chunk}' para cierre {cierre_id} (historial {historial_id})")
    
    try:
        from .models import CierreNomina, HistorialVerificacionCierre
        from .utils.GenerarDiscrepancias import (
            generar_discrepancias_libro_vs_novedades,
            generar_discrepancias_movimientos_vs_analista
        )
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        historial = HistorialVerificacionCierre.objects.get(id=historial_id)
        
        if tipo_chunk == 'libro_vs_novedades':
            resultado = generar_discrepancias_libro_vs_novedades(cierre)
            logger.info(f"üìö Libro vs Novedades: {resultado['total_discrepancias']} discrepancias")
            
            # Crear registros de DiscrepanciaHistorial
            from .utils.GenerarDiscrepancias import crear_discrepancias_historial
            registros_creados = crear_discrepancias_historial(cierre_id, historial_id, 'libro_vs_novedades')
            logger.info(f"üìù Creados {registros_creados} registros de auditor√≠a para libro vs novedades")
            
        elif tipo_chunk == 'movimientos_vs_analista':
            resultado = generar_discrepancias_movimientos_vs_analista(cierre)
            logger.info(f"üìã Movimientos vs Analista: {resultado['total_discrepancias']} discrepancias")
            
            # Crear registros de DiscrepanciaHistorial
            from .utils.GenerarDiscrepancias import crear_discrepancias_historial
            registros_creados = crear_discrepancias_historial(cierre_id, historial_id, 'movimientos_vs_analista')
            logger.info(f"üìù Creados {registros_creados} registros de auditor√≠a para movimientos vs analista")
            
        else:
            raise ValueError(f"Tipo de chunk desconocido: {tipo_chunk}")
        
        return {
            'chunk_tipo': tipo_chunk,
            'cierre_id': cierre_id,
            'total_discrepancias': resultado['total_discrepancias'],
            'detalle': resultado,
            'success': True,
            'timestamp': timezone.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando chunk '{tipo_chunk}' para cierre {cierre_id}: {e}")
        return {
            'chunk_tipo': tipo_chunk,
            'cierre_id': cierre_id,
            'success': False,
            'error': str(e),
            'timestamp': timezone.now().isoformat()
        }


@shared_task
def consolidar_discrepancias_finales(resultados_chunks, cierre_id, historial_id):
    """
    üéØ TASK FINAL: Consolida los resultados de ambos chunks y actualiza el estado
    
    Args:
        resultados_chunks: Resultados de los chunks procesados
        cierre_id: ID del cierre
        historial_id: ID del historial de verificaci√≥n para auditor√≠a
    """
    logger.info(f"üéØ Consolidando discrepancias finales para cierre {cierre_id} (historial {historial_id})")
    
    try:
        from .models import CierreNomina, HistorialVerificacionCierre, DiscrepanciaHistorial
        from django.utils import timezone
        
        cierre = CierreNomina.objects.get(id=cierre_id)
        historial = HistorialVerificacionCierre.objects.get(id=historial_id)
        
        # Procesar resultados de ambos chunks
        total_discrepancias = 0
        discrepancias_libro_vs_novedades = 0
        discrepancias_movimientos_vs_analista = 0
        resultados_detallados = {}
        chunks_exitosos = 0
        
        for resultado in resultados_chunks:
            if resultado.get('success', False):
                chunks_exitosos += 1
                chunk_tipo = resultado['chunk_tipo']
                chunk_total = resultado['total_discrepancias']
                
                total_discrepancias += chunk_total
                
                # Contar por tipo para el historial
                if chunk_tipo == 'libro_vs_novedades':
                    discrepancias_libro_vs_novedades = chunk_total
                elif chunk_tipo == 'movimientos_vs_analista':
                    discrepancias_movimientos_vs_analista = chunk_total
                
                resultados_detallados[chunk_tipo] = resultado['detalle']
                
                logger.info(f"‚úÖ Chunk '{chunk_tipo}': {chunk_total} discrepancias")
            else:
                logger.error(f"‚ùå Chunk fallido: {resultado}")
        
        # Actualizar estado del cierre seg√∫n los resultados
        if chunks_exitosos == 2:  # Ambos chunks exitosos
            if total_discrepancias == 0:
                cierre.estado = 'verificado_sin_discrepancias'
                mensaje = "Sin discrepancias - Datos verificados exitosamente"
            else:
                cierre.estado = 'con_discrepancias'
                mensaje = f"Con discrepancias - {total_discrepancias} diferencias detectadas"
        else:
            cierre.estado = 'con_discrepancias'  # Por seguridad
            mensaje = "Error en verificaci√≥n - Estado de seguridad activado"
        
        cierre.save(update_fields=['estado'])
        
        consolidacion_final = {
            'cierre_id': cierre_id,
            'total_discrepancias': total_discrepancias,
            'chunks_exitosos': chunks_exitosos,
            'chunks_procesados': len(resultados_chunks),
            'estado_final': cierre.estado,
            'mensaje': mensaje,
            'resultados_detallados': resultados_detallados,
            'timestamp': timezone.now().isoformat(),
            'success': True
        }

        logger.info(f"‚úÖ Consolidaci√≥n completada para cierre {cierre_id}: {mensaje}")
        # IMPORTANTE: Esta tarea es solo para consolidar los resultados de la
        # verificaci√≥n y registrar el historial de ejecuci√≥n. NO debe iniciar
        # autom√°ticamente el proceso de consolidaci√≥n de datos (`consolidar_datos_nomina_task`).
        # La consolidaci√≥n debe ser disparada expl√≠citamente por el usuario cuando
        # confirme que desea proceder (por ejemplo, mediante el bot√≥n UI que llama
        # a `consolidar_datos_nomina_task.delay(cierre_id)` cuando `total_discrepancias == 0`).
        # Esto evita condiciones de carrera y ejecuciones indeseadas durante flows
        # autom√°ticos de verificaci√≥n.

        # Actualizar historial de verificaci√≥n si existe
        if historial_id:
            try:
                from .models import HistorialVerificacionCierre
                from django.utils import timezone

                historial = HistorialVerificacionCierre.objects.get(id=historial_id)

                # Calcular tiempo de ejecuci√≥n en segundos
                tiempo_ejecucion_segundos = None
                if historial.fecha_ejecucion:
                    tiempo_ejecucion_segundos = int((timezone.now() - historial.fecha_ejecucion).total_seconds())

                # Actualizar con todos los campos requeridos
                historial.fecha_finalizacion = timezone.now()
                historial.tiempo_ejecucion = tiempo_ejecucion_segundos
                historial.total_discrepancias_encontradas = total_discrepancias
                historial.estado_verificacion = 'completado'  # ‚úÖ Estado correcto del modelo
                historial.observaciones = mensaje  # ‚úÖ Campo correcto

                # Actualizar estad√≠sticas por tipo si est√°n disponibles
                if 'libro_vs_novedades' in resultados_detallados:
                    historial.discrepancias_libro_vs_novedades = resultados_detallados['libro_vs_novedades'].get('total_discrepancias', 0)

                if 'movimientos_vs_analista' in resultados_detallados:
                    historial.discrepancias_movimientos_vs_analista = resultados_detallados['movimientos_vs_analista'].get('total_discrepancias', 0)

                historial.save(update_fields=[
                    'fecha_finalizacion',
                    'tiempo_ejecucion',
                    'total_discrepancias_encontradas',
                    'discrepancias_libro_vs_novedades',
                    'discrepancias_movimientos_vs_analista',
                    'estado_verificacion',
                    'observaciones'
                ])

                logger.info(f"‚úÖ Historial {historial_id} completado - Usuario: {historial.usuario_ejecutor.correo_bdo if historial.usuario_ejecutor else 'Sin usuario'}, Duraci√≥n: {tiempo_ejecucion_segundos}s, Discrepancias: {total_discrepancias}")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è No se pudo actualizar historial {historial_id}: {e}")

        return consolidacion_final

    except Exception as e:
        logger.error(f"‚ùå Error consolidando discrepancias para cierre {cierre_id}: {e}")
        
        # Actualizar historial en caso de error
        if historial_id:
            try:
                from .models import HistorialVerificacionCierre
                from django.utils import timezone
                
                historial = HistorialVerificacionCierre.objects.get(id=historial_id)
                
                # Calcular tiempo de ejecuci√≥n en segundos
                tiempo_ejecucion_segundos = None
                if historial.fecha_ejecucion:
                    tiempo_ejecucion_segundos = int((timezone.now() - historial.fecha_ejecucion).total_seconds())
                
                historial.fecha_finalizacion = timezone.now()
                historial.tiempo_ejecucion = tiempo_ejecucion_segundos
                historial.estado_verificacion = 'error'  # ‚úÖ Estado correcto del modelo
                historial.observaciones = f"Error durante consolidaci√≥n: {str(e)}"  # ‚úÖ Campo correcto
                historial.save(update_fields=[
                    'fecha_finalizacion', 
                    'tiempo_ejecucion',
                    'estado_verificacion', 
                    'observaciones'  # ‚úÖ Campo correcto
                ])
                
                logger.info(f"‚ùå Historial {historial_id} marcado con error - Usuario: {historial.usuario_ejecutor.email if historial.usuario_ejecutor else 'Sin usuario'}, Duraci√≥n: {tiempo_ejecucion_segundos}s")
                
            except Exception as e2:
                logger.warning(f"‚ö†Ô∏è No se pudo actualizar historial {historial_id} con error: {e2}")
        
        # En caso de error, establecer estado de seguridad
        try:
            from .models import CierreNomina
            cierre = CierreNomina.objects.get(id=cierre_id)
            cierre.estado = 'con_discrepancias'  # Estado de seguridad
            cierre.save(update_fields=['estado'])
        except:
            pass


# ========================================
# üöÄ TASKS OPTIMIZADAS CON CELERY CHORD
# ========================================

@shared_task
def procesar_chunk_empleados_task(libro_id, chunk_data):
    """
    üë• Task para procesar un chunk espec√≠fico de empleados en paralelo.
    
    Args:
        libro_id: ID del LibroRemuneracionesUpload
        chunk_data: Datos del chunk a procesar
        
    Returns:
        Dict: Estad√≠sticas del procesamiento del chunk
    """
    from .utils.LibroRemuneracionesOptimizado import procesar_chunk_empleados_util
    
    logger.info(f"üîÑ Iniciando procesamiento de chunk empleados {chunk_data.get('chunk_id')}")
    
    try:
        resultado = procesar_chunk_empleados_util(libro_id, chunk_data)
        logger.info(f"‚úÖ Chunk empleados {chunk_data.get('chunk_id')} completado exitosamente")
        return resultado
    except Exception as e:
        error_msg = f"Error en chunk empleados {chunk_data.get('chunk_id')}: {str(e)}"
        logger.error(error_msg)
        return {
            'chunk_id': chunk_data.get('chunk_id', 0),
            'empleados_procesados': 0,
            'errores': [error_msg],
            'libro_id': libro_id
        }


@shared_task
def procesar_chunk_registros_task(libro_id, chunk_data):
    """
    üìù Task para procesar registros de n√≥mina de un chunk espec√≠fico en paralelo.
    
    Args:
        libro_id: ID del LibroRemuneracionesUpload
        chunk_data: Datos del chunk a procesar
        
    Returns:
        Dict: Estad√≠sticas del procesamiento del chunk
    """
    from .utils.LibroRemuneracionesOptimizado import procesar_chunk_registros_util
    
    logger.info(f"üîÑ Iniciando procesamiento de chunk registros {chunk_data.get('chunk_id')}")
    
    try:
        resultado = procesar_chunk_registros_util(libro_id, chunk_data)
        logger.info(f"‚úÖ Chunk registros {chunk_data.get('chunk_id')} completado exitosamente")
        return resultado
    except Exception as e:
        error_msg = f"Error en chunk registros {chunk_data.get('chunk_id')}: {str(e)}"
        logger.error(error_msg)
        return {
            'chunk_id': chunk_data.get('chunk_id', 0),
            'registros_procesados': 0,
            'errores': [error_msg],
            'libro_id': libro_id
        }


@shared_task
def consolidar_empleados_task(resultados_chunks):
    """
    üìä Task callback para consolidar resultados de procesamiento de empleados.
    
    Args:
        resultados_chunks: Lista de resultados de todos los chunks
        
    Returns:
        Dict: Estad√≠sticas consolidadas
    """
    from .utils.LibroRemuneracionesOptimizado import consolidar_stats_empleados
    
    logger.info(f"üìä Consolidando resultados de {len(resultados_chunks)} chunks de empleados")
    
    try:
        consolidado = consolidar_stats_empleados(resultados_chunks)
        
        # Log resultado final
        logger.info(
            f"‚úÖ Consolidaci√≥n empleados: {consolidado['total_empleados_procesados']} empleados, "
            f"{consolidado['chunks_exitosos']}/{consolidado['total_chunks']} chunks exitosos"
        )
        
        return consolidado
        
    except Exception as e:
        error_msg = f"Error consolidando empleados: {str(e)}"
        logger.error(error_msg)
        return {
            'total_empleados_procesados': 0,
            'chunks_exitosos': 0,
            'total_chunks': len(resultados_chunks),
            'errores': [error_msg],
            'procesamiento_exitoso': False
        }


@shared_task
def consolidar_registros_task(resultados_chunks):
    """
    üìä Task callback para consolidar resultados de procesamiento de registros.
    
    Args:
        resultados_chunks: Lista de resultados de todos los chunks
        
    Returns:
        Dict: Estad√≠sticas consolidadas
    """
    from .utils.LibroRemuneracionesOptimizado import consolidar_stats_registros
    
    logger.info(f"üìä Consolidando resultados de {len(resultados_chunks)} chunks de registros")
    
    try:
        consolidado = consolidar_stats_registros(resultados_chunks)
        
        # Actualizar estado del libro a "procesado" si todo sali√≥ bien
        if consolidado.get('procesamiento_exitoso') and consolidado.get('libro_id'):
            try:
                libro = LibroRemuneracionesUpload.objects.get(id=consolidado['libro_id'])
                libro.estado = "procesado"
                libro.save(update_fields=['estado'])
                logger.info(f"‚úÖ Estado del libro {consolidado['libro_id']} actualizado a 'procesado'")
            except Exception as e:
                logger.error(f"Error actualizando estado del libro: {e}")
        
        # Log resultado final
        logger.info(
            f"‚úÖ Consolidaci√≥n registros: {consolidado['total_registros_procesados']} registros, "
            f"{consolidado['chunks_exitosos']}/{consolidado['total_chunks']} chunks exitosos"
        )
        
        return consolidado
        
    except Exception as e:
        error_msg = f"Error consolidando registros: {str(e)}"
        logger.error(error_msg)
        return {
            'total_registros_procesados': 0,
            'chunks_exitosos': 0,
            'total_chunks': len(resultados_chunks),
            'errores': [error_msg],
            'procesamiento_exitoso': False
        }


@shared_task
def actualizar_empleados_desde_libro_optimizado(result, usar_chord=True):
    """
    üöÄ Versi√≥n optimizada que usa Celery Chord para procesar empleados en paralelo.
    
    Args:
        result: Resultado de la task anterior (contiene libro_id)
        usar_chord: Si usar chord (True) o procesamiento secuencial (False)
        
    Returns:
        Dict: Estad√≠sticas del procesamiento o resultado del chord
    """
    libro_id = result.get("libro_id") if isinstance(result, dict) else result
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        if not usar_chord:
            # Fallback al m√©todo original
            logger.info(f"üìù Usando m√©todo secuencial para empleados del libro {libro_id}")
            count = actualizar_empleados_desde_libro_util(libro)
            return {"libro_id": libro_id, "empleados_actualizados": count}
        
        # üöÄ USAR CHORD PARA PARALELIZACI√ìN
        from .utils.LibroRemuneracionesOptimizado import dividir_dataframe_empleados
        
        logger.info(f"üöÄ Iniciando procesamiento optimizado con Chord para libro {libro_id}")
        
        # Calcular chunk size din√°mico
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        total_filas = len(df)
        chunk_size = calcular_chunk_size_dinamico(total_filas)
        
        logger.info(f"üìä Total filas: {total_filas}, Chunk size: {chunk_size}")
        
        # Dividir en chunks
        chunks = dividir_dataframe_empleados(libro.archivo.path, chunk_size)
        
        if not chunks:
            logger.warning(f"‚ö†Ô∏è No se encontraron chunks v√°lidos para libro {libro_id}")
            return {"libro_id": libro_id, "empleados_actualizados": 0}
        
        # Crear tasks paralelas usando chord
        tasks_paralelas = [
            procesar_chunk_empleados_task.s(libro_id, chunk_data) 
            for chunk_data in chunks
        ]
        
        # Ejecutar chord: tasks paralelas | callback
        callback = consolidar_empleados_task.s()
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(
            f"üöÄ Chord iniciado para libro {libro_id}: {len(chunks)} chunks en paralelo"
        )
        
        # Retornar referencia al chord para monitoreo
        return {
            "libro_id": libro_id,
            "chord_id": resultado_chord.id if hasattr(resultado_chord, 'id') else None,
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord"
        }
        
    except Exception as e:
        error_msg = f"Error en procesamiento optimizado de empleados para libro {libro_id}: {e}"
        logger.error(error_msg)
        
        # Fallback al m√©todo original en caso de error
        try:
            logger.info(f"üîÑ Fallback a m√©todo secuencial para libro {libro_id}")
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            count = actualizar_empleados_desde_libro_util(libro)
            return {"libro_id": libro_id, "empleados_actualizados": count, "fallback": True}
        except Exception as fallback_error:
            logger.error(f"Error en fallback: {fallback_error}")
            raise


@shared_task
def guardar_registros_nomina_optimizado(result, usar_chord=True):
    """
    üöÄ Versi√≥n optimizada que usa Celery Chord para procesar registros en paralelo.
    
    Args:
        result: Resultado de la task anterior (contiene libro_id)
        usar_chord: Si usar chord (True) o procesamiento secuencial (False)
        
    Returns:
        Dict: Estad√≠sticas del procesamiento o resultado del chord
    """
    libro_id = result.get("libro_id") if isinstance(result, dict) else result
    
    try:
        libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
        
        if not usar_chord:
            # Fallback al m√©todo original
            logger.info(f"üìù Usando m√©todo secuencial para registros del libro {libro_id}")
            count = guardar_registros_nomina_util(libro)
            libro.estado = "procesado"
            libro.save(update_fields=['estado'])
            return {
                "libro_id": libro_id, 
                "registros_actualizados": count,
                "estado": "procesado"
            }
        
        # üöÄ USAR CHORD PARA PARALELIZACI√ìN
        from .utils.LibroRemuneracionesOptimizado import dividir_dataframe_empleados
        
        logger.info(f"üöÄ Iniciando procesamiento optimizado de registros con Chord para libro {libro_id}")
        
        # Calcular chunk size din√°mico
        df = pd.read_excel(libro.archivo.path, engine="openpyxl")
        total_filas = len(df)
        chunk_size = calcular_chunk_size_dinamico(total_filas)
        
        logger.info(f"üìä Total filas: {total_filas}, Chunk size: {chunk_size}")
        
        # Dividir en chunks
        chunks = dividir_dataframe_empleados(libro.archivo.path, chunk_size)
        
        if not chunks:
            logger.warning(f"‚ö†Ô∏è No se encontraron chunks v√°lidos para libro {libro_id}")
            # Actualizar estado y retornar
            libro.estado = "procesado"
            libro.save(update_fields=['estado'])
            return {
                "libro_id": libro_id, 
                "registros_actualizados": 0,
                "estado": "procesado"
            }
        
        # Crear tasks paralelas usando chord
        tasks_paralelas = [
            procesar_chunk_registros_task.s(libro_id, chunk_data) 
            for chunk_data in chunks
        ]
        
        # Ejecutar chord: tasks paralelas | callback
        callback = consolidar_registros_task.s()
        resultado_chord = chord(tasks_paralelas)(callback)
        
        logger.info(
            f"üöÄ Chord de registros iniciado para libro {libro_id}: {len(chunks)} chunks en paralelo"
        )
        
        # Retornar referencia al chord para monitoreo
        return {
            "libro_id": libro_id,
            "chord_id": resultado_chord.id if hasattr(resultado_chord, 'id') else None,
            "chunks_totales": len(chunks),
            "modo": "optimizado_chord",
            "estado": "procesando"  # El estado se actualizar√° en el callback
        }
        
    except Exception as e:
        error_msg = f"Error en procesamiento optimizado de registros para libro {libro_id}: {e}"
        logger.error(error_msg)
        
        # Fallback al m√©todo original en caso de error
        try:
            logger.info(f"üîÑ Fallback a m√©todo secuencial para registros del libro {libro_id}")
            libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
            count = guardar_registros_nomina_util(libro)
            libro.estado = "procesado"
            libro.save(update_fields=['estado'])
            return {
                "libro_id": libro_id, 
                "registros_actualizados": count,
                "estado": "procesado",
                "fallback": True
            }
        except Exception as fallback_error:
            logger.error(f"Error en fallback: {fallback_error}")
            # Marcar como error
            try:
                libro = LibroRemuneracionesUpload.objects.get(id=libro_id)
                libro.estado = "con_error"
                libro.save(update_fields=['estado'])
            except:
                pass
            raise
        
        return {
            'success': False,
            'error': str(e),
            'cierre_id': cierre_id,
            'timestamp': timezone.now().isoformat()
        }

# ==========================================================
# üì¶ Tasks para generar Informe de Cierre (Libro + Movimientos)
# ==========================================================
from django.db.models import Sum, Count, F, DecimalField, IntegerField, ExpressionWrapper, Value, Q
from django.db.models.functions import Coalesce


@shared_task(name='nomina.build_informe_libro')
def build_informe_libro(cierre_id: int) -> dict:
    """
    Construye payload del Libro de Remuneraciones desde NominaConsolidada,
    equivalente a la data que consumen las p√°ginas (detalle + resumen).
    """
    from .models import CierreNomina, NominaConsolidada, HeaderValorEmpleado, ConceptoConsolidado
    cierre = CierreNomina.objects.get(id=cierre_id)

    if not cierre.nomina_consolidada.exists():
        return {
            'error': 'No hay datos consolidados para este cierre',
            'detalle': None,
            'resumen': None,
        }

    qs = NominaConsolidada.objects.filter(cierre=cierre)

    # Resumen general (totales por campo principal)
    liquido_expr = ExpressionWrapper(
        F('haberes_imponibles') + F('haberes_no_imponibles') - F('dctos_legales') - F('otros_dctos') - F('impuestos'),
        output_field=DecimalField(max_digits=20, decimal_places=2),
    )
    zero_dec_2 = Value(0, output_field=DecimalField(max_digits=20, decimal_places=2))
    zero_dec_4 = Value(0, output_field=DecimalField(max_digits=20, decimal_places=4))
    zero_int = Value(0, output_field=IntegerField())
    resumen_agg = qs.aggregate(
        total_empleados=Count('id'),
        total_haberes_imponibles=Coalesce(Sum('haberes_imponibles'), zero_dec_2),
        total_haberes_no_imponibles=Coalesce(Sum('haberes_no_imponibles'), zero_dec_2),
        total_dctos_legales=Coalesce(Sum('dctos_legales'), zero_dec_2),
        total_otros_dctos=Coalesce(Sum('otros_dctos'), zero_dec_2),
        total_impuestos=Coalesce(Sum('impuestos'), zero_dec_2),
        total_aportes_patronales=Coalesce(Sum('aportes_patronales'), zero_dec_2),
        horas_extras_cantidad_total=Coalesce(Sum('horas_extras_cantidad'), zero_dec_4),
    )

    # Desglose por concepto (por tipo_concepto)
    conceptos_base = (
        ConceptoConsolidado.objects
        .filter(nomina_consolidada__cierre=cierre)
        .values('tipo_concepto', 'nombre_concepto')
        .annotate(
            total_monto=Coalesce(Sum('monto_total'), zero_dec_2),
            total_cantidad=Coalesce(Sum('cantidad'), zero_dec_4),
        )
        .order_by('tipo_concepto', '-total_monto')
    )

    def _lista_categoria(tipo):
        return [
            {
                'concepto': it['nombre_concepto'],
                'monto_total': it['total_monto'],
            }
            for it in conceptos_base if it['tipo_concepto'] == tipo
        ]

    # Intentar detectar horas extras por nombre de concepto
    horas_extras_det = (
        ConceptoConsolidado.objects
        .filter(
            Q(nomina_consolidada__cierre=cierre)
            & Q(nombre_concepto__icontains='hora')
            & Q(nombre_concepto__icontains='extra')
        )
        .values('nombre_concepto')
        .annotate(total_cantidad=Coalesce(Sum('cantidad'), zero_dec_4))
        .order_by('-total_cantidad')
    )

    resumen_payload = {
        'cierre': {
            'id': cierre.id,
            'cliente_id': cierre.cliente.id,
            'cliente': getattr(cierre.cliente, 'nombre', str(cierre.cliente)),
            'periodo': cierre.periodo,
        },
    # Mantener compatibilidad: 'resumen' conserva los totales con claves previas
    'resumen': resumen_agg,
    'totales': {
            'empleados': resumen_agg['total_empleados'],
            'haberes_imponibles': resumen_agg['total_haberes_imponibles'],
            'haberes_no_imponibles': resumen_agg['total_haberes_no_imponibles'],
            'descuentos_legales': resumen_agg['total_dctos_legales'],
            'otros_descuentos': resumen_agg['total_otros_dctos'],
            'impuestos': resumen_agg['total_impuestos'],
            'aportes_patronales': resumen_agg['total_aportes_patronales'],
            'horas_extras_cantidad': resumen_agg['horas_extras_cantidad_total'],
        },
        'desglose_por_concepto': {
            'haberes_imponibles': _lista_categoria('haber_imponible'),
            'haberes_no_imponibles': _lista_categoria('haber_no_imponible'),
            'descuentos_legales': _lista_categoria('descuento_legal'),
            'otros_descuentos': _lista_categoria('otro_descuento'),
            'impuestos': _lista_categoria('impuesto'),
            'aportes_patronales': _lista_categoria('aporte_patronal'),
            'horas_extras': [
                {'concepto': it['nombre_concepto'], 'cantidad': it['total_cantidad']} for it in horas_extras_det
            ],
        },
    }

    # No incluimos detalle por empleado para mantener JSON liviano
    return {'detalle': None, 'resumen': resumen_payload}


@shared_task(name='nomina.build_informe_movimientos')
def build_informe_movimientos(cierre_id: int) -> dict:
    """
    Construye un resumen compacto de Movimientos del Mes:
    - Total de ingresos
    - Total de finiquitos
    - Total de ausentismos y desglose por tipo (motivo)
    """
    from .models import CierreNomina, MovimientoPersonal
    cierre = CierreNomina.objects.get(id=cierre_id)

    if not cierre.nomina_consolidada.exists():
        return {'error': 'No hay datos consolidados para este cierre', 'movimientos': None}

    movimientos = MovimientoPersonal.objects.filter(nomina_consolidada__cierre=cierre)

    # Totales principales
    total_ingresos = movimientos.filter(tipo_movimiento='ingreso').count()
    total_finiquitos = movimientos.filter(tipo_movimiento='finiquito').count()
    ausentismos_qs = movimientos.filter(tipo_movimiento='ausentismo')
    total_ausentismos = ausentismos_qs.count()

    # Desglose de ausentismos por "tipo" usando el campo motivo
    ausentismos_por_motivo = (
        ausentismos_qs
        .values('motivo')
        .annotate(c=Count('id'))
        .order_by('-c')
    )
    ausentismos_por_tipo = {}
    for row in ausentismos_por_motivo:
        key = row['motivo'] or 'Sin especificar'
        ausentismos_por_tipo[key] = row['c']

    # Conteo por tipo general (compatibilidad m√≠nima)
    por_tipo = {}
    for tipo, display in MovimientoPersonal.TIPO_MOVIMIENTO_CHOICES:
        por_tipo[tipo] = {
            'count': movimientos.filter(tipo_movimiento=tipo).count(),
            'display': display,
        }

    payload = {
        'cierre': {
            'id': cierre.id,
            'cliente': getattr(cierre.cliente, 'nombre', str(cierre.cliente)),
            'periodo': cierre.periodo,
            'estado': cierre.estado,
        },
        'resumen': {
            'total_movimientos': movimientos.count(),
            'por_tipo': por_tipo,
            'totales': {
                'ingresos': total_ingresos,
                'finiquitos': total_finiquitos,
                'ausentismos': total_ausentismos,
            },
            'ausentismos_por_tipo': ausentismos_por_tipo,
        },
        # Mantener clave por compatibilidad pero vac√≠a para JSON liviano
        'movimientos': [],
    }
    return payload


@shared_task(name='nomina.unir_y_guardar_informe')
def unir_y_guardar_informe(resultados: list, cierre_id: int, version: str = 'v2-compact') -> dict:
    """
    Callback del chord: une resultados de libro y movimientos, guarda InformeNomina
    con una estructura compacta en la ra√≠z.
    """
    from .models import CierreNomina
    from .models_informe import InformeNomina

    cierre = CierreNomina.objects.get(id=cierre_id)
    libro, movimientos = resultados
    # Sanitizar versi√≥n para cumplir con max_length=10 del modelo
    raw_version = str(version) if version is not None else '1.0'
    safe_version = raw_version[:10]
    if raw_version != safe_version:
        logger.warning(f"version_calculo '{raw_version}' truncada a '{safe_version}' (max 10)")

    # Extraer totales desde subtareas (manteniendo robustez ante None)
    resumen_libro = (libro or {}).get('resumen') or {}
    totales_libro = resumen_libro.get('totales') or {}

    resumen_mov = (movimientos or {}).get('resumen') or {}
    totales_mov = resumen_mov.get('totales') or {}
    desglose_libro = resumen_libro.get('desglose_por_concepto') or {}

    # Formato de periodo AAAAMM a partir de 'YYYY-MM'
    periodo_aaaamm = (cierre.periodo or '').replace('-', '')

    # Estructura compacta solicitada
    payload = {
        'cierre_id': cierre.id,
        'cliente_nombre': getattr(cierre.cliente, 'nombre', None),
        'cliente_rut': getattr(cierre.cliente, 'rut', None),
        'periodo': periodo_aaaamm,
        'totales_libro': totales_libro,
        'totales_movimientos': totales_mov,
        'desglose_libro': desglose_libro,
        # Los KPIs se agregan en la task siguiente (calcular_kpis_cierre)
        'kpis': {},
    }

    # Asegurar que el JSON no contenga tipos no serializables (Decimal, Timestamp, etc.)
    payload = _json_safe(payload)

    informe, _ = InformeNomina.objects.get_or_create(
        cierre=cierre,
        defaults={'datos_cierre': payload, 'version_calculo': safe_version}
    )
    # actualizar siempre con la √∫ltima versi√≥n
    inicio = timezone.now()
    informe.datos_cierre = payload
    informe.version_calculo = safe_version
    informe.fecha_generacion = inicio
    informe.tiempo_calculo = timezone.now() - inicio
    informe.save(update_fields=['datos_cierre', 'version_calculo', 'fecha_generacion', 'tiempo_calculo'])

    return {'informe_id': informe.id, 'cierre_id': cierre_id, 'saved': True}


@shared_task(name='nomina.finalizar_cierre_post_informe')
def finalizar_cierre_post_informe(prev_result: dict, cierre_id: int, usuario_id: int | None = None) -> dict:
    """
    Task posterior a la generaci√≥n del informe: marca el cierre como 'finalizado',
    setea fecha y usuario de finalizaci√≥n. NO env√≠a a Redis.

    Args:
        prev_result: resultado de la task anterior (unir_y_guardar_informe)
        cierre_id: ID del cierre a finalizar
        usuario_id: ID del usuario que inici√≥ la finalizaci√≥n (opcional)

    Returns:
        dict con estado de finalizaci√≥n y metadata b√°sica.
    """
    from django.utils import timezone
    from django.contrib.auth import get_user_model
    from .models import CierreNomina

    cierre = CierreNomina.objects.get(id=cierre_id)

    user = None
    if usuario_id:
        User = get_user_model()
        try:
            user = User.objects.get(id=usuario_id)
        except User.DoesNotExist:
            user = None

    # Actualizar estado y metadatos sin empujar a Redis
    cierre.estado = 'finalizado'
    cierre.fecha_finalizacion = timezone.now()
    if user:
        cierre.usuario_finalizacion = user
        cierre.save(update_fields=['estado', 'fecha_finalizacion', 'usuario_finalizacion'])
    else:
        cierre.save(update_fields=['estado', 'fecha_finalizacion'])

    return {
        'success': True,
        'cierre_id': cierre_id,
        'finalizado': True,
        'informe_id': (prev_result or {}).get('informe_id')
    }


@shared_task(name='nomina.calcular_kpis_cierre')
def calcular_kpis_cierre(prev_result: dict, cierre_id: int) -> dict:
    """Calcula y guarda solo las tasas: rotaci√≥n, ingreso y ausentismo (sobre dotaci√≥n)."""
    from .models import CierreNomina, NominaConsolidada, MovimientoPersonal
    from .models_informe import InformeNomina

    cierre = CierreNomina.objects.get(id=cierre_id)

    # Base dotaci√≥n
    nc_qs = NominaConsolidada.objects.filter(cierre=cierre)
    dotacion_total = nc_qs.count()

    # Ingresos / Finiquitos / Ausentismos
    mov_qs = MovimientoPersonal.objects.filter(nomina_consolidada__cierre=cierre)
    ingresos_total = mov_qs.filter(tipo_movimiento='ingreso').count()
    finiquitos_total = mov_qs.filter(tipo_movimiento='finiquito').count()
    ausentismos_total = mov_qs.filter(tipo_movimiento='ausentismo').count()

    # Tasas
    denom = float(dotacion_total) if dotacion_total else 1.0
    tasa_rotacion = round(float(finiquitos_total) / denom, 4)
    tasa_ingreso = round(float(ingresos_total) / denom, 4)
    tasa_ausentismo = round(float(ausentismos_total) / denom, 4)

    kpis = {
        'tasa_rotacion': tasa_rotacion,
        'tasa_ingreso': tasa_ingreso,
        'tasa_ausentismo': tasa_ausentismo,
    }

    # Guardar en InformeNomina
    informe = InformeNomina.objects.get(cierre=cierre)
    datos = informe.datos_cierre or {}
    # Actualizar KPIs
    datos['kpis'] = _json_safe(kpis)
    # Remover keys_order si existe (no se debe incluir en el JSON final)
    datos.pop('keys_order', None)
    informe.datos_cierre = datos
    informe.save(update_fields=['datos_cierre'])

    return {'informe_id': informe.id, 'cierre_id': cierre_id, 'kpis': datos['kpis']}


@shared_task(name='nomina.enviar_informe_redis')
def enviar_informe_redis_task(prev_result: dict, cierre_id: int, ttl_hours: int | None = None) -> dict:
    """Env√≠a el informe compacto a Redis DB2 en la ruta sgm:nomina:{cliente_id}:{periodo}:informe."""
    from .models import CierreNomina
    from .models_informe import InformeNomina
    from .cache_redis import get_cache_system_nomina

    cierre = CierreNomina.objects.get(id=cierre_id)
    informe = InformeNomina.objects.get(cierre=cierre)

    # Copia defensiva y purga de keys no deseadas
    payload = dict(informe.datos_cierre or {})
    payload.pop('keys_order', None)
    cliente_id = cierre.cliente.id
    periodo_key = cierre.periodo  # Formato esperado 'YYYY-MM'

    cache_system = get_cache_system_nomina()
    ttl_seconds = None if (ttl_hours is None or ttl_hours <= 0) else int(ttl_hours) * 3600
    ok = cache_system.set_informe_nomina(cliente_id, periodo_key, payload, ttl=ttl_seconds)

    redis_key = f"sgm:nomina:{cliente_id}:{periodo_key}:informe"
    return {
        'informe_id': informe.id,
        'cierre_id': cierre_id,
        'sent_to_redis': bool(ok),
        'redis_key': redis_key,
    }
